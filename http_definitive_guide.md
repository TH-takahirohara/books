HTTP: The Definitive Guide

# Part I: HTTP: The Web's Foundation
- Ch1 ~ Ch4

# Ch1. Overview of HTTP
## 1.1 

## 1.2 Web Clients and Servers
- HTTPサーバーはインターネットのデータを保持し、HTTPクライアントからリクエストされたらそれを渡す。
- 最も一般的なクライアントはWebブラウザ。

## 1.3 Resources
- webサーバーはwebリソースをホストする。webリソースは、webコンテンツの元となるもの。
- 最も単純なwebリソースは、HTMLなどの静的ファイルであるが、静的ファイルでなくても良い。リソースは要求に応じたコンテンツを生成するプログラムでも良い。
- 要約すると、リソースとはあらゆる種類のコンテンツソースのこと。

### 1.3.1 Media Types
- HTTPは、MIMEというデータフォーマットラベルを各オブジェクトに付与して転送する。
- Webサーバは、すべてのHTTPオブジェクトデータにMIMEタイプを付加する。
  - ヘッダー Content-type: image/jpeg のimage/jpegがMIME typeにあたる。
- MIMEタイプは、主なオブジェクトタイプと特定のサブタイプをスラッシュで区切って表現したテキストラベルである。
  - HTMLからなるテキストドキュメントは、text/html となる。

### 1.3.2 URIs
- クライアントは、自分が興味のあるリソースを指摘することができる。サーバーリソース名をURIと呼ぶ。
- URIはインターネットの郵便番号のようなもので、世界中の情報資源を一意に識別し、位置を特定する。
- URIはURLとURNからなる。

### 1.3.3 URLs
- URLは、特定のサーバー上のリソースの特定の場所を示す。
- スキーマ、アドレス、リソースの３箇所に主に分かれる。

### 1.3.4 URNs
- URNは、 リソースが現在どこにあるかに関係なく、特定のコンテンツのユニークな名前を提供する。
- まだ広く採用されていない。対応するインフラが必要なため。

## 1.4 Transactions

### 1.4.1 Methods

### 1.4.2 Status Codes

### 1.4.3 Web pages can consist of multiple objects
- アプリケーションは、タスクを達成するために複数のHTTPトランザクションを発行することがよくある。
- 埋め込まれたリソースは、異なるサーバーに存在することもある。

## 1.5 Messages
- HTTPメッセージはStart line、Header fields、Body からなる。
- ボディには任意のバイナリデータを格納可能

## 1.6 Connection
- TCP
  - エラーのないデータ転送
  - in-order delivery（データは常に送信された順に届く）
  - セグメント化されていないデータストリーム（任意のサイズのデータをいつでも垂れ流すことができる）
- TCP/IPは個々のネットワークやハードウェアの特殊性・欠点を隠す。

## 1.7 Protocol Versions

## 1.8 Architectural components of the web
### 1.8.1 Proxies
- HTTPプロキシサーバーは、Webセキュリティ、アプリケーション統合、パフォーマンス最適化のための重要な構成要素。
- ユーザーの代理として、ユーザーに代わってサーバーにアクセスするアプリケーション。
- プロキシはセキュリティのために使用されることが多く、すべてのウェブトラフィックが流れる信頼できる仲介者として機能する。

### 1.8.2 Caches
- webキャッシュやキャッシングプロキシは、プロキシを通過する主要なドキュメントのコピーを保持する特殊なHTTPプロキシサーバーである。
- クライアントは、離れたwebサーバーからよりも近くのキャッシュを利用することで、はるかに早くドキュメントを取得できる。HTTPは、キャッシュをより効果的にし、キャッシュされたコンテンツの鮮度やプライバシーを管理するための多くの機能を定義している。

### 1.8.3 Gateways
- ゲートウェイとは、他のサーバーを仲介する特別なサーバー。一般にHTTPトラフィックを別のプロトコルに変換する。ゲートウェイはオリジンサーバーのように振る舞うので、クライアントはゲートウェイと通信していると気づかない。

### 1.8.4 Tunnels
- トンネルは、セットアップ後、2つの接続間で生データを盲目的に中継するHTTPアプリケーション。HTTPトンネルは、HTTP以外のデータを1つまたは複数のHTTP接続を介して、データを見ることなく転送するためによく使用される。
- HTTPトンネルは、暗号化されたセキュア・ソケット・レイヤー（SSL）トラフィックをHTTP接続で伝送することで、Webトラフィックのみを許可する企業のファイアウォールを介してSSLトラフィックを許可するという使い方が一般的である。

### 1.8.5 Agents
- ユーザーエージェント（または単にエージェント）は、ユーザーに代わってHTTPリクエストを行うクライアントプログラムである。ブラウザ以外のユーザーエージェントが多くある。
- 例えば、人間が監視することなく、自律的にウェブを歩き回り、HTTPトランザクションを発行したり、コンテンツを取得したりする機械によって自動化されたユーザーエージェントがある。これらの自動化されたエージェントには、「スパイダー」や「ウェブロボット」などのカラフルな名前がついていることが多い。スパイダーは、検索エンジンのデータベースや比較ショッピングロボットの製品カタログなど、ウェブコンテンツの有用なアーカイブを構築するためにウェブを徘徊する。

# Ch2. URLs and Resources
- URLはインターネットリソースの標準的な名前である。

## 2.1 Navigating the Internet's Resources
- URLは、ブラウザが情報を検索する際に必要となるリソースの位置情報です。インターネット上の何十億ものデータリソースを人々やアプリケーションが見つけ、利用し、共有するためのものです。URLは、HTTPやその他のプロトコルに対する人間の通常のアクセスポイントです。人間がブラウザでURLを指定すると、裏ではブラウザが適切なプロトコルメッセージを送信し、人間が求めるリソースを取得します。
- URLの最初の部分はURLスキーマ。webクライアントに「どうやって」リソースにアクセスするかを伝える。
- URLの第二の部分はサーバーの場所。webクライアントに、リソースが「どこに」ホストされているかを伝える。
- URLの第三の部分は、リソースパス。これはサーバー上の「何の」ローカルリソースがリクエストされているかを伝える。
- 世の中にあるすべてのリソースと、そのリソースを得るためのすべての方法に対して、それぞれのリソースに単一の名前を付けることで、誰もがその名前を使ってリソースを見つけることができるようになっているのです。しかし、これは必ずしもそうではありませんでした。

### 2.1.1 The dark days before URLs

## 2.2 URL Syntax
- ほとんどのURLは一般的なURL構文に準拠しており、異なるURLスキームの間でスタイルや構文が大きく重複しています。
- ほとんどのURLは9つのパートに分かれる。

```
<scheme>://<user>:<password>@<host>:<port>/<path>;<params>?<qu
ery>#<frag>
```
- これらの構成要素をすべて含むURLはほとんどありません。最も重要な要素はscheme, host, pathです。

### 2.2.1 Schemes: What Protocol to Use
- schemeは与えられたリソースにアクセスする方法についての主要なIDである。schemeはURLを解釈するアプリケーションに、どのプロトコルを使う必要があるかを伝える。

### 2.2.2 hosts and ports
- インターネット上のリソースを見つけるために、アプリケーションはリソースをホストしているマシンを知り、そのマシンのどこに目的のリソースにアクセスできるサーバーがあるのかを知る必要があります。
- hostコンポーネントは、リソースにアクセスできるインターネット上のホストマシンを特定します。名前は、上記のようにホスト名（「www.joes-hardware.com」）として提供することも、IPアドレスとして提供することもできます。
- portコンポーネントは、サーバーがリッスンしているネットワークポートを特定します。基本的にTCPプロトコルを使用するHTTPの場合、デフォルトのポートは80です。

### 2.2.3 usernames and passwords

### 2.2.4 Paths
- URLのpathコンポーネントは、サーバーマシン上のどこにリソースがあるかを指定します。

### 2.2.5 parameters
- アプリケーションがサーバーと正しく対話するために必要な入力パラメータを与えるために、URLにはparamsコンポーネントがあります。このコンポーネントは、URL内の名前と値のペアのリストで、URLの他の部分と（そしてお互いに）「;」文字で区切られています。このコンポーネントは、アプリケーションがリソースにアクセスするために必要な追加情報を提供します。

### 2.2.6 query strings
- データベースサービスなどの一部のリソースでは、要求されているリソースの種類を絞り込むために、質問や問い合わせをすることができます。
- URLのqueryコンポーネントはゲートウェイ・リソースに渡され、URLのpathコンポーネントはゲートウェイ・リソースを特定します。基本的に、ゲートウェイは他のアプリケーションへのアクセスポイントと考えることができます。

### 2.2.7 fragments
- リソースの一部や断片を参照できるように、URLはリソース内の断片を識別するfragコンポーネントをサポートしています。例えば、URLは、HTMLドキュメント内の特定の画像やセクションを指し示すことができます。
- フラグメントは、URLの右端に#文字を付けてぶら下げます。
- クライアントはリソース全体を取得した後、fragmentで指定するリソースの部分を表示する。（ブラウザの場合、そこまでスクロールする。）

## 2.3 URL Shortcuts
- Web クライアントは、いくつかの URL ショートカットを理解して使用しています。相対URLは、リソースの中にリソースを指定する便利な略語です。また、多くのブラウザはURLの「自動展開」をサポートしています。ユーザーがURLの重要な（記憶に残る）部分を入力すると、ブラウザが残りの部分を埋めてくれます。

### 2.3.1 Relative URLs - 相対URL
- URLには、絶対URLと相対URLの2種類があります。
- ここまでは、絶対URLについてのみ説明してきました。絶対URLには、リソースにアクセスするために必要な情報がすべて含まれています。一方、相対URLは不完全なものです。相対URLからリソースへのアクセスに必要なすべての情報を得るためには、ベースと呼ばれる別のURLから相対的に解釈しなければなりません。相対URLは、URLの便利な略記法です。
- 省略可能な相対URL構文により、HTML作成者はURLからスキームやホストなどの構成要素を省略することができます。これらの要素は、それらが含まれるリソースのベースURLから推測できます。他のリソースのURLもこの短縮形で指定することができます。
- また、相対URLは、一連のリソース（HTMLページなど）をポータブルに保つための便利な方法です。相対URLを使用すると、一連のドキュメントを移動しても、新しいベースに対して相対的に解釈されるため、リンクが機能します。これにより、他のサーバにコンテンツをミラーリングすることなどが可能になります。

#### 2.3.1.1 Base URLs
- 変換プロセスの最初のステップは、ベースURLを見つけることです。ベースURLは、相対URLを参照するためのポイントとなります。ベースURLはいくつかの場所から得られます。
  - リソースで明示されている
  - カプセル化されたリソースのベースURL　（HTMLなど）相対URLが埋め込まれているリソースのURLをベースにすることができる。
  - ベースとなるURLがない

#### 2.3.1.2 Resolving relative references
- 相対URLを絶対URLに変換するアルゴリズムがある。もともとRFC1808で規定され、その後RFC2396に組み込まれた。

### 2.3.2 Expandomatic URLs - URLの拡大
- 一部のブラウザでは、URLを送信した後や入力中に、自動的にURLを展開しようとするものがあります。これにより、ユーザーは完全なURLを入力する必要がなく、自動的に展開されるため、ショートカットを得ることができます。
- これらの「expandomatic」機能には、2つの種類があります。
  - ホスト名展開
    - ブラウザは簡単なヒューリスティクスを利用して、ユーザーの助けを借りずに、入力したホスト名を完全なホスト名に展開することができます。
  - 履歴の展開
    - URLを入力する手間を省くためにブラウザが採用しているもうひとつのテクニックが、過去にアクセスしたURLの履歴を保存することです。URLを入力する際に、入力した内容と履歴に残っているURLの接頭辞を照合することで、完成度の高い選択肢を提供することができます。

## 2.4 Shady Characters
- URLには、比較的小さく、普遍的に安全なアルファベットの文字のみを含めることが許可されている。
- URLは完全でなければなりません。URL の設計者は、URL にバイナリデータや、普遍的に安全なアルファベット以外の文字を含めたいと思う場合があることに気付きました。そこで、安全でない文字を安全な文字にエンコードして転送できるように、エスケープメカニズムを追加しました。

### 2.4.1 The URL Character Set
- URL設計者は、完全性の必要性を認識して、エスケープシーケンスを取り入れました。エスケープシーケンスは、任意の文字値やデータをUS-ASCII文字セットの限定されたサブセットを使ってエンコードすることで、移植性と完全性を実現します。

### 2.4.2 Encoding Mechanisms
- 安全な文字セットの表現の限界を回避するために、安全ではないURLの文字を表現するエンコード方式が考案されました。このエンコーディングでは、安全でない文字を、パーセント記号（%）と、その文字のASCIIコードを表す2つの16進数からなる「エスケープ」表記で単純に表します。

### 2.4.3 Character Restrictions - 文字の制限
- いくつかの文字は、URLの中で特別な意味を持つように予約されています。

### 2.4.4 A Bit More

## 2.5 A sea of schemes
- httpスキーマは、usernameとpasswordがないこと以外は一般的なURLフォーマットと同じ。デフォルトポートは80。
- https
- mailto

## 2.6 the future
- URN
- PURL

# Ch3 HTTP Messages
- HTTP がインターネットの運び屋であるとすれば、HTTP メッセージは物を移動するためのパッケージです。この章では以下を学びます。
  - メッセージの流れ
  - HTTP メッセージの 3 つの部分 (開始行、ヘッダー、エンティティボディ)
  - リクエストメッセージとレスポンスメッセージの違い
  - リクエストメッセージがサポートするさまざまな機能（メソッド）について
  - レスポンスメッセージで返されるさまざまなステータスコード
  - 各種 HTTP ヘッダーの役割 

## 3.1 The Flow of Messages
- HTTPメッセージは、HTTPアプリケーション間で送信されるデータのブロックです。これらのデータブロックは、メッセージの内容や意味を説明するテキストのメタ情報で始まり、その後にオプションのデータが続きます。これらのメッセージは、クライアント、サーバー、およびプロキシの間を流れます。メッセージの方向性を示す用語として、「インバウンド」、「アウトバウンド」、「アップストリーム」、「ダウンストリーム」があります。

### 3.1.1 Messages Commute Inbound to the Origin Server
- HTTPでは、トランザクションの方向性を表すために、インバウンドとアウトバウンドという用語を使用しています。メッセージはオリジンサーバーに向かってインバウンドで送信され、その作業が終わるとユーザーエージェントに向かってアウトバウンドで送信されます（図3-1参照）。

### 3.1.2 Messages Flow Downstream
- HTTP メッセージは川のように流れます。すべてのメッセージは、リクエスト・メッセージかレスポンス・メッセージかにかかわらず、下流に向かって流れます（図3-2参照）。メッセージの送信者は受信者の上流に位置します。

## 3.2 The Parts of a Message
- HTTPメッセージは、フォーマットされたシンプルなデータの塊です。図3-3にその例を示していますのでご覧ください。各メッセージには、クライアントからのリクエストまたはサーバからのレスポンスが含まれています。メッセージは3つの部分から構成されています。メッセージを説明するスタートライン、属性を含むヘッダーのブロック、そしてデータを含むオプションのボディです。
- 開始行とヘッダーは単なるASCIIテキストで、行ごとに区切られています。各行の終わりには、キャリッジリターン（ASCII 13）とラインフィード（ASCII 10）の2文字の行末シーケンスがあります。
- この行末記号は "CRLF "と表記されます。HTTPの行末仕様はCRLFですが、堅牢なアプリケーションでは、ラインフィード文字だけを受け入れるべきであることを指摘しておきます。古いHTTPアプリケーションや壊れたHTTPアプリケーションの中には、キャリッジリターンとラインフィードの両方を送るとは限らないものもあります。
- エンティティボディまたはメッセージボディ（または単に「ボディ」）は、単なるオプションのデータチャンクです。開始行やヘッダーとは異なり、ボディにはテキストやバイナリデータを含めることができ、また空にすることもできます。

### 3.2.1 Message Syntax
- すべてのHTTPメッセージは、「リクエストメッセージ」と「レスポンスメッセージ」の2種類に分類されます。リクエストメッセージはウェブサーバに動作を要求し，レスポンスメッセージはリクエストの結果をクライアントに返します。
```
// リクエストメッセージのフォーマット
<method> <request-URL> <version>
<headers>

<entity-body> 

// レスポンスメッセージのフォーマット
<version> <status> <reason-phrase>
<headers>

<entity-body>
```
- method
  - クライアントがサーバーにリソースに対して実行してほしいアクション。GET"、"HEAD"、"POST "などの単一の単語で構成されます。この章の後半では、メソッドについて詳しく説明します。
- request-URL
  - 要求されたリソースを指定する完全なURL、またはURLのパスコンポーネントです。サーバーと直接やりとりする場合は、リソースへの絶対パスであれば、通常、URLのパスコンポーネントは問題ありません（サーバーは自分自身をURLのホスト／ポートとみなすことができます）。
- version
- status-code
- reason-phrase
  - ステータスコードの数値を人間が読めるようにしたもので、行末までのすべてのテキストで構成される。理由のフレーズは、人間が消費するためだけのものです。
- headers
  - 0個以上のヘッダは、名前の後にコロン(:)、任意のホワイトスペース、値、CRLFが続きます。ヘッダーは空行 (CRLF) で終了し、ヘッダーのリストの終わりと、エンティティボディの始まりを示します。HTTP/1.1 などの一部のバージョンの HTTP では、リクエストやレスポンスのメッセージが有効であるために、特定のヘッダが存在することが要求されます。様々な HTTP ヘッダーについては、この章の後半で説明します。
- entity-body
  - エンティティボディは、任意のデータブロックを含みます。すべてのメッセージにエンティティ・ボディが含まれているわけではないので、メッセージが CRLF で終了することもあります。エンティティについては第 15 章で詳しく説明します。
- HTTP ヘッダは、ヘッダがなくても、エンティティボディがなくても、常に空行(CRLF)で終わるべきであることに注意してください。しかし、これまで多くのクライアントやサーバーは、エンティティボディがない場合、最後のCRLFを（誤って）省略していました。

### 3.2.2 Start Lines

#### 3.2.2.1 リクエスト・ライン
- リクエスト・メッセージは、リソースに対して何かをするようサーバに要求します。リクエスト・メッセージのスタート・ライン（リクエスト・ライン）には、サーバが実行すべき操作を記述したメソッドと、そのメソッドを実行するリソースを記述したリクエストURLが含まれます。また、リクエストラインには、クライアントが使用しているHTTPの方言をサーバーに伝えるHTTPバージョンも含まれています。
- これらのフィールドはすべてホワイトスペースで区切られています。

#### 3.2.2.2 レスポンスライン
- レスポンス・メッセージは、操作によって得られたステータス情報や結果のデータをクライアントに返します。レスポンスメッセージの開始行（レスポンスライン）には、レスポンスメッセージが使用しているHTTPバージョン、数値化されたステータスコード、操作のステータスを説明するテキストの理由フレーズが含まれます。これらのフィールドはすべて、ホワイトスペースで区切られています。

#### 3.2.2.2 メソッド
- メソッドは、リクエストの開始行を開始し、サーバーに何をすべきかを伝えます。たとえば、「GET /specials/saw-blade.gif HTTP/1.0」という行では、メソッドはGETです。
- HTTPの仕様では、一般的なリクエストメソッドのセットが定義されています。例えば、GETメソッドはサーバーからドキュメントを取得し、POSTメソッドはサーバーにデータを送信して処理を行い、OPTIONSメソッドはWebサーバーの一般的な機能や特定のリソースに対するWebサーバーの機能を決定します。
- 表3-1では、これらのうち7つのメソッドについて説明しています。なお、リクエストメッセージにボディがあるメソッドと、ボディのないリクエストがあるメソッドがあります。
- すべてのサーバーが表3-1の7つのメソッドをすべて実装しているわけではありません。また、HTTPは拡張性を重視して設計されているため、他のサーバーがこれらに加えて独自のリクエストメソッドを実装している場合もあります。これらの追加メソッドは、HTTPの仕様を拡張するものであるため、拡張メソッドと呼ばれます。

#### 3.2.2.4 ステータスコード
- ステータスコードは、各応答メッセージの開始行に返されます。ステータスには、数字と人間が読める文字の両方が含まれます。数字はプログラムのエラー処理を容易にし、理由は人間が理解しやすいようになっています。
- それぞれのステータスコードは、3桁の数値コードによってクラス分けされています。200から299までのステータスコードは成功を表す。300から399までのコードは、リソースが移動されたことを示す。400から499までのコードは、クライアントがリクエストで何か間違ったことをしたことを意味します。500から599までのコードは、サーバー上で何か問題が発生したことを意味します。
- 現在のバージョンのHTTPでは、各ステータスカテゴリにいくつかのコードしか定義されていません。プロトコルの進化に伴い、より多くのステータスコードがHTTP仕様で公式に定義される予定です。見覚えのないステータスコードを受け取った場合は、誰かが現在のプロトコルの拡張として定義した可能性があります。そのコードは、そのコードが含まれるクラスの一般的なメンバーとして扱われるべきです。

#### 3.2.2.5 reason phrases
- HTTPの仕様では、理由を示す語句がどのようなものであるべきか、明確なルールは定められていません。

#### 3.2.2.6 Version numbers
- バージョン番号は、HTTP アプリケーションが相互にどのバージョンのプロトコルに準拠しているかを伝える手段となります。バージョン番号は、HTTP を使用するアプリケーションに、互いの能力やメッセージの形式についての手がかりを提供することを目的としています。HTTP バージョン 1.1 のアプリケーションと通信する HTTP バージョン 1.2 のアプリケーションは、1.2 の新機能を使用すべきではないことを知るべきです。なぜなら、それらの機能は、古いバージョンのプロトコルを使用するアプリケーションによって実装されていない可能性が高いからです。

### 3.2.3 Headers
- HTTP ヘッダーフィールドは、リクエストとレスポンスのメッセージに追加の情報を与えます。これらは基本的に、名前と値のペアのリストです。

#### 3.2.3.1 Header classifications
- 各HTTPヘッダーは、名前の後にコロン（：）、任意のホワイトスペース、フィールド値、CRLFというシンプルな構文になっています。

### 3.2.4 Entity Bodies
- HTTPメッセージの第3の部分は、オプションのエンティティボディです。エンティティボディは、HTTPメッセージのペイロードです。エンティティボディは、HTTPメッセージのペイロードであり、HTTPが輸送するために設計されたものです。
- HTTPメッセージは、画像、ビデオ、HTML文書、ソフトウェアアプリケーション、クレジットカード、電子メールなど、さまざまな種類のデジタルデータを伝送することができます。

### 3.2.5 Version 0.9 Messages

## 3.3 Methods
- ここでは、表3-1に示した基本的なHTTPメソッドのいくつかについて詳しく説明します。ただし、すべてのサーバがすべてのメソッドを実装しているわけではありません。HTTP バージョン 1.1 に準拠するためには、サーバーはそのリソースに対して GET と HEAD メソッドのみを実装する必要があります。
- サーバーがこれらのメソッドをすべて実装していたとしても、そのメソッドはほとんどの場合、用途が制限されています。例えば、DELETEやPUT（後述）をサポートするサーバは、誰でもリソースを削除したり保存したりできることを望んでいません。このような制限は、一般的にはサーバーの設定で設定されるため、サイトやサーバーによって異なります。

### 3.3.1 Safe Methods
- HTTP では、安全なメソッドと呼ばれる一連のメソッドが定義されています。GET メソッドと HEAD メソッドは安全であると言われています。つまり、GET メソッドまたは HEAD メソッドを使用した HTTP リクエストの結果、何も起こらないことを意味します。何も起こらないというのは、HTTPリクエストの結果としてサーバー上で何も起こらないということです。
- 例えば、あなたがJoe's Hardwareでオンラインショッピングをしていて、「購入」ボタンをクリックしたとします。このボタンをクリックすると、クレジットカード情報を含むPOSTリクエスト(後述)が送信され、お客様に代わってサーバー上でアクションが実行されます。この場合のアクションとは、お客様のクレジットカードに購入代金が請求されることです。
- 安全なメソッドを使用しても、アクションが実行されないという保証はありません（実際には、ウェブ開発者次第ですが...）。安全なメソッドとは、何らかのアクションが実行される可能性のある安全でないメソッドが使用されている場合に、HTTPアプリケーションの開発者がユーザーに知らせるためのものです。Joe's Hardwareの例では、Webブラウザが警告メッセージを表示し、安全でないメソッドでリクエストを行っていること、その結果、サーバ上で何かが起こる可能性があることを知らせます（例：クレジットカードに課金される）。

### 3.3.2 GET
- GETは最も一般的なメソッドです。通常、サーバーにリソースの送信を依頼する際に使用されます。HTTP/1.1では、サーバーがこのメソッドを実装する必要があります。

### 3.3.3 HEAD
- ヘッダーだけを返すメソッド。下記用途で使う。
  - リソースを取得せずに、そのリソースについて調べる（例えば、タイプを決定する）。
  - レスポンスのステータスコードを見て、オブジェクトが存在するかどうかを確認する。
  - ヘッダを見て、リソースが変更されたかどうかを確認する。

- サーバの開発者は、返されるヘッダが、GETリクエストが返すヘッダと同じであることを確認する必要があります。HTTP/1.1 に準拠するためには、HEAD メソッドも必要です。

### 3.3.4 PUT
- PUTメソッドは、GETがサーバーからドキュメントを読み取るのとは逆に、サーバーにドキュメントを書き込むものである。パブリッシングシステムの中には、PUTを使ってWebページを作成し、Webサーバーに直接インストールできるものもある（図3-9参照）。
- PUTメソッドのセマンティクスは、サーバーがリクエストのボディを受け取り、それを使ってリクエストされたURLで名付けられた新しいドキュメントを作成するか、そのURLがすでに存在する場合は、ボディを使ってそれを置き換えるというものです。PUTはコンテンツの変更を可能にするため、多くのウェブサーバーではPUTを実行する前にパスワードによるログインを要求しています。パスワード認証については、第12章で詳しく説明しています。

### 3.3.5 POST
- OSTメソッドは、入力データをサーバーに送信するために設計されました *。 実際には、HTMLフォームをサポートするためによく使用されます。入力されたフォームからのデータは通常、サーバーに送信され、サーバーはそれを必要な場所（例えば、サーバー・ゲートウェイ・プログラムに送信し、それを処理します）に転送します。
- * POSTは、データをサーバーに送信するために使用されます。PUTは、サーバー上のリソース（ファイルなど）にデータを預けるために使用されます。

### 3.3.6 TRACE
- クライアントがリクエストを行う際、そのリクエストは、ファイアウォール、プロキシ、ゲートウェイ、その他のアプリケーションを経由しなければならない場合があります。これらのそれぞれが、オリジナルのHTTPリクエストを変更する機会を持っています。TRACEメソッドでは、クライアントは、リクエストが最終的にサーバに到達したときの様子を確認することができます。
- TRACEリクエストは、送信先のサーバで「ループバック」診断を開始します。最終目的地のサーバは、受け取ったバージンリクエストメッセージを本文に含んだTRACEレスポンスを返信します。クライアントは、元のメッセージが、HTTPアプリケーションのリクエスト／レスポンスチェーンに沿って、どのように、あるいはどのように変更されたかを確認することができます（図3-11参照）。
- TRACEメソッドは、主に診断のために使用されます。つまり、リクエストが意図したとおりにリクエスト／レスポンスチェーンを通過しているかどうかを検証します。また、プロキシや他のアプリケーションがリクエストに与える影響を確認するのにも適したツールです。TRACEは診断に適していますが、介在するアプリケーションが異なるタイプのリクエスト（異なるメソッド-GET、HEAD、POSTなど）を同じように扱うことを前提としているという欠点があります。多くの HTTP アプリケーションは、メソッドに応じて異なる処理を行います。一般的に、TRACEリクエストをどのように処理するかについては、介在するアプリケーションが判断します。
- TRACEリクエストでは、エンティティボディを送信することはできません。TRACEレスポンスのエンティティボディには、応答サーバが受け取ったリクエストがそのまま含まれています。

### 3.3.7 OPTIONS
- OPTIONSメソッドは、ウェブサーバがサポートしている様々な機能について、サーバに問い合わせるものです。一般的に、あるいは特定のリソースに対してどのようなメソッドをサポートしているかをサーバーに尋ねることができます。(サーバーによっては、特定の種類のオブジェクトに対してのみ特定の操作をサポートしている場合もあります)。
- これにより、クライアントアプリケーションは、様々なリソースに実際にアクセスすることなく、どのようにアクセスするのが最適かを判断することができます。

### 3.3.8 DELETE
- DELETEメソッドは、イメージ通り、リクエストURLで指定されたリソースの削除をサーバーに依頼するものです。しかし、クライアントアプリケーションは、その削除が実行されることを保証されていません。これは、HTTPの仕様では、サーバーがクライアントに断りなくリクエストを上書きすることができるためです。

### 3.3.9 Extension Methods
- HTTPは、新しい機能が旧来のソフトウェアの障害とならないように、フィールド拡張可能な設計となっています。
- 注意すべき点は、すべての拡張メソッドが正式な仕様で定義されているわけではないということです。拡張メソッドを定義しても、ほとんどの HTTP アプリケーションでは理解できない可能性があります。同様に、他のアプリケーションが使用している、理解できない拡張メソッドにHTTPアプリケーションが遭遇する可能性もあります。
- このような場合には、拡張メソッドに対して寛容であることが最善です。プロキシは、エンドツーエンドの動作を壊すことなく、未知のメソッドを持つメッセージを下流のサーバに中継することが可能であれば、それを試みるべきです。そうでない場合は、501 Not Implementedのステータスコードで応答すべきです。

## 3.4 Status Codes
- HTTPステータスコードは、前述の表3-2に示すように、大きく5つのクラスに分類されます。ここでは、5つのクラスごとにHTTPステータスコードをまとめています。

### 3.4.1 100-199: Informational Status Codes

### 3.4.2 200-299: Success Status Codes

### 3.4.3 300-399: Redirection Status Codes
- リダイレクションのステータスコードは、クライアントが興味を持っているリソー スのために別の場所を使用するように伝えるか、あるいはコンテンツの代わりに別の 応答を提供する。リソースが移動した場合には、リダイレクト・ステータス・コードとオプションのLocationヘッダを送信して、リソースが移動したことと、現在どこで見つけられるかをクライアントに伝えることができます（図3-14参照）。これにより、ブラウザは人間のユーザに迷惑をかけることなく、透過的に新しい場所に移動することができます。
- リダイレクトステータスコードの中には、アプリケーションのリソースのローカルコピーをオリジンサーバーで検証するために使用できるものがあります。例えば、HTTPアプリケーションは、リソースのローカルコピーがまだ最新であるかどうか、またはリソースがオリジンサーバーで変更されているかどうかを確認することができます。確認して変更がない旨のレスポンス（304 Not Modified）が返ってきたら、ローカルコピー（キャッシュなど）を表示する。
- 304
  - クライアントは、リクエストヘッダーを含めることで、リクエストを条件付きで行うことができます。条件付きヘッダの詳細については、第3章を参照してください。クライアントが、リソースが最近変更されていない場合のGETなど、条件付きのリクエストを行う場合、リソースが変更されていないことを示すためにこのコードが使用されます。

### 3.4.4 400-499: Client Error Status Codes

### 3.4.5 500-599: Server Error Status Codes

## 3.5 Headers
- ヘッダーとメソッドが連携して、クライアントとサーバーの動作を決定します。このセクションでは、標準的な HTTP ヘッダーと、HTTP/1.1 仕様 (RFC 2616) で明示的に定義されていないいくつかのヘッダーの目的について、簡単に説明します。付録Cでは、これらすべてのヘッダーの詳細をまとめています。
- ヘッダーには、メッセージの種類ごとに固有のものと、リクエストメッセージとレスポンスメッセージの両方に情報を提供する、より一般的な目的のものとがあります。ヘッダーには、大きく分けて5つのクラスがあります。
  - General headers
  - Request headers
  - Response headers
  - Entity headers
    - エンティティ・ヘッダとは、エンティティ・ボディを扱うヘッダのことです。例えば、エンティティヘッダは、エンティティボディ内のデータのタイプを伝えることができます。
  - Extension headers

### 3.5.1 General Headers

### 3.5.2 Request Headers

### 3.5.3 Response Headers

### 3.5.4 Entity Headers
- HTTP メッセージのペイロードを記述するヘッダは数多くあります。リクエストメッセージにもレスポンスメッセージにもエンティティが含まれるため、これらのヘッダはどちらのタイプのメッセージにも含まれます。エンティティヘッダーは、オブジェクトのタイプに関する情報から、リソース上で実行可能な有効なリクエストメソッドまで、エンティティとそのコンテンツに関する幅広い情報を提供します。一般的に、エンティティヘッダーは、メッセージの受信者に何を扱っているかを伝えます。表3-21は、エンティティ情報ヘッダーの一覧です。

#### 3.5.4.1 Content headers
- コンテンツヘッダーは、エンティティのコンテンツに関する特定の情報を提供し、そのタイプ、サイズ、その他処理に役立つ情報を明らかにします。例えば、Webブラウザは返されたコンテンツタイプを見て、オブジェクトをどのように表示するかを知ることができます。表3-22は、さまざまなコンテンツヘッダーの一覧です。

#### 3.5.4.2 Entity caching headers
- 一般的なキャッシュヘッダは、いつ、どのようにキャッシュするかを指示します。エンティティキャッシュヘッダは、キャッシュされるエンティティに関する情報を提供します。たとえば、リソースのキャッシュされたコピーがまだ有効であるかどうかを検証するために必要な情報や、キャッシュされたリソースがいつ無効になるかをよりよく見積もるためのヒントを提供します。
- 第7章では、HTTPリクエストとレスポンスのキャッシュについて詳しく説明します。これらのヘッダは、そこで再び目にすることになるでしょう。表3-23は、エンティティキャッシュヘッダーの一覧です。

# Ch4 Connection Management
- HTTP仕様では、HTTPメッセージについてはよく説明されていますが、HTTPメッセージが流れる際の重要な配管であるHTTPコネクションについてはあまり説明されていません。もしあなたがHTTPアプリケーションを書いているプログラマーならば、HTTPコネクションの裏と表、そしてその使い方を理解する必要があります。
- HTTP 接続の管理は、ちょっとしたブラックアートのようなもので、出版された文献からだけでなく、実験や見習いからも学ぶことができます。この章では、以下のことを学びます。
  - HTTPのTCPコネクションの使い方
  - TCP 接続の遅延、ボトルネック、詰まり
  - HTTP の最適化 (並列接続、キープアライブ接続、パイプライン接続など)
  - 接続を管理する上での注意点 

## 4.1 TCP Connections
- 世界中のHTTP通信は、世界中のコンピュータやネットワーク機器で使用されているパケットスイッチネットワークプロトコルであるTCP/IPを介して行われています。クライアントアプリケーションは、世界中のあらゆる場所で動作しているサーバーアプリケーションとのTCP/IP接続を開くことができます。いったん接続が確立されると、クライアントとサーバーのコンピュータ間で交換されたメッセージが失われたり、破損したり、順番が狂って受信されることはありません。

### 4.1.1 TCP Reliable Data Pipes
- HTTP接続の正体は、TCP接続と、その使い方に関するいくつかのルールに過ぎません。TCP接続は、インターネットの信頼できる接続です。データを正確かつ迅速に送信するためには、TCPの基本的な知識が必要です。
- TCP は HTTP に信頼性の高いビットパイプを提供します。TCPコネクションの片側に詰め込まれたバイトは、正しい順序で反対側に出てきます（図4-2参照）。

### 4.1.2 TCP Streams Are Segmented and Shipped by IP Packets - TCPストリームを分割してIPパケットで出荷する
- TCPは、IPパケット（またはIPデータグラム）と呼ばれる小さな塊でデータを送信します。このように、HTTP は「HTTP over TCP over IP」という「プロトコル・スタック」の最上位の層です（図 4-3a）。安全なバージョンであるHTTPSは、HTTPとTCPの間に暗号化層（TLSまたはSSLと呼ばれる）を挿入します（図4-3b）。
- HTTP は、メッセージを送信する際に、開いている TCP コネクションを介して、メッセージデータのコンテンツを順にストリームします。TCP は、データのストリームを受け取り、セグメントと呼ばれる塊に切り分け、IP パケットと呼ばれるエンベロープ内でセグメントをインターネット上に転送します (図 4-4 参照)。これらはすべてTCP/IPソフトウェアによって処理され、HTTPプログラマーには何も見えません。
各TCPセグメントは、IPパケットによって、あるIPアドレスから別のIPアドレスへと運ばれます。これらのIPパケットには、以下のものが含まれます。

  - IPパケットヘッダ（通常20バイト
  - TCPセグメントのヘッダ（通常20バイト
  - TCPデータのチャンク（0バイトまたはそれ以上

- IPヘッダには、送信元と送信先のIPアドレス、サイズ、その他のフラグが含まれています。TCPセグメントヘッダには、TCPポート番号、TCP制御フラグ、データの順序付けや整合性チェックに使用する数値などが含まれます。

### 4.1.3 Keeping TCP Connections Straight
- コンピュータは、一度に複数のTCPコネクションを開いていることがあります。TCPは、これらの接続をポート番号で管理しています。ポート番号は、社員の電話の内線番号のようなものです。会社の電話番号がフロントデスクにつながり、内線番号が適切な従業員につながるように、IPアドレスが適切なコンピュータにつながり、ポート番号が適切なアプリケーションにつながるのです。TCP接続は4つの値で区別されます。

```
<source-IP-address, source-port, destination-IP-address,
destination-port>
```

- これらの4つの値を組み合わせることで、接続を一意に定義します。2つの異なるTCPコネクションは、4つのアドレス構成要素すべてに同じ値を持つことはできません（ただし、異なるコネクションが、いくつかの構成要素に同じ値を持つことは可能です）。図4-5では、4つのコネクションがあります。各ポートの関連情報を表4-1に示します。

### 4.1.4 Programming with TCP Sockets
- オペレーティングシステムは、TCP接続を操作するためのさまざまな機能を提供しています。
- ソケットAPIは、TCPとIPの詳細をすべてHTTPプログラマから隠しています。
- ソケット API では、TCP エンドポイントのデータ構造を作成したり、これらのエンドポイントをリモート・サーバの TCP エンドポイントに接続したり、データ・ストリームを読み書きしたりすることができます。TCP APIでは、基礎となるネットワーク・プロトコルのハンドシェイクや、IPパケットとの間で行われるTCPデータ・ストリームのセグメント化と再構成などの詳細はすべて隠されています。

### 4.2 TCP Performance Considerations
- HTTP は TCP に直接レイヤリングされているため、HTTP トランザクションのパフォーマンスは、基礎となる TCP 配管のパフォーマンスに決定的に依存します。このセクションでは、これらのTCP接続のパフォーマンスに関するいくつかの重要な検討事項を紹介します。TCP の基本的なパフォーマンス特性を理解することで、HTTP の接続最適化機能をよりよく理解することができ、よりパフォーマンスの高い HTTP アプリケーションを設計、実装することができます。

### 4.2.1 HTTP Transaction Delays
- 基本的に、ほとんどのHTTPの遅延はTCPネットワークの遅延が原因である。
  - DNSルックアップ　ローカルにキャッシュされていない場合、数十秒かかることもある。
  - TCPコネクション確立　新しいTCP接続ごとに発生する。通常はせいぜい1,2秒だが、HTTPトランザクションが多数発生すると累積する。
  - TCP接続確立後のリクエスト、レスポンスがインターネット上を移動し、処理されるまでの時間がかかる。

### 4.2.2 Performance Focus Areas
- このセクションでは、HTTPプログラマに影響を与える最も一般的なTCP関連の遅延について、その原因やパフォーマンスへの影響などを説明します。

  - TCP 接続設定のハンドシェイク
  - TCP スロースタート輻輳制御
  - データ集約のためのNagleのアルゴリズム
  - TCPの遅延確認応答アルゴリズムによるピギーバック確認応答
  - TIME_WAITの遅延とポートの枯渇

### 4.2.3 TCP Connection Handshake Delays

### 4.2.4 Delayed Acknowledgments

### 4.2.5 TCP Slow Start

### 4.2.6 Nagle's Algorithm and TCP_NODELAY

### 4.2.7 TIME_WAIT Accumulation and Port Exhaustion

## 4.3 HTTP Connection Handling
- この章の残りの部分では、接続を操作して最適化するための HTTP 技術について説明します。まず、誤解されがちですが、HTTP接続管理の重要な部分であるHTTP Connectionヘッダーについて説明します。次に、HTTP の接続最適化技術について説明します。

### 4.3.1 The Oft-Misunderstood Connection Header - 意外と知られていない接続用ヘッダー
- HTTPでは、クライアントと最終的なオリジンサーバーの間に、一連のHTTP中間体が存在します。(プロキシ、キャッシュなど)があります。HTTPメッセージは、クライアントから仲介装置を経由して、オリジンサーバー（またはその逆）にホップ・バイ・ホップで転送されます。
- 場合によっては、隣接する2つのHTTPアプリケーションが、共有された コネクションにオプションセットを適用したい場合があります。HTTP Connection ヘッダーフィールドには、他のコネクションに伝搬されないコネクションのオプションを指定するコネクショントークンがコンマで区切られて表示されます。例えば、次のメッセージを送信した後に閉じなければならないコネクションは、Connection: closeで示されます。Connectionヘッダーには、3つの異なるタイプのトークンが含まれているため、混乱することがあります。

  - HTTP ヘッダーフィールド名（この接続のみに関連するヘッダーのリスト）
  - 任意のトークンの値：この接続の非標準的なオプションを示します。
  - closeという値は、接続が完了したときに永続的な接続が閉じられることを示します。

- ConnectionトークンにHTTPヘッダーフィールド名が含まれている場合、そのヘッダーフィールドには接続固有の情報が含まれており、転送してはいけません。Connection ヘッダーに記載されているヘッダーフィールドは、メッセージを転送する前に削除する必要があります。ホップバイホップのヘッダー名を Connection ヘッダーに置くことは、「ヘッダーの保護」として知られています。これは、Connection ヘッダーが、ローカルヘッダーの偶発的な転送から保護するためです。その例を図4-9に示します。
- HTTP アプリケーションが Connection ヘッダーを持つメッセージを受信すると、受信者は、送信者が要求したすべてのオプションを解析して適用します。その後、Connection ヘッダーと Connection ヘッダーに記載されているすべてのヘッダーを削除してから、メッセージを次のホップに転送します。さらに、Connectionヘッダーの値としてリストアップされていないが、プロキシされてはならないいくつかのhop-by-hopヘッダーがあります。これらには、Proxy-Authenticate、Proxy-Connection、Transfer-Encoding、および Upgrade が含まれます。Connectionヘッダーの詳細については、付録Cを参照してください。

### 4.3.2 Serial Transaction Delays
- TCPのパフォーマンスの遅延は、接続が単純に管理されている場合（シリアルローディング）、累積することがあります。
- HTTP接続のパフォーマンスを向上させるために、いくつかの最新の技術があります。次のいくつかのセクションでは、そのような技術のうち4つについて説明します。
  - Parallel connections：複数のTCPコネクションで同時にHTTPリクエストを行う 
  - Persistent connections：TCPコネクションを再利用することで、接続/終了時の遅延を解消
  - Pipelined connections：共有されたTCPコネクション上で、同時にHTTPリクエストを実行
  - Multiplexed connections：リクエストとレスポンスのチャンクをインターリーブする（実験的）

## 4.4 Parallel Connections
- HTTPでは、図4-11に示すように、クライアントが複数のコネクションを開き、複数のHTTPトランザクションを並行して実行することができます。この例では，4つの埋め込み画像が並行して読み込まれ，各トランザクションが独自のTCPコネクションを取得しています。

### 4.4.1 Parallel Connections May Make Pages Load Faster
- 埋め込みオブジェクトで構成された複合ページは、単一の接続によるデッドタイムと帯域幅の制限を利用すると、より速く読み込まれることがあります。

### 4.4.2 Parallel Connections Are Not Always Faster
- しかし、並列接続の方が速いといっても、必ずしも速いとは限りません。クライアントのネットワーク帯域幅が狭い場合（例えば、28.8Kbpsのモデムでインターネットに接続しているブラウザなど）、ほとんどの時間がデータの転送だけに費やされてしまうことがあります。このような状況では、高速なサーバーへの1つのHTTPトランザクションが、利用可能なモデムの帯域幅のすべてを簡単に消費してしまいます。複数のオブジェクトを並行してロードする場合、各オブジェクトは限られた帯域幅を奪い合うことになるため、各オブジェクトのロード速度は比例して遅くなり、パフォーマンス上の利点はほとんどありません
- また、多数のオープンコネクションは大量のメモリを消費し、それ自体がパフォーマンス問題を引き起こす可能性があります。複雑なWebページでは、数十から数百のオブジェクトが埋め込まれている場合があります。クライアントは何百もの接続を開くことができるかもしれませんが、ウェブサーバーは他の多くのユーザーのリクエストを同時に処理していることが多いため、その状況を望むサーバーはほとんどいないでしょう。100人のユーザーが同時に100個の接続を開くと、サーバーには1万個の接続の負担がかかります。これは、サーバーの大幅なスローダウンの原因となります。このような状況は、高負荷のプロキシについても同様です。実際には、ブラウザは並列接続を使用しますが、並列接続の総数は少数（多くは4）に制限されています。サーバーは、特定のクライアントからの過剰な接続を自由に閉じることができます。

### 4.4.3 Parallel Connections May "Feel" Faster
- なるほど、並列接続をすれば必ずページの読み込みが速くなるわけではありません。しかし、実際にページの転送速度が速くならなくても、先に述べたように、並列接続によって、ユーザーがページの読み込みが速くなったと感じることはよくあります。人間は、たとえストップウォッチで実際にページのダウンロード時間の合計が遅くなったとしても、画面全体でたくさんのアクションが行われていると、ウェブページの読み込みが速くなったと感じるのです 

## 4.5 Persistent Connections
- ウェブクライアントは、しばしば同じサイトへの接続を開きます。例えば、Webページに埋め込まれている画像のほとんどは、同じWebサイトから来ていることが多く、他のオブジェクトへのハイパーリンクのかなりの数が同じサイトを指しています。このように、あるサーバーにHTTPリクエストを開始したアプリケーションは、近い将来、そのサーバーにさらにリクエストを行う可能性があります（例えば、インライン画像を取得するために）。この性質をサイトロカリティといいます。
- このような理由から、HTTP/1.1 (およびHTTP/1.0の拡張版) では、HTTPデバイスが、トランザクションが完了した後もTCPコネクションを開いたままにしておき、将来のHTTPリクエストに既存のコネクションを再利用できるようになっています。トランザクションが完了した後も開かれているTCPコネクションは、持続的なコネクションと呼ばれます。非持続的な接続は、各トランザクションの後に閉じられます。持続的な接続は、クライアントまたはサーバのいずれかが接続を閉じると決定するまで、トランザクションを超えて開かれたままになります。
- 対象となるサーバーに対してすでにオープンしているアイドル状態の持続的な接続を再利用することで、遅い接続設定を回避することができます。また、すでに開いている接続を利用することで、開始時に時間のかかる輻輳適応フェーズを回避でき、より高速なデータ転送が可能になります。

### 4.5.1 Persistent Versus Parallel Connections
- 並列接続は速いが欠点もあった。
  - 各トランザクションが新しいコネクションを開閉するため、時間と帯域幅を消費する
  - それぞれの新しいコネクションはTCPスロースタートのためにパフォーマンスが低下する
  - 開いている並列接続数には実用上の制限がある
- 持続的な接続は、並列接続に比べていくつかの利点があります。接続確立の遅延とオーバーヘッドを減らし、接続を調整済みの状態に保ち、潜在的なオープンコネクションの数を減らすことができます。しかし、持続的接続は慎重に管理する必要があります。そうしないと、多数のアイドル接続が蓄積され、ローカルリソースやリモートクライアントおよびサーバーのリソースを消費してしまう可能性があります。
- 持続的な接続は、並列接続と組み合わせて使用すると最も効果的です。現在、多くのウェブアプリケーションでは、少数の並列接続を開き、それぞれを持続的に使用しています。持続的接続には、古い HTTP/1.0+ の "keep-alive "接続と、新しい HTTP/1.1 の "persistent "接続の2種類があります。次のいくつかのセクションでは、この2つのタイプについて説明します。

### 4.5.2 HTTP/1.0+ Keep-Alive Connections
- 多くのHTTP/1.0ブラウザやサーバーは、（1996年頃から）キープアライブ接続と呼ばれる初期の実験的なタイプの持続的な接続をサポートするように拡張されました。この初期の持続的接続には、相互運用性の設計上の問題があり、後のHTTP/1.1の改訂で修正されましたが、多くのクライアントやサーバーは今でもこの初期のキープアライブ接続を使用しています。
- 図4-13は、シリアル接続による4つのHTTPトランザクションのタイムラインと、単一の持続的接続による同じトランザクションのタイムラインを比較したものです。タイムラインが短縮されているのは、コネクトとクローズのオーバーヘッドが取り除かれているからです[16]。

### 4.5.3 Keep-Alive Operation
- Keep-alive は非推奨で、現在の HTTP/1.1 仕様では文書化されていません。しかし、Keep-alive ハンドシェイクはブラウザやサーバで比較的よく使われていますので、HTTP 実装者は相互運用の準備をしておく必要があります。ここでは、キープアライブの動作について簡単に説明します。keep-alive ハンドシェイクについてのより詳細な説明は、古いバージョンの HTTP/1.1 仕様 (RFC 2068 など) を参照してください。
- HTTP/1.0 のキープアライブ接続を実装するクライアントは、Connection:Keep-Alive リクエストヘッダを含めることで、接続を維持することを要求できます。
- サーバは、次のリクエストに対して接続を維持することを希望する場合は、同じヘッダをレスポンスに含めて応答します（図4-14参照）。応答に Connection: keep-alive ヘッダがない場合、クライアントは、サーバが keep-alive をサポートしておらず、応答メッセージが返送された時点でサーバが接続を終了するものと想定します。

### 4.5.4 Keep-Alive Options
- keep-aliveヘッダーは、接続を維持するための単なる要求であることに注意してください。クライアントとサーバーは、keep-aliveセッションが要求されても、それに同意する必要はありません。クライアントとサーバーは、アイドル状態のkeep-alive接続をいつでも閉じることができ、keep-alive接続で処理されるトランザクションの数を自由に制限することができます。
Keep-Aliveの動作は、Keep-Alive一般ヘッダで指定されるコンマ区切りのオプションによって調整できます。

  - timeoutパラメータは、Keep-Aliveレスポンスヘッダで送信されます。このパラメータは、サーバが接続を維持する可能性のある時間を推定します。これは保証されるものではありません。
  - max パラメーターは、Keep-Alive 応答ヘッダーで送信されます。これは、サーバーがあと何回のHTTPトランザクションの間、接続を維持できるかを示します。これは保証されるものではありません。
  - Keep-Alive ヘッダーは、主に診断やデバッグを目的とした、任意の未処理属性もサポートしています。構文は name [= value] です。

- Keep-Alive ヘッダーは完全にオプションですが、Connection.Keep-Alive も存在する場合にのみ許可されます。

### 4.5.5 Keep-Alive Connection Restrictions and Rules

### 4.5.6 Keep-Alive and Dumb Proxies
- ウェブクライアントのConnection: Keep-Aliveヘッダーは、クライアントから出て行く1つのTCPリンクだけに影響を与えることを目的としています。これが、「コネクション」ヘッダーと名付けられた理由です。クライアントがWebサーバーと通信している場合、クライアントはConnection: Keep-Aliveヘッダを送信して、キープアライブを希望することをサーバに伝えます。
- サーバーは、キープアライブをサポートしている場合はConnection: Keep-Aliveヘッダーを送り返し、サポートしていない場合は送らない。

#### 4.5.6.1 The Connection header and blind relays
- 問題はプロキシにあります。特に、Connectionヘッダを理解しておらず、プロキシ処理を行う前にこのヘッダを削除する必要があることを知らないプロキシです。古いプロキシやシンプルなプロキシの多くは、Connectionヘッダを特別に処理することなく、あるコネクションから別のコネクションへとバイトをトンネリングするブラインドリレーとして動作します。
- プロキシがConnectionヘッダーを知らないことにより、keep-alive接続をしようとするとハングアップが起こる。

#### 4.5.6.2 Proxies and hop-by-hop headers
- このようなプロキシのミスコミュニケーションを避けるために、最近のプロキシはConnectionヘッダーやConnectionの値の中に名前が出てくるヘッダーを決してプロキシしてはいけません。つまり、プロキシが Connection: Keep-Aliveヘッダを受け取った場合、プロキシはConnectionヘッダもKeep-Aliveという名前のヘッダもプロキシしてはいけません。
- さらに、Connectionヘッダーの値としてリストされていないが、プロキシしたり、キャッシュ応答として提供してはならないいくつかのhop-by-hopヘッダーがある。これには、ProxyAuthenticate、Proxy-Connection、Transfer-Encoding、および Upgrade が含まれます。詳細については、4.3.1項を参照してください。

### 4.5.7 The Proxy-Connection Hack
- ネットスケープ社のブラウザとプロキシの実装者たちは、ブラインドリレーの問題を解決するために、すべてのウェブアプリケーションが高度なバージョンのHTTPをサポートする必要のない、巧妙な回避策を提案した。この回避策では、Proxy-Connectionと呼ばれる新しいヘッダを導入し、クライアントの直後に介在する単一のブラインドリレーの問題を解決しましたが、他のすべての状況を解決することはできませんでした。
- Proxy-Connectionは、最近のブラウザではプロキシが明示的に設定されている場合に実装され、多くのプロキシが理解しています。ダムプロキシが問題になるのは、Connection:Keep-Aliveなどのhop-by-hopヘッダをやみくもに転送するからだということです。
- ホップバイホップヘッダは、その単一の特定の接続にのみ関連しており、転送してはいけません。これは、転送されたヘッダが下流のサーバによって、プロキシ自身が接続を制御するための要求であると誤解された場合に問題となります。
- ネットスケープの回避策では、ブラウザは公式にサポートされている有名なConnectionヘッダーの代わりに、非標準のProxy-Connection拡張ヘッダーをプロキシに送信します。プロキシがブラインド・リレーの場合、プロキシは無意味なProxy-Connectionヘッダをwebサーバに中継し、webサーバはそのヘッダを無害に無視します。しかし、プロキシがスマートプロキシ(持続的な接続ハンドシェイクを理解できる)であれば、無意味なProxy-ConnectionヘッダをConnectionヘッダに置き換え、それをサーバに送信することで望ましい効果を得ることができます。
- この方式は、クライアントとサーバーの間にプロキシが1台しかない場合に有効だが、ダムプロキシの両側にスマートプロキシがある場合、再び問題が出てくる。
- さらに、ファイアウォール、インターセプトキャッシュ、リバースプロキシサーバアクセラレータなど、「見えない」プロキシがネットワーク上に登場することが非常に多くなっています。これらのデバイスはブラウザからは見えないため、ブラウザはProxy-Connectionヘッダを送信しません。透過的なWebアプリケーションでは、持続的な接続を正しく実装することが重要です。

### 4.5.8 HTTP/1.1 Persistent Connections
- HTTP/1.1では、キープアライブ接続のサポートを段階的に廃止し、代わりに持続的接続と呼ばれる改良された設計を採用しました。持続的接続の目的は keep-alive 接続と同じですが、メカニズムの動作が改善されています。
- HTTP/1.0+ のキープアライブ接続とは異なり、HTTP/1.1 の持続的な接続はデフォルトで有効です。
- HTTP/1.1 は、他に指示がない限り、すべての接続が持続的であると仮定します。HTTP/1.1 アプリケーションは、トランザクションの完了後に接続を閉じることを示すために、メッセージに Connection: close ヘッダを明示的に追加する必要があります。これは、キープアライブ接続が任意であるか、完全にサポートされていなかった以前のバージョンの HTTP プロトコルとの大きな違いです。
- HTTP/1.1 クライアントは、レスポンスに Connection: close ヘッダーが含まれていない限り、レスポンス後も HTTP/1.1 コネクションが開いたままであることを想定します。しかし、クライアントやサーバーは、いつでもアイドル状態のコネクションを閉じることができます。Connection: close を送信しないことは、サーバーがコネクションを永遠に開いたままにすることを約束するものではありません。

### 4.5.9 Persistent Connection Restrictions and Rules
- Connection: close request headerを送信した後、クライアントはそのコネクション上でそれ以上のリクエストを送信することはできません。
- クライアントがそのコネクション上で他のリクエストを送りたくない場合は、最終リクエストの中でConnection: close request headerを送るべきです。
- コネクション上のすべてのメッセージが、正しく自己定義されたメッセージ長を持つ場合のみ、コネクションを持続させることができます。すなわち、エンティティボディが正しい Content-Length を持つか、chunked transfer エンコーディングでエンコードされていなければなりません。
- HTTP/1.1 プロキシは、クライアントとサーバーとの間の永続的な接続を別々に管理しなければならず、各永続的な接続は単一のトランスポートホップに適用される。
- HTTP/1.1 プロキシサーバーは、クライアントの能力について何かを知らない限り、HTTP/1.0 クライアントとの持続的な接続を確立すべきではありません (Connection ヘッダを転送する古いプロキシの問題のため)。
クライアントの能力を知らない限り、HTTP/1.0クライアントとの持続的な接続を確立してはいけません。これは実際には難しく、多くのベンダーがこのルールを曲げています。
- Connection ヘッダの値にかかわらず、HTTP/1.1 デバイスはいつでも接続を閉じることができますが、サーバーはメッセージ送信の途中で閉じることは避け、閉じる前に少なくとも 1 つのリクエストに応答する必要があります。
- HTTP/1.1 アプリケーションは、非同期のクローズから回復できなければなりません。クライアントは、蓄積されるような副作用がない限り、リクエストを再試行するべきです。
- クライアントは、レスポンス全体を受け取る前にコネクションがクローズした場合、リクエストが繰り返されると副作用が発生する可能性がない限り、リクエストを再試行する準備をしなければなりません。
- シングルユーザーのクライアントは、サーバーの過負荷を防ぐために、どのサーバーやプロキシに対しても最大で2つの持続的な接続を維持する必要があります。プロキシは、同時ユーザをサポートするためにサーバへのより多くの 接続を必要とするかもしれないので、サーバにアクセスしようとする ユーザがN人いる場合、プロキシはいかなるサーバまたは親プロキシに 対しても最大で2N個の接続を維持すべきである。

## 4.6 Pipelined Connections
- HTTP/1.1では、持続的な接続におけるオプションのリクエストパイプライニングが可能です。これは、キープアライブ接続よりもさらにパフォーマンスが向上します。複数のリクエストを、レスポンスの到着前に待ち受けることができます。最初のリクエストが地球の反対側にあるサーバーにネットワークを介してストリーミングされている間に、2番目と3番目のリクエストが進行します。これにより、ネットワークのラウンドトリップを減らすことができ、高レイテンシーのネットワーク環境でのパフォーマンスを向上させることができます。
- 図4-18a-cは、持続的な接続によってTCP接続の遅延を解消し、パイプライン化されたリクエスト（図4-18c）によって転送の遅延を解消する方法を示しています。
- パイプラインにはいくつかの制限があります。
  - HTTPクライアントは、接続が持続することを確認するまでパイプラインを使用してはいけません。
  - HTTP レスポンスは、リクエストと同じ順序で返されなければなりません。HTTP メッセージはシーケンス番号でタグ付けされていないため、レスポンスを順不同で受信した場合、レスポンスとリクエストを一致させる方法はありません。
  - HTTP クライアントは、接続がいつでも閉じられるように準備しておく必要があります。 HTTP クライアントは、いつでも接続を閉じることができるように準備し、終了しなかったパイプライン化されたリクエストをやり直す準備をしておく必要があります。クライアントが永続的な接続を開き、すぐに10個のリクエストを発行した場合、サーバーは例えば5個のリクエストだけを処理した後に自由に接続を閉じることができます。残りの5つのリクエストは失敗します。クライアントは、このような早すぎるクローズを処理して、リクエストを再発行することを望まなければなりません。
  - HTTP クライアントは、副作用のあるリクエスト（POST など）をパイプラインで処理すべきではありません。一般に、エラー時にパイプライン化すると、クライアントはパイプライン化された一連のリクエストのうち、どのリクエストがサーバーで実行されたかを知ることができません。POST のような非一時的なリクエストは安全に再試行できないため、エラー状態では一部のメソッドが実行されないというリスクがあります。

## 4.7 The Mysteries of Connection Close
- 接続管理、特にいつ、どのように接続を閉じるかということは、HTTPの実用的なブラックアートの1つです。この問題は、多くの開発者が最初に認識しているよりも微妙なもので、この問題について書かれたものはほとんどありません。

### 4.7.1 "At Will" Disconnection
- HTTP クライアント、サーバ、プロキシは、いつでも TCP トランスポート接続を閉じることができます。コネクションは通常、メッセージの最後で閉じられますが、エラー状態の時には、ヘッダー行の途中やその他の奇妙な場所で閉じられることがあります。
- この状況は、パイプライン化された持続的な接続でよく見られます。HTTPアプリケーションは、任意の期間後に永続的な接続を閉じることができます。例えば、持続的な接続がしばらくアイドル状態になった後、サーバーはその接続をシャットダウンすることができます。
- しかし、サーバーは、「アイドル」接続がサーバーによってシャットダウンされている間に、回線の反対側にいるクライアントがデータを送信しようとしていなかったことを確実に知ることはできません。これが起こると、クライアントはリクエストメッセージを書いている最中に接続エラーを確認します。

### 4.7.2 Content-Length and Truncation
- 各 HTTP レスポンスには、レスポンスボディのサイズを示す正確な Content-Length ヘッダーが必要です。古い HTTP サーバーの中には、データの実際の終了を示すサーバーのコネクションクローズに依存して、Content-Length ヘッダーを省略したり、誤った長さを含めたりするものがあります。
- クライアントやプロキシがコネクションクローズで終了する HTTP レスポンスを受け取り、実際に転送されたエンティティの長さが Content-Length と一致しない (または Content-Length がない) 場合、受信者は長さの正しさを疑うべきです。
- 受信者がキャッシュプロキシの場合、受信者は応答をキャッシュすべきではない(潜在的なエラーの将来的な複合化を最小限にするため)。プロキシは、セマンティックな透明性を維持するために、Content-Lengthの「修正」を試みることなく、疑わしいメッセージをそのまま転送すべきである。

### 4.7.3 Connection Close Tolerance, Retries, and Idempotency
- 接続は、エラーが発生していない状態でも、いつでも閉じることができます。HTTP アプリケーションは、予期せぬクローズを適切に処理する準備をしなければなりません。クライアントがトランザクションを実行しているときにトランスポート接続が閉じた場合、トランザクションに副作用がない限り、クライアントは接続を再度開き、1回だけ再試行する必要があります。
- パイプライン接続の場合、状況はさらに悪化します。クライアントは多数のリクエストをエンキューすることができますが、オリジンサーバーは接続を閉じることができ、多数のリクエストが処理されずに残り、再スケジューリングが必要になります。
- 副作用は重要です。あるリクエストデータが送信された後、レスポンスが返される前にコネクションがクローズされた場合、クライアントは、トランザクションのうちどれだけが実際にサーバによって呼び出されたかを100％確信することはできません。静的なHTMLページをGETするようなトランザクションは、何も変更せずに何度でも繰り返すことができます。他のトランザクション、例えばオンライン書店への注文のPOSTなどは繰り返すべきではありません、さもないと複数の注文を受ける危険性があります。
- トランザクションは、一度実行されても何度も実行されても同じ結果が得られる場合、冪等であると言えます。実装者は、GET、HEAD、PUT、DELETE、TRACE、OPTIONSの各メソッドがこの性質を持つと仮定することができます。
メソッドがこの性質を持っていると仮定することができます。 クライアントは（POSTのような）冪等でないリクエストをパイプラインで送るべきではありません。
- そうしないと、トランスポート接続の早期終了により、不確定な結果になる可能性があります。非冪等のリクエストを送信したい場合は、前のリクエストのレスポンスステータスを待つべきです。
- 非べき等のメソッドやシーケンスは自動的に再試行されてはならないが、ユーザエージェントは人間のオペレータにリクエストを再試行する選択肢を提供することができる。例えば、ほとんどのブラウザは、キャッシュされたPOSTレスポンスを再読み込みする際に、トランザクションを再度投稿するかどうかを尋ねるダイアログボックスを提供します。

### 4.7.4 Graceful Connection Close
- 図4-19に示すように、TCPコネクションは双方向である。TCPコネクションの各側には、データの読み書きのための入力キューと出力キューがあります。

#### 4.7.4.1 Full and half closes
- アプリケーションは、TCP の入力チャネルと出力チャネルのいずれか、または両方を閉じることができます。close()ソケット・コールは、TCP 接続の入力チャネルと出力チャネルの両方を閉じます。これは「フル・クローズ」と呼ばれ、図 4-20a に示されています。shutdown()ソケット・コールを使用すると、入力チャネルまたは出力チャネルを個別に閉じることができます。これは「ハーフクローズ」と呼ばれるもので、図 4-20b に示されています。

a. server full close
<--|
---|
b. server output half close (graceful close)
<--|
--->
c. server input half close
<---
---|

#### 4.7.4.2 TCP close and reset errors
- 単純なHTTPアプリケーションでは、フルクローズしか使用できません。しかし、アプリケーションが他の多くのタイプのHTTPクライアント、サーバー、プロキシと通信するようになったり、パイプラインによる持続的な接続を使用するようになると、相手が予期しない書き込みエラーを起こさないようにするために、ハーフクローズを使用することが重要になります。
- 一般的に、接続の出力チャネルを閉じることは常に安全です。接続の反対側にいるピアは、バッファからすべてのデータが読み込まれると、end-of-stream通知を受け取って、接続を閉じたことが通知されます。
- 接続の入力チャネルを閉じることは、相手側がこれ以上データを送る予定がないことがわかっている場合を除き、リスクが高くなります。相手側があなたの閉じた入力チャネルにデータを送信すると、オペレーティングシステムは、図4-21に示すように、TCPの「Connection reset by peer」メッセージを相手側のマシンに返します。ほとんどのオペレーティングシステムはこれを重大なエラーとして扱い、相手側がまだ読んでいないバッファリングされたデータを消去します。これは、パイプライン接続にとって非常に悪いことです。
- 持続的な接続でパイプライン化された10個のリクエストを送信し、レスポンスはすでに到着してオペレーティングシステムのバッファに置かれているとします（ただし、アプリケーションはまだ読んでいません）。ここで、あなたがリクエスト#11を送信したとします。しかし、サーバーはあなたがこの接続を十分に使用したと判断し、接続を閉じました。あなたのリクエスト#11は閉じたコネクションに到着し、あなたにリセットを返します。このリセットにより、入力バッファが消去されます。ようやくデータを読めるようになったとき、ピアエラーによるコネクションリセットが発生し、バッファリングされた未読のレスポンスデータは、その多くが正常にあなたのマシンに到着したにもかかわらず、失われてしまいます。

### 4.7.4.3 Graceful close
- HTTP仕様では、クライアントやサーバが予期せずに接続を閉じたい場合、「トランスポート接続にグレースフルクローズを発行する」ことが推奨されていますが、その方法については記述されていません。
- 一般的に、グレースフルクローズを実装するアプリケーションは、まず自分の出力チャンネルを閉じ、次に接続の反対側にいる相手が自分の出力チャンネルを閉じるのを待ちます。双方がもうデータを送信しないことを伝え終わったら（つまり、出力チャンネルを閉じたら）、リセットのリスクなしに、接続を完全に閉じることができます。
- 残念ながら、相手側がハーフクローズを実装しているかどうか、チェックしているかどうかは保証されていません。このため、gracefullyに閉じたいアプリケーションは、出力チャネルをハーフクローズし、入力チャネルの状態を定期的にチェックする必要があります（データの有無やストリームの終了を確認する）。あるタイムアウト時間内に入力チャネルがピアによってクローズされなかった場合、アプリケーションはリソースを節約するために強制的に接続をクローズすることができます。

# Part II: HTTP Architecture
- Ch5 ~ 10

# Ch5 Web Servers
- ウェブサーバは、1日に何十億ものウェブページを配信しています。天気予報を伝えたり、オンラインショッピングのカートを積み込んだり、懐かしい高校時代の友人を探したりしています。ウェブサーバは、World Wide Webの主力製品です。この章では、以下のことを説明します。

  - ソフトウェアおよびハードウェアのウェブサーバのさまざまな種類について説明します。
  - Perlで簡単な診断用Webサーバを書く方法を説明します。
  - ウェブサーバがどのようにHTTPトランザクションを処理するのか、順を追って説明します。

- 物事を具体的に説明するために、例ではApache Webサーバとその設定オプションを使用します。

## 5.1 Web Servers Come in All Shapes and Sizes
- Webサーバーは、HTTPリクエストを処理し、レスポンスを提供します。「Webサーバー」という言葉は、Webサーバーソフトウェアを指す場合と、Webページを提供するための専用のデバイスやコンピューターを指す場合があります。
- ウェブサーバーには、さまざまな種類、形、サイズがあります。些細な10行のPerlスクリプトのウェブサーバもあれば、50MBの安全なコマースエンジンもありますし、小さなカード型のサーバもあります。しかし、機能的な違いはあっても、すべてのウェブサーバはリソースに対するHTTPリクエストを受け取り、クライアントにコンテンツを提供します（図1-5を参照）。

### 5.1.1 Web Server Implementations
- Webサーバは、HTTPとそれに関連するTCPコネクションの処理を実装します。また、ウェブサーバーが提供するリソースを管理し、ウェブサーバーを設定、制御、強化するための管理機能を提供します。
- Webサーバーロジックは、HTTPプロトコルを実装し、Webリソースを管理し、Webサーバーの管理機能を提供します。Webサーバのロジックは、TCPコネクションの管理をOSと分担しています。
- オペレーティングシステムは、基礎となるコンピュータシステムのハードウェアの詳細を管理し、TCP/IPネットワークのサポート、ウェブリソースを保持するファイルシステム、および現在のコンピューティングアクティビティを制御するプロセス管理を提供します。
- Webサーバーにはさまざまな種類があります。

  - 一般的なコンピュータシステムに、汎用ソフトウェアのウェブサーバーをインストールして実行することができます。
  - ソフトウェアをインストールするのが面倒な場合は、ウェブサーバーアプライアンスを購入することができます。このアプライアンスは、ソフトウェアがあらかじめコンピュータにインストールされ、設定された状態で、スタイリッシュな筐体に収められています。
  - また、マイクロプロセッサの進歩により、少数のチップで構成された組み込み型のウェブサーバーを提供している企業もあり、コンシューマー機器の管理コンソールとしても最適です。

- それでは、それぞれのタイプの実装を見ていきましょう。

### 5.1.2 General-Purpose Software Web Servers
- 汎用ソフトウェアのウェブサーバーは、ネットワークに接続された標準的なコンピュータシステム上で動作する。オープンソースのソフトウェア（ApacheやW3CのJigsawなど）や商用ソフトウェア（MicrosoftやiPlanetのWebサーバーなど）を選択することができます。Webサーバーのソフトウェアは、ほぼすべてのコンピュータとOSに対応しています。
- 何万種類ものウェブサーバープログラム（カスタムメイドの特別な目的のウェブサーバーを含む）がありますが、ほとんどのウェブサーバーソフトウェアは、少数の組織から提供されています。
- 比率としてはApacheソフトウェアが多いとされる。

### 5.1.3 Web Server Appliances
- Webサーバーアプライアンスは、ソフトウェアとハードウェアがパッケージ化されたソリューションです。ベンダーは、ベンダーが選択したコンピュータプラットフォームにソフトウェアサーバーをあらかじめインストールし、ソフトウェアを事前に設定します。以下の例があります。
- Webサーバーアプライアンスの例としては、次のようなものがあります。

  - Sun/Cobalt RaQ Webアプライアンス (http://www.cobalt.com)
  - 東芝Magnia SG10 (http://www.toshiba.com)
  - IBM Whistle Webサーバーアプライアンス (http://www.whistle.com) 

- アプライアンスソリューションは、ソフトウェアのインストールや設定の必要性を排除し、多くの場合、管理を大幅に簡素化します。しかし、Webサーバーの柔軟性や機能性は低く、サーバーハードウェアの再利用やアップグレードも容易ではありません。

### 5.1.4 Embedded Web Servers
- 組み込み型サーバーは、プリンターや家電製品などのコンシューマー製品に組み込むことを目的とした小型のウェブサーバーです。
- 組込み型Webサーバを使用することで、ユーザは便利なWebブラウザのインターフェースを使用して、コンシューマ機器を管理することができます。
- 組み込み型Webサーバの中には、1平方インチ以下の大きさで実装できるものもありますが、通常は最小限の機能しか提供されていません。

## 5.2 A Minimal Perl Web Server
- フル機能のHTTPサーバーを構築するには、いくつかの作業が必要になります。Apacheウェブサーバのコア部分には5万行以上のコードがあり、オプションの処理モジュールを使えば、その数はもっと多くなります。
- これらのソフトウェアは、HTTP/1.1の機能をサポートするために必要です。リッチリソースサポート、バーチャルホスティング、アクセスコントロール、ロギング、コンフィギュレーション、モニタリング、パフォーマンス機能などです。とはいえ、最低限の機能を備えたHTTPサーバは、30行以下のPerlで作ることができます。（例5-1 type-o-serveというPerlのプログラム）

## 5.3 What Real Web Servers Do
- 最新の商用Webサーバは例5-1よりもっと複雑ですが、図5-3に示すように、いくつかの一般的なタスクを実行します。

  1. 接続の設定：クライアントの接続を受け入れるか、クライアントが不要な場合は閉じる。
  2. リクエストの受信：ネットワークからのHTTPリクエストメッセージを読み込みます。
  3. リクエストの処理：リクエストメッセージを解釈してアクションを起こす。
  4. Access resource：メッセージで指定されたリソースにアクセスする。
  5. Construct response：正しいヘッダーを持つ HTTP レスポンスメッセージを作成する。
  6. レスポンスの送信：レスポンスをクライアントに返信します。
  7. トランザクションの記録：完了したトランザクションに関するメモをログファイルに記録します。

- 次の7つのセクションでは、ウェブサーバーがこれらの基本的なタスクをどのように実行するかを説明します。

## 5.4 Step 1: Accepting Client Connections
- クライアントは、サーバーとの持続的接続がすでに開かれている場合は、その接続を使用してリクエストを送信することができます。そうでない場合は、クライアントはサーバーへの新しい接続を開く必要があります

### 5.4.1 Handling New Connections
- クライアントがウェブサーバへのTCP接続を要求すると、ウェブサーバは接続を確立し、TCP接続からIPアドレスを抽出して、接続の相手方がどのクライアントであるかを判断します。 新しい接続が確立されて受け入れられると、サーバは新しい接続を既存のウェブサーバの接続リストに追加し、接続上のデータを監視する準備をします。
- ウェブサーバは、どのような接続も自由に拒否し、直ちに閉鎖することができます。ウェブサーバの中には、クライアントのIPアドレスやホスト名が不正なものであったり、既知の悪意のあるクライアントであったりするために、接続を閉じるものもあります。その他の識別技術を使用することもできます。

### 5.4.2 Client Hostname Identification
- ほとんどのウェブサーバは、"リバースDNS "を使用して、クライアントのIPアドレスをクライアントのホスト名に変換するように設定できます。ウェブサーバは、クライアントのホスト名を利用して、詳細なアクセス制御やロギングを行うことができます。ただし、ホスト名の検索には非常に長い時間がかかるため、Webトランザクションの速度が低下する可能性があることに注意してください。大容量のウェブサーバの多くは、ホスト名解決を無効にしたり、特定のコンテンツに対してのみ有効にしたりしています。
- Apache では Hostname Lookups 設定ディレクティブを使ってホスト名検索を有効にすることができます。（例ではHTML、CGIリソースが設定されているが、これはクライアントがこれらの種類のリソース取得を依頼した場合にはクライアントのIPをホスト名に変換するということか？）

### 5.4.3 Determining the Client User Through ident
- 一部のウェブサーバーは、IETFのidentプロトコルをサポートしています。identプロトコルでは、どのユーザーがHTTP接続を開始したかをサーバーが知ることができます。この情報は、ウェブサーバのログ収集に特に役立ちます。一般的なCommon Log Formatの第2フィールドには、各HTTPリクエストのidentユーザー名が含まれています。
- クライアントがidentプロトコルをサポートしている場合、クライアントはTCPポート113でidentリクエストを待ち受けます。図5-4にidentプロトコルの仕組みを示します。図5-4aでは、クライアントがHTTPコネクションを開きます。サーバは、クライアントのidentdサーバポート（113）に戻る独自の接続を開き、新しい接続（クライアントとサーバのポート番号で指定）に対応するユーザ名を求める単純なリクエストを送信し、ユーザ名を含む応答をクライアントから取得します。
- identは組織内では機能しますが、公共のインターネット上では、以下のような多くの理由でうまく機能しません。

  - 多くのクライアントPCでは、Identification Protocolのデーモンソフトウェアであるidentdが動作しません。
  - identプロトコルは、HTTPトランザクションを大幅に遅延させます。
  - 多くのファイアウォールがidentのトラフィックの受信を許可しない。
  - identプロトコルは安全ではなく、偽造も容易です。
  - identプロトコルは、仮想IPアドレスをうまくサポートできません。
  - クライアントのユーザ名を公開することには、プライバシーの問題があります。

- ApacheのIdentityCheck onディレクティブを使って、Identルックアップを使うようにApacheのウェブサーバに伝えることができます。ident の情報が得られない場合、Apache は ident のログフィールドをハイフン (-) で埋めます。Common Log Format のログファイルは、ID 情報がないため、通常 2 番目のフィールドにハイフンが含まれています。

## 5.5 Step 2: Receiving Request Messages
- コネクションにデータが到着すると、ウェブサーバーはネットワークコネクションからデータを読み取って リクエストメッセージの一部を解析します（図5-5）。
- リクエストメッセージを解析する際、ウェブサーバーは以下の作業を行います。

  - リクエスト行を解析して，リクエストメソッド，指定されたリソース識別子（URI），バージョン番号を探し[3]，それぞれを1つのスペースで区切り，最後にCRLF（carriage-return line-feed）シーケンスで終了する[4]。
    - [3] HTTPの初期バージョンであるHTTP/0.9は、バージョン番号をサポートしていません。一部のウェブサーバーはバージョン番号の欠落をサポートしており、メッセージをHTTP/0.9のリクエストとして解釈します。
    - [4] クライアントの中には、LF を行末のターミネーターとして誤って送信するものがあるため、多くのウェブサーバーは行末のシーケンスとして LF または CRLF をサポートしています。
  - CRLF で終わるメッセージヘッダーを読み取ります。
  - CRLF で終わるヘッダの空行を検出します（存在する場合）。
  - リクエストボディがあれば読み取る（Content-Length ヘッダーで指定された長さ）。

- リクエストメッセージを解析する際、ウェブサーバーはネットワークから不規則に入力データを受け取ります。ネットワーク接続はどの時点でも失速する可能性があります。Web サーバーは、ネットワークからデータを読み取り、メッセージの部分的なデータを一時的にメモリに格納してから、メッセージを解析して意味を理解するのに十分なデータを受け取る必要があります。

### 5.5.1 Internal Representations of Messages
- ウェブサーバの中には、リクエストメッセージを内部のデータ構造に格納して、メッセージの操作を容易にしているものもあります。例えば、データ構造にはリクエストメッセージの各部分のポインタと長さが格納され、ヘッダは高速ルックアップテーブルに格納され、特定のヘッダの特定の値に素早くアクセスできるようになっているかもしれません（図5-6）。

### 5.5.2 Connection Input/Output Processing Architectures
- 高性能なウェブサーバは、数千の同時接続をサポートしています。これらの接続により、ウェブサーバは世界中のクライアントと通信することができ、各クライアントはサーバに対して1つまたは複数の接続を開いています。これらの接続の中には、ウェブサーバーに素早くリクエストを送信しているものもあれば、ゆっくりと、あるいはまれにしかリクエストを送信しないものもあります。また、将来の活動を静かに待っているアイドル状態の接続もあります。
- リクエストはいつでも発生する可能性があるため、Webサーバは常に新しいWebリクエストを監視しています。図5-7に示すように、ウェブサーバのアーキテクチャによって、リクエストの処理方法は異なります。

- シングルスレッドのウェブサーバ（図5-7a）
  - シングルスレッドのウェブサーバは、一度に1つのリクエストを完了するまで処理します。トランザクションが完了すると、次の接続が処理されます。このアーキテクチャは実装が簡単ですが、処理中は他のすべての接続が無視されます。これは深刻なパフォーマンス問題を引き起こすので、負荷の低いサーバーやtype-o-serveのような診断ツールにのみ適しています。

- マルチプロセス・マルチスレッドWebサーバ（図5-7b）
  - マルチプロセス・マルチスレッドWebサーバは、複数のプロセスや効率の良いスレッドを使用して、リクエストを同時に処理します[5]。スレッド/プロセスは、必要に応じて作成される場合もあれば、事前に作成される場合もあります[6]。接続ごとにスレッドやプロセスを割り当てるサーバもありますが、サーバが数百、数千、あるいは数万の同時接続を処理する場合、結果としてプロセスやスレッドの数が多くなり、メモリやシステムリソースを消費してしまう可能性があります。そのため、マルチスレッド対応のウェブサーバーの多くは、スレッド/プロセスの最大数に制限を設けています。
  - [5]プロセスとは、個々のプログラムの制御の流れであり、独自の変数群を持つものです。スレッドは、プロセスのより高速で効率的なバージョンです。スレッドもプロセスも、1つのプログラムで複数のことを同時に行うことができます。説明を簡単にするために、ここではプロセスとスレッドを同じように扱います。しかし、性能の違いから、多くの高性能サーバーはマルチプロセスとマルチスレッドの両方を採用している。
  - [6]スレッドをあらかじめ作成しておくシステムを「ワーカープール」と呼びますが、これはスレッドの集合体がプールで仕事を待っているからです。

- 多重化されたI/Oサーバー（図5-7c）
  - 大量の接続をサポートするために、多くのウェブサーバは多重化アーキテクチャを採用しています。多重化されたアーキテクチャでは、すべての接続が同時に監視され、活動が行われます。接続の状態が変化したとき（データが利用可能になったときや、エラー状態が発生したときなど）、その接続に対して少量の処理が行われ、その処理が完了すると、その接続は次の状態変化のために開いている接続リストに戻される。処理が完了すると、次の状態変更のために接続がオープンリストに戻されます。処理すべきことがある場合にのみ、接続上で作業が行われ、スレッドやプロセスがアイドル状態の接続を待つことはありません。

- 多重化されたマルチスレッドのWebサーバ（図5-7d）
  - マルチスレッドとマルチプレックスを組み合わせて、コンピュータ・プラットフォームの複数のCPUを活用するシステムもあります。複数のスレッド（多くの場合、物理的なプロセッサごとに1つ）がそれぞれ開いている接続（または開いている接続のサブセット）を監視し、各接続に対して少量の作業を行います。

## 5.6 Step 3: Processing Requests
- Web サーバーは、リクエストを受信すると、メソッド、リソース、ヘッダー、オプションのボディを使用してリクエストを処理できます。
- 一部のメソッド（POSTなど）では、リクエストメッセージにエンティティボディデータを必要とします。その他のメソッド (例: OPTIONS) は、リクエストボディを許可しますが、必要としません。いくつかのメソッド (例: GET) は、リクエスト・メッセージ内のエンティティ・ボディ・データを禁止しています。
- ここでは、リクエスト処理については触れません。なぜなら、この本の残りの部分のほとんどの章で扱われるからです。

## 5.7 Step 4: Mapping and Accessing Resources
- ウェブサーバはリソースサーバであり、HTMLページやJPEG画像などの事前に作成されたコンテンツや、サーバ上で動作するリソース生成アプリケーションからの動的コンテンツを配信する。
- ウェブサーバは、クライアントにコンテンツを配信する前に、リクエストメッセージのURIをウェブサーバ上の適切なコンテンツまたはコンテンツジェネレータにマッピングすることで、コンテンツのソースを特定する必要があります。

### 5.7.1 Docroots
- ウェブサーバはさまざまな種類のリソースマッピングをサポートしていますが、最もシンプルなリソースマッピングは、リクエストURIを使ってウェブサーバのファイルシステム内のファイル名を指定するものです。通常、ウェブサーバーのファイルシステムには、ウェブコンテンツ用の特別なフォルダが用意されています。このフォルダはドキュメントルート（docroot）と呼ばれています。Web サーバーはリクエストメッセージから URI を受け取り、ドキュメントルートに追加します。
- 図 5-8 では、/specials/saw-blade.gif のリクエストが届いています。この例の Web サーバはドキュメントルート /usr/local/httpd/files を持っています。Web サーバーはファイル /usr/local/httpd/files/specials/sawblade.gif を返します。

- Apache Web サーバーのドキュメントルートを設定するには、httpd.conf コンフィギュレーションファイルに DocumentRoot 行を追加します。

```
DocumentRoot /usr/local/httpd/files
```

- サーバーは、相対URLがドキュメントルートから戻ってきて、ファイルシステムの他の部分を公開しないように注意します。例えば、ほとんどの成熟したウェブサーバーは、次のURIがJoe's Hardwareのドキュメントルートより上のファイルを見ることを許可しません。 http://www.joes-hardware.com/../

#### 5.7.1.1 Virtually hosted docroots
- 仮想ホスト型Webサーバーは、複数のWebサイトを同一のWebサーバー上でホストし、各サイトにサーバー上の個別のドキュメントルートを与えます。仮想ホスト型Webサーバーは、URIやHostヘッダーのIPアドレスやホスト名から、使用する正しいドキュメントルートを特定します。このようにして、同じWebサーバーでホストされている2つのWebサイトは、リクエストURIが同じであっても、完全に異なるコンテンツを持つことができます。
- 図5-9では、サーバーは www.joes-hardware.com と www.marys-antiques.com という2つのサイトをホストしています。サーバーは、HTTP Host ヘッダーを使用して、または異なる IP アドレスから Web サイトを識別できます。

```
-- リクエストA
GET /index.html HTTP/1.0
Host: www.joes-hardware.com

-- リクエストB
GET /index.htmld HTTP/1.0
Host: ww.marys-antiques.com
```
  - リクエストAが到着すると、サーバーは/docs/joe/index.htmlのファイルをフェッチします。
  - リクエストBが到着すると、サーバーは/docs/mary/index.htmlのファイルをフェッチします。

- ほとんどのウェブサーバでは、仮想ホストされたドキュメントを設定するのは簡単です。一般的な Apache Web サーバーでは、仮想 Webサイトごとに VirtualHost ブロックを設定し、各仮想サーバーの DocumentRoot を含める必要があります（例 5-3）。
- バーチャル・ホスティングの詳細については、セクション18.2を参照してください。

#### 5.7.1.2 User home directory docroots
- docrootsのもう一つの一般的な使い方は、ウェブサーバー上にプライベートなウェブサイトを提供することです。典型的な規約では、パスがスラッシュとチルダ（/~）で始まり、ユーザー名が続くURIを、そのユーザーのプライベートドキュメントルートにマッピングします。プライベートドキュメントルートは、多くの場合、そのユーザーのホームディレクトリ内のpublic_htmlというフォルダですが、別の構成にすることもできます（図5-10）。

### 5.7.2 Directory Listings
- Webサーバーは、パスがファイルではなくディレクトリに解決されるディレクトリURLのリクエストを受け取ることができます。ほとんどのウェブサーバーは、クライアントがディレクトリURLをリクエストしたときに、いくつかの異なるアクションを取るように設定できます。ディレクトリURLを要求した場合、多くのウェブサーバは以下のような動作をします。

  - エラーを返す。
  - ディレクトリではなく、特別なデフォルトの「インデックスファイル」を返す。
  - ディレクトリをスキャンして、その内容を含むHTMLページを返す。

- ほとんどのウェブサーバは、ディレクトリを表すために、ディレクトリ内にindex.htmlまたはindex.htmという名前のファイルを探します。ユーザーがあるディレクトリのURLを要求したときに、そのディレクトリにindex.html（またはindex.htm）という名前のファイルがあれば、サーバーはそのファイルの内容を返します。
- Apacheウェブサーバでは、DirectoryIndex設定ディレクティブを使って、デフォルトのディレクトリファイルとして解釈されるファイル名のセットを設定することができます。DirectoryIndex ディレクティブは、ディレクトリ・インデックス・ファイルとして機能するすべてのファイル名を、優先順にリストアップします。以下の設定行は、ディレクトリ URL のリクエストに応答して、Apache がディレクトリを検索して、リストされたファイルを探すようにします。

```
DirectoryIndex index.html index.htm home.html home.htm index.cgi 
```

- ユーザがディレクトリ URI を要求したときにデフォルトのインデックスファイルが存在せず、ディレクトリインデックスが無効になっていない場合、多くの Web サーバは、そのディレクトリ内のファイルの一覧、各ファイルのサイズと更新日、各ファイルへの URI リンクを含む HTML ファイルを自動的に返します。このファイル一覧は便利ですが、おせっかいな人が通常は見つけられないようなファイルをウェブサーバ上で見つけてしまう可能性があります。
- ディレクトリインデックスファイルの自動生成は、Apache ディレクティブで無効にすることができます。 `Options -Indexes`

### 5.7.3 Dynamic Content Resource Mapping
- Webサーバは、URIをダイナミックリソース、つまり必要に応じてコンテンツを生成するプログラムにマッピングすることもできます（図5-11）。実際、アプリケーション・サーバと呼ばれるウェブ・サーバのクラスは、ウェブ・サーバと高度なバックエンド・アプリケーションとを接続します。ウェブサーバは、あるリソースが動的リソースであること、動的コンテンツ生成プログラムがどこにあるか、そしてそのプログラムをどのように実行するかを知る必要があります。ほとんどのウェブサーバは、動的リソースを識別してマッピングするための基本的なメカニズムを備えています。
- Apache では、URI のpathnameコンポーネントを実行可能なプログラムのディレクトリに マップすることができます。サーバは、実行可能なパス成分を持つ URI のリクエストを受け取ると、対応するサーバのディレクトリにあるプログラムを実行しようとします。例えば、次の Apache 設定ディレクティブは、パスが /cgi-bin/ で始まるすべての URI が /usr/local/etc/httpd/cgi-programs/ ディレクトリにある対応するプログラムを実行することを指定しています。

```
ScriptAlias /cgi-bin/ /usr/local/etc/httpd/cgi-programs/
```

- Apache では、実行ファイルを特別なファイル拡張子でマークすることもできます。この方法では、実行可能なスクリプトをどのディレクトリにも置くことができます。以下のApacheの設定ディレクティブは、.cgiで終わるすべてのウェブリソースを実行するように指定しています。

```
AddHandler cgi-script .cgi
```

- CGI は、サーバサイドアプリケーションを実行するための初期のシンプルで人気のあるインターフェイスです。最近のアプリケーションサーバは、Microsoft の Active Server Pages や Java servlet など、より強力で効率的なサーバサイドのダイナミックコンテンツをサポートしています。

### 5.7.4 Server-Side Includes (SSI)
- 多くのウェブサーバーは、サーバーサイドインクルードをサポートしています。リソースにサーバーサイドインクルードのフラグが立っている場合、サーバーはクライアントに送信する前にリソースのコンテンツを処理します。
- リソースのコンテンツは、変数名や埋め込まれたスクリプトなどの特殊なパターン（多くの場合、特別なHTMLコメントの中に含まれています）をスキャンします。特殊なパターンは、変数の値や、実行可能なスクリプトの出力に置き換えられます。これにより、簡単に動的なコンテンツを作成することができます。

### 5.7.5 Access Controls
- ウェブサーバは、特定のリソースにアクセス制御を割り当てることもできます。アクセス制御されたリソースへのリクエストが到着すると、ウェブサーバーはクライアントのIPアドレスに基づいてアクセスを制御したり、リソースへのアクセスを得るためにパスワードチャレンジを発行したりすることができます。
- HTTP認証の詳細については、第12章を参照してください。

## 5.8 Step 5: Building Responses
- Webサーバーは、リソースを特定すると、リクエストメソッドに記述されたアクションを実行し、レスポンスメッセージを返します。応答メッセージには、応答ステータスコード、応答ヘッダー、および応答ボディ（生成された場合）が含まれます。HTTP レスポンスコードについては、第 3 章の 3.4 節で詳しく説明しています。

### 5.8.1 Response Entities
- トランザクションでレスポンスボディが生成された場合、その内容が応答メッセージとともに返送されます。ボディがあった場合、応答メッセージには通常以下が含まれる。
  - 応答ボディのMIMEタイプを記述するContent-Typeヘッダー。
  - レスポンスボディのサイズを表す Content-Length ヘッダー
  - 実際のメッセージボディの内容 

### 5.8.2 MIME Typing
- レスポンスボディのMIMEタイプを決定するのは、ウェブサーバーの役目です。リソースにMIMEタイプを関連付けるためにサーバーを設定する方法はたくさんあります。

  - mime.types
    - Webサーバーは、ファイル名の拡張子を使って MIME タイプを示すことができます。Webサーバーは、各拡張子の MIME タイプを含むファイルをスキャンして、各リソースの MIME タイプを計算します。この拡張子ベースのタイプの関連付けは最も一般的なもので、図5-12に示されています。

  - Magic typing
    - Apache ウェブサーバは、各リソースのコンテンツをスキャンして、既知のパターンのテーブル（マジックファイルと呼ばれる）とコンテンツをパターンマッチさせて、各ファイルの MIME タイプを決定することができます。これには時間がかかりますが、特にファイル名に標準的な拡張子が付いていない場合には便利です。

  - Explicit typing
    - Webサーバーでは、ファイルの拡張子や内容にかかわらず、特定のファイルやディレクトリのコンテンツに強制的に MIME タイプを指定するように設定できます。

  - Type negotiation
    - Webサーバーの中には、リソースを複数のドキュメント形式で保存するように設定できるものがあります。この場合、Webサーバーは、ユーザーとのネゴシエーションプロセスによって、使用する「最適な」フォーマット（および関連する MIME タイプ）を決定するように設定できます。これについては第17章で説明します。

- また、ウェブサーバーは、特定のファイルとMIMEタイプを関連付けるように設定することもできます。

### 5.8.3 Redirection
- Webサーバーは、成功メッセージではなく、リダイレクトレスポンスを返すことがあります。Webサーバーは、リクエストを実行するためにブラウザを別の場所にリダイレクトすることができます。リダイレクションレスポンスは、3XXというリターンコードで示されます。Locationレスポンスヘッダには、コンテンツの新しい、または好ましい場所を示すURIが含まれています。リダイレクトは以下のような場合に有効です。

  - Permanently moved resources
    - リソースが新しい場所に移動されたり、名前が変更されたりして、新しいURLが与えられている場合です。ウェブサーバは、リソースの名前が変更されたことをクライアントに伝え、クライアントは新しい場所からリソースを取得する前に、ブックマークなどを更新することができます。この種のリダイレクトには、ステータスコード301 Moved Permanentlyが使用されます。

  - Temporarily moved resources
    - リソースが一時的に移動したり、名前が変更された場合、サーバーはクライアントを新しい場所にリダイレクトしたいと思うかもしれません。しかし、名前の変更は一時的なものなので、サーバーはクライアントが将来古いURLで戻ってくることを望んでおり、ブックマークを更新しないようにしています。このようなリダイレクトには、ステータスコード303「See Other」と307「Temporary Redirect」が使用されます。

  - URL augmentation - URLの拡張
    - サーバーはしばしばリダイレクトを使ってURLを書き換え、文脈を埋め込むことがあります。リクエストが到着すると、サーバーは埋め込まれた状態情報を含む新しいURLを生成し、ユーザーをこの新しいURLにリダイレクトします[7]。クライアントはリダイレクトに従い、リクエストを再発行しますが、その際には完全な拡張された状態のURLが含まれています。これは、トランザクション間で状態を維持するための便利な方法です。この種のリダイレクトには、ステータスコード303「See Other」および307「Temporary Redirect」が使用される。
    - [7] これらの拡張された状態のURLは、"fat URL "と呼ばれることもある。

  - Load balancing
    - 負荷の高いサーバーがリクエストを受けた場合、サーバーはクライアントを負荷の低いサーバーにリダイレクトすることができます。この種のリダイレクトには、ステータスコード303「See Other」および307「Temporary Redirect」が使用される。

  - Server affinity
    - Webサーバーは、特定のユーザーのローカル情報を持っている場合があります。サーバーは、クライアントに関する情報を持っているサーバーにクライアントをリダイレクトすることができます。このようなリダイレクトには、ステータスコード 303 See Other および 307 Temporary Redirect が使用されます。

  - Canonicalizing directory names - ディレクトリ名の正規化
    - クライアントが末尾にスラッシュのないディレクトリ名のURIを要求した場合、ほとんどのWebサーバーは、相対リンクが正しく動作するように、スラッシュを追加したURIにクライアントをリダイレクトします。

## 5.9 Step 6: Sending Responses
- ウェブサーバは、コネクションを介してデータを送信する際にも、受信する際と同様の問題に直面します。サーバーは、多くのクライアントとの間に多数の接続を持ち、あるものはアイドル状態、あるものはデータをサーバーに送信し、あるものはレスポンスデータをクライアントに返送します。
- サーバーは、接続状態を追跡し、持続的な接続を特別な注意を払って処理する必要があります。非持続的な接続では、メッセージがすべて送信された時点でサーバー側の接続を閉じることになっています。
- 持続的な接続では、接続が継続される可能性があります。その場合、サーバーはContent-Lengthヘッダーを正しく計算するために特別な注意を払う必要があり、そうしないとクライアントはいつレスポンスが終了するのかを知る方法がありません（第4章参照）。

## 5.10 Step 7: Logging
- 最後に、トランザクションが完了すると、ウェブサーバは実行されたトランザクションを記述したエントリをログファイルに記録します。ほとんどのウェブサーバーは、いくつかの設定可能な形式のログを提供しています。詳細は第21章を参照してください。

# Ch6 Proxies
- Webプロキシサーバーは、クライアントとサーバーの間に位置し、両者の間でHTTPメッセージをやりとりする「仲介役」です。本章では、HTTPプロキシサーバー、プロキシ機能の特別なサポート、およびプロキシサーバーを使用する際に見られる厄介な動作についてすべて説明します。この章では、以下のことを説明します。
  - HTTPプロキシについて、Webゲートウェイとの対比や、プロキシの導入方法を説明します。
  - プロキシが役立つ方法をいくつか紹介します。
  - 実際のネットワークにプロキシがどのように導入され、どのようにトラフィックがプロキシサーバーに誘導されるかを説明する。
  - プロキシを使用するようにブラウザを設定する方法を紹介します。
  - HTTP プロキシリクエストのデモンストレーションを行い、サーバーのリクエストとの違いや、プロキシがブラウザの動作を微妙に変化させる方法を説明する。
  - ViaヘッダとTRACEメソッドを使用して、プロキシサーバのチェーンを介してメッセージのパスを記録する方法を説明します。
  - プロキシベースのHTTPアクセスコントロールについて説明する。
  - クライアントとサーバの間でプロキシがどのように相互運用されるかを説明します。

## 6.1 Web Intermediaries
- Webプロキシサーバーは、クライアントの代わりにトランザクションを実行する仲介役です。Webプロキシがない場合、HTTPクライアントはHTTPサーバーと直接会話します。Webプロキシを使うと、クライアントに代わってプロキシサーバがサーバと通信します。クライアントは、プロキシサーバーの優れたサービスを利用して、トランザクションを完了します。
- HTTPプロキシサーバーは、ウェブサーバーであると同時にウェブクライアントでもあります。HTTPクライアントはプロキシにリクエストメッセージを送信するため、プロキシサーバーはウェブサーバーのようにリクエストと接続を適切に処理し、レスポンスを返さなければなりません。同時に、プロキシ自身もサーバーにリクエストを送信するので、プロキシも正しいHTTPクライアントのように振る舞い、リクエストを送信し、レスポンスを受信する必要があります（図6-1参照）。独自のHTTPプロキシを作成する場合は、HTTPクライアントとHTTPサーバーの両方のルールに注意深く従う必要があります。

### 6.1.1 Private and Shared Proxies
- プロキシサーバーには、1つのクライアント専用のものと、多数のクライアントで共有するものがあります。一人のクライアント専用のプロキシをプライベートプロキシと呼びます。多数のクライアントで共有するプロキシをパブリックプロキシと呼ぶ。

- パブリックプロキシ
  - ほとんどのプロキシは、パブリックプロキシ、つまり共有プロキシです。一元化されたプロキシの方が費用対効果が高く、管理もしやすいからです。また、キャッシングプロキシサーバーなどのプロキシアプリケーションは、より多くのユーザーが同じプロキシサーバーに集まることで、ユーザー間の共通のリクエストを利用できるようになり、より便利になります。

- プライベートプロキシ
  - 専用のプライベートプロキシはそれほど一般的ではありませんが、特にクライアントコンピュータ上で直接実行される場合には、その役割があります。一部のブラウザアシスタント製品や一部のISPサービスでは、ブラウザの機能拡張やパフォーマンスの向上、あるいは無料ISPサービスの広告のホストとして、ユーザーのPC上で小さなプロキシを直接実行している。

### 6.1.2 Proxies Versus Gateways
- 厳密に言えば、プロキシは同じプロトコルを使用する2つ以上のアプリケーションを接続し、ゲートウェイは異なるプロトコルを使用する2つ以上の相手を接続します。ゲートウェイは、クライアントとサーバーのプロトコルが異なっていても、クライアントがサーバーとのトランザクションを完了できるようにする「プロトコルコンバータ」の役割を果たします。
- 図6-2は、プロキシとゲートウェイの違いを示しています。

  - プロキシはクライアントとサーバーの両方にHTTPを話すので、図6-2aの仲介装置はHTTPプロキシである。
  - 図6-2bの仲介装置は、HTTPフロントエンドとPOPメールバックエンドを結びつけるので、HTTP/POPゲートウェイである。このゲートウェイは、Webトランザクションを適切なPOPトランザクションに変換し、ユーザーがHTTPで電子メールを読めるようにします。Yahoo! MailやMSN Hotmailなどのウェブベースの電子メールプログラムは、HTTP電子メールゲートウェイです。

- 実際には、プロキシとゲートウェイの違いは曖昧です。ブラウザとサーバは異なるバージョンのHTTPを実装しているため、プロキシはしばしばある程度のプロトコル変換を行います。また、市販のプロキシサーバは、SSLセキュリティプロトコル、SOCKSファイアウォール、FTPアクセス、Webベースのアプリケーションをサポートするゲートウェイ機能を実装しています。ゲートウェイについては第8章で詳しく説明します。

## 6.2 Why Use Proxies?
- プロキシサーバーには、さまざまな便利な機能があります。セキュリティの向上、パフォーマンスの向上、コストの削減などが可能です。また、プロキシサーバは、通過するすべてのHTTPトラフィックを見たり触ったりすることができるので、プロキシはトラフィックを監視したり変更したりして、多くの有用な付加価値のあるウェブサービスを実装することができます。ここでは、プロキシの利用方法のほんの一例をご紹介します。
- Child filter (Figure 6-3)
  - 小学校では、フィルタリングプロキシを使用して、アダルトコンテンツへのアクセスをブロックする一方で、教育用サイトへのアクセスは制限しないようにしている。図6-3に示すように、プロキシは、教育用コンテンツへの無制限のアクセスを許可するが、子供に不適切なサイトへのアクセスを強制的に拒否することができる[1]。
  - [1] 不快なコンテンツを特定してアクセスを制限するために、いくつかの企業や非営利団体がフィルタリングソフトウェアを提供し、「ブラックリスト」を管理している。

- Document access controller (Figure 6-4)
  - プロキシサーバーは、大規模なウェブサーバーやウェブリソースに対して統一的なアクセスコントロール戦略を実施し、動作記録を作成するために使用することができます。これは、大企業やその他の分散した官僚組織で有効です。すべてのアクセス制御は、集中管理されたプロキシサーバで設定することができ、異なる組織によって管理されている、メーカーやモデルの異なる多数のウェブサーバでアクセス制御を頻繁に更新する必要はありません[2]。
  - [2] 洗練されたユーザが故意に制御プロキシを迂回することを防ぐために、ウェブ・サーバはプロキシ・サーバからの要求のみを受け入れるように静的に構成することができる。
  - 図6-4は，集中型のアクセス制御プロキシである。
    - クライアント1にサーバAからのニュースページへのアクセスを無制限に許可する
    - クライアント2にインターネットコンテンツへの無制限のアクセスを許可する。
    - サーバBへのアクセスを許可する前に、クライアント3からパスワードを要求する。

- Security firewall (Figure 6-5)
  - ネットワーク・セキュリティ・エンジニアは、セキュリティを強化するためにプロキシ・サーバを使用することが多い。プロキシ・サーバは、アプリケーション・レベルのプロトコルが組織に出入りするのを、ネットワーク上の安全な一点で制限します。また、ウイルスを排除するウェブや電子メールのプロキシのように、トラフィックを精査するためのフックを提供することもできます（図6-5）。

- Web cache (Figure 6-6)
  - プロキシ・キャッシュは、人気のあるドキュメントのローカル・コピーを維持し、必要に応じてそれらを提供することで、時間とコストのかかるインターネット通信を削減します。
  - 図6-6では、クライアント1と2は近くのウェブ・キャッシュからオブジェクトAにアクセスし、クライアント3と4はオリジン・サーバーからドキュメントにアクセスする。

- Surrogate (Figure 6-7)
  - プロキシは、Webサーバーを装うことができます。サロゲートやリバースプロキシと呼ばれるプロキシは、実際のWebサーバーのリクエストを受信しますが、Webサーバーとは異なり、要求されたコンテンツを見つけるために他のサーバーとの通信を開始することがあります。
  - サロゲートは、一般的なコンテンツを扱う低速のウェブサーバーのパフォーマンスを向上させるために使用することができます。この構成では、サロゲートはしばしばサーバーアクセラレータと呼ばれます（図6-7）。サロゲートは、コンテンツ・ルーティング機能と組み合わせて、オンデマンドで複製されたコンテンツの分散型ネットワークを構築するためにも使用できます。

- Content router (Figure 6-8)
  - プロキシサーバは、インターネットのトラフィック状況やコンテンツの種類に応じて、リクエストを特定のWebサーバに転送する「コンテンツルータ」として機能します。
  - コンテンツルーターは、さまざまなサービスレベルの提供にも使用できます。例えば、コンテンツルータは、ユーザやコンテンツプロバイダがより高いパフォーマンスを求めている場合には、リクエストを近くのレプリカキャッシュに転送したり（図6-8）、ユーザがフィルタリングサービスに申し込んでいる場合には、HTTPリクエストをフィルタリングプロキシに通したりすることができます。アダプティブ・コンテンツ・ルーティング・プロキシを使って、多くの興味深いサービスを構築することができます。

- Transcoder (Figure 6-9)
  - プロキシサーバーは、クライアントに配信する前に、コンテンツのボディフォーマットを変更することができる。このようなデータ表現間の透過的な変換をトランスコードと呼ぶ[3]。
  - [3]「トランスコード」と「トランスレーション」を区別して、トランスコードはデータのエンコーディングの比較的単純な変換（可逆圧縮など）、トランスレーションはデータの再フォーマットやセマンティックな変更など、より重要なものと定義する人もいる。本書では、トランスコーディングという用語を、コンテンツのあらゆる仲介ベースの変更を意味するものとして使用しています。
  - トランスコーディングプロキシは、GIF画像をJPEG画像に変換してサイズを小さくすることができる。また、テレビで見やすいように画像を縮小したり、色の濃さを減らしたりすることもできる。同様に、テキストファイルも圧縮され、インターネット対応のポケベルやスマートフォン用にWebページの小さなテキスト要約を生成することができます。さらに、プロキシがその場でドキュメントを外国語に変換することも可能です。
  - 図6-9は、英語のテキストをスペイン語のテキストに変換したり、HTMLページを携帯電話の小さな画面に表示できるようなシンプルなテキストに再フォーマットするトランスコーディングプロキシを示している。

- Anonymizer (Figure 6-10)
  - アノニマイザープロキシは、HTTPメッセージから識別可能な特性（クライアントのIPアドレス、Fromヘッダー、Refererヘッダー、Cookie、URIセッションIDなど）を積極的に取り除くことで、プライバシーと匿名性を高めます[4]。
  - [4] ただし、識別情報が除去されるため、ユーザーのブラウジング体験の質が低下したり、一部のWebサイトが正常に機能しない場合があります。
  - 図6-10では、アノニマイザープロキシはプライバシーを高めるためにユーザのメッセージに以下の変更を加えている。プライバシーを向上させる。
    - User-AgentヘッダからユーザのコンピュータおよびOSタイプを削除する。
    - ユーザーの電子メールアドレスを保護するために、Fromヘッダーが削除される。
    - Refererヘッダを削除することで、ユーザが訪問した他のサイトを見えにくくしています。
    - Cookieヘッダを削除することで、プロファイリングやIDデータを排除しています。

## 6.3 Where Do Proxies Go?
- ここでは、プロキシがネットワーク・アーキテクチャに導入されたときの位置について説明します。以下の内容を説明します。
  - プロキシをネットワークに配置する方法
  - プロキシを階層化する方法
  - そもそもトラフィックがプロキシサーバーに誘導される仕組み 

### 6.3.1 Proxy Server Deployment
- プロキシは用途に応じて様々な場所に設置することができます。図6-11は、プロキシサーバーを配置する方法をいくつか示しています。

- Egress proxy (Figure 6-11a)
  - ローカルネットワークの出口にプロキシを貼り付けて、ローカルネットワークとインターネットの間のトラフィックフローを制御することができます。企業では、社外の悪質なハッカーに対するファイアウォールの保護や、インターネットトラフィックの帯域料金の削減とパフォーマンスの向上のために、エグレスプロキシを使用することがあります。小学校では、早熟な生徒が不適切なコンテンツを閲覧するのを防ぐために、フィルタリング用のイグレスプロキシを使用することがあります。

- Access (ingress) proxy (Figure 6-11b)
  - プロキシはISPのアクセスポイントに設置されることが多く、顧客からのリクエストを集約して処理する。ISPは、キャッシングプロキシを使用して人気のある文書のコピーを保存し、ユーザー（特に高速接続のユーザー）のダウンロード速度を向上させ、インターネット帯域幅のコストを削減する。

- Surrogates (Figure 6-11c)
  - プロキシは、サロゲート（一般的にリバースプロキシとも呼ばれる）としてネットワークの端、Webサーバの前に配置されることが多く、Webサーバに向けられたすべてのリクエストを処理し、必要な場合にのみWebサーバにリソースを要求することができます。サロゲートは、ウェブサーバーにセキュリティ機能を追加したり、高速なウェブサーバーのキャッシュを低速なウェブサーバーの前に置くことでパフォーマンスを向上させることができます。サロゲートは通常、ウェブサーバーの名前とIPアドレスを直接想定しているため、すべてのリクエストはサーバーではなくプロキシに送られます。

- Network exchange proxy (Figure 6-11d)
  - 十分な馬力があれば、プロキシはインターネット上のネットワーク間のピアリング交換ポイントに設置され、キャッシュによるインターネットジャンクションでの混雑の緩和や、トラフィックフローの監視を行うことができる[5]。
  - [5] コアプロキシは、インターネットの帯域幅が非常に高価な場所（特にヨーロッパ）でよく導入されています。また、イギリスなどの一部の国では、国家安全保障上の懸念からインターネット・トラフィックを監視するために、プロキシの導入を検討している。

### 6.3.2 Proxy Hierarchies
- プロキシは、プロキシ階層と呼ばれるチェーンでカスケード接続できます。プロキシ階層では、図6-12に示すように、メッセージはプロキシからプロキシへと渡され、最終的にオリジン・サーバーに到達するまで（その後、プロキシを経由してクライアントに戻されるまで）、メッセージが渡される。
- プロキシ階層内のプロキシサーバには、親と子の関係が割り当てられています。次の受信プロキシ(サーバに近い方)を親と呼び、次の送信プロキシ(クライアントに近い方)を子と呼ぶ。図6-12では、プロキシ1はプロキシ2の子プロキシである。同様に、プロキシ2はプロキシ3の子プロキシであり、プロキシ3はプロキシ2の親プロキシである。

```
Fig 6-12. ３レベルのプロキシ階層
Client <--> Proxy1 <--> Proxy2 <--> Proxy3 <--> Origin server
```

#### 6.3.2.1 Proxy hierarchy content routing
- 図 6-12 のプロキシ階層は静的で、プロキシ 1 は常にプロキシ 2 にメッセージを転送し、プロキシ 2 は常にプロキシ 3 にメッセージを転送します。しかし、階層は静的である必要はありません。プロキシ・サーバは、多くの要因に基づいて、変化するプロキシ・サーバとオリジン・サーバのセットにメッセージを転送することができます。
- 例えば、図6-13では、アクセスプロキシは異なる状況下で親プロキシやオリジンサーバにルーティングする。

  - 要求されたオブジェクトが、コンテンツ配信にお金を払っているウェブサーバに属している場合、プロキシはリクエストを近くのキャッシュサーバにルーティングし、キャッシュされたオブジェクトを返すか、利用できない場合にはそれを取り出すことができる。
  - リクエストが特定の種類の画像の場合、アクセスプロキシはリクエストを専用の圧縮プロキシに転送し、画像を取得してから圧縮することで、遅いモデムでもクライアントへのダウンロードを高速化することができます。

- ここでは、動的な親の選択の他の例をいくつか紹介します。

- 負荷分散
  - 子プロキシは、親プロキシの現在の作業負荷レベルに基づいて親プロキシを選択し、負荷を分散することができる。

- 地理的近接性のあるルーティング
  - 子プロキシがオリジンサーバの地理的な地域を担当する親を選択することがある。

- プロトコル/タイプルーティング
  - 子プロキシは、URIに基づいて異なる親やオリジンサーバにルーティングすることがある。URIの種類によっては、特別なプロトコルを処理するために、特別なプロキシサーバを経由してリクエストを転送することがあります。

- サブスクリプションベースのルーティング
  - パブリッシャーが高性能なサービスのために追加料金を支払っている場合、そのURIはパフォーマンスを向上させるために大規模なキャッシュや圧縮エンジンにルーティングされることがあります。

- 動的なペアレントルーティングロジックは、設定ファイル、スクリプト言語、動的実行可能なプラグインなど、製品によって実装方法が異なります。

### 6.3.3 How Proxies Get Traffic
- 通常、クライアントはウェブサーバーと直接対話するため、そもそもHTTPトラフィックがどのようにしてプロキシにたどり着くのかを説明する必要があります。クライアントのトラフィックがプロキシに到達するための一般的な方法は4つあります。

- クライアントを変更する
  - NetscapeやMicrosoftのブラウザをはじめとする多くのWebクライアントは、手動および自動のプロキシ設定をサポートしています。クライアントがプロキシサーバーを使用するように設定されている場合、クライアントはHTTPリクエストをオリジンサーバーにではなく、プロキシに直接かつ意図的に送信します（図6-14a）。

- ネットワークの変更
  - ネットワーク・インフラストラクチャが、クライアントの認識や参加なしにウェブ・トラフィックを傍受し、プロキシに誘導する手法がいくつかあります。この傍受は通常、クライアントに気づかれないようにHTTPトラフィックを監視し、それを傍受し、トラフィックをプロキシに振り分けるスイッチングおよびルーティング装置に依存しています（図6-14b）。これを「インターセプティングプロキシ」と呼びます[6]。
  - [6] 傍受プロキシは、その存在を意識せずに接続することから、一般に「トランスペアレントプロキシ」と呼ばれています。HTTP仕様では、セマンティックな動作を変更しない機能を示すために、すでに「トランスペアレンシー」という用語が使われているため、標準化団体では、トラフィックの捕捉に「インターセプト」という用語を使うことを提案している。ここではこの命名法を採用する。

- DNS名前空間の変更
  - Webサーバの前に置かれたプロキシサーバであるサロゲートは、Webサーバの名前とIPアドレスを直接想定しているため、すべてのリクエストはサーバではなくサロゲートに送られます（図 6-14c). これを実現するには、DNSネーミングテーブルを手動で編集するか、適切なプロキシまたはサーバーをオンデマンドで計算する特別なダイナミックDNSサーバーを使用します。一部のインストールでは、実際のサーバーのIPアドレスと名前が変更され、サロゲートには以前のアドレスと名前が与えられます。

- Webサーバーの変更 
  - ウェブサーバの中には、クライアントにHTTPリダイレクションコマンド（レスポンスコード305）を送り返すことで、クライアントのリクエストをプロキシにリダイレクトするように設定できるものもあります。リダイレクトを受け取ったクライアントは、プロキシとの取引を行います（図6-14d）。

- 次の章では、プロキシにトラフィックを送信するためのクライアントの設定方法を説明します。第20章では、プロキシサーバーにトラフィックをリダイレクトするためのネットワーク、DNS、サーバーの設定方法について説明します。

## 6.4 Client Proxy Settings
- 最近のウェブブラウザでは、プロキシの使用を設定することができます。実際、多くのブラウザでは、以下のような複数の方法でプロキシを設定することができます。
  - Manual configuration
  - Browser preconfiguration
    - ブラウザのベンダーやディストリビューターは、顧客に提供する前に、ブラウザ（またはその他のWebクライアント）のプロキシ設定を手動で事前に行っています。
  - Proxy auto-configuration (PAC)
    - クライアントは、JavaScriptのプロキシ自動設定（PAC）ファイルのURIを提供し、JavaScriptファイルを取得して実行することで、プロキシを使用するかどうか、使用する場合はどのプロキシサーバーを使用するかを決定します。
  - WPAD proxy discovery
    - 一部のブラウザはWPAD（Web Proxy Autodiscovery Protocol）をサポートしており、ブラウザが自動設定ファイルをダウンロードできる「設定サーバー」を自動的に検出する[7]。

### 6.4.1 Client Proxy Configuration: Manual
- 多くのWebクライアントでは、プロキシを手動で設定することができます。Netscape NavigatorもMicrosoft Internet Explorerも、プロキシの設定を便利にサポートしています。
- プロキシのホストとポートを指定することで設定する。

### 6.4.2 Client Proxy Configuration: PAC Files
- 手動でのプロキシ設定は簡単ですが、柔軟性に欠けます。すべてのコンテンツに対して1つのプロキシサーバーしか指定できず、フェイルオーバーにも対応していません。また、手動によるプロキシ設定は、大規模な組織では管理上の問題を引き起こします。設定されたブラウザの数が多い場合、変更が必要になったときにすべてのブラウザを再設定することは困難または不可能です。
- PAC（Proxy Auto-Configuration）ファイルは、プロキシ設定をその場で計算する小さなJavaScriptプログラムなので、プロキシ設定のためのよりダイナミックなソリューションです。ドキュメントにアクセスするたびに、JavaScriptの関数が適切なプロキシサーバーを選択します。
- PACファイルを使用するには、JavaScriptのPACファイルのURIをブラウザに設定します（設定は手動設定と似ていますが、「自動設定」ボックスにURIを指定します）。ブラウザはこのURIからPACファイルを取得し、JavaScriptのロジックを使って各アクセスに適切なプロキシサーバーを計算します。PACファイルは通常、接尾辞が.pacで、MIMEタイプが "application/x-ns-proxy-autoconfig "となっています。各PACファイルは、URIへのアクセスに使用する適切なプロキシサーバーを計算するFindProxyForURL(url,host)という関数を定義する必要があります。この関数の戻り値は，表6-1に示す値のいずれかとなる。
  - DIRECT
  - PROXY host:port
  - SOCKS host:port
- PACファイルの詳細はChapter20を参照。

### 6.4.3 Client Proxy Configuration: WPAD
- ブラウザを設定するもうひとつの仕組みとして、WPAD（Web Proxy Autodiscovery Protocol）があります。WPADは、発見メカニズムのエスカレーション戦略を用いて、ブラウザに適切なPACファイルを自動的に見つけるアルゴリズムです。WPADプロトコルを実装したクライアントは、以下のことを行います。

  - WPADを使ってPACのURIを見つける。
  - そのURIからPACファイルを取得する。
  - PACファイルを実行してプロキシサーバを決定する。
  - リクエストに対してプロキシサーバーを使用する。

- WPADは、適切なPACファイルを決定するために、一連のリソース発見技術を使用する。すべての組織がすべての技術を使用できるわけではないので、複数の発見技術が使用される。WPADは、成功するまで各技術を1つずつ試みる。現在のWPADの仕様では、以下の技術が順に定義されています。

  - ダイナミックホストディスカバリープロトコル（DHCP）
  - サービスロケーションプロトコル(SLP)
  - DNS well-known hostnames
  - DNSのSRVレコード
  - TXTレコード内のDNSサービスURI

- 詳細については、第20章をご参照ください。

## 6.5 Tricky Things About Proxy Requests
- このセクションでは、プロキシサーバーのリクエストについて、以下のような厄介で誤解されやすい点を説明します。

  - プロキシリクエストのURIがサーバーリクエストとどのように異なるか
  - プロキシのインターセプトやリバースプロキシによってサーバのホスト情報がどのように不明瞭になるか
  - URIの変更に関するルール
  - プロキシがブラウザの巧妙なURI自動補完機能やホスト名拡張機能に与える影響

### 6.5.1 Proxy URIs Differ from Server URIs
- WebサーバとWebプロキシのメッセージは、1つの例外を除いて同じ構文です。クライアントがプロキシではなくサーバにリクエストを送信する場合、HTTPリクエストメッセージ内のURIが異なります。クライアントがウェブサーバにリクエストを送信する場合、リクエスト行には、次の例に示すように、URIの一部（スキーム、ホスト、ポートを含まない）のみが含まれます。

```
GET /index.html HTTP/1.0
User-Agent: SuperBrowserv1.3
```

- しかし、クライアントがプロキシにリクエストを送信する場合、リクエストラインには完全なURIが含まれます。例えば、次のようになります。

```
GET http://www.marys-antiques.com/index.html HTTP/1.0
User-Agent: SuperBrowser v1.3 
```

- なぜ、プロキシ用とサーバー用の2つの異なるリクエストフォーマットがあるのですか？オリジナルのHTTPデザインでは、クライアントは1つのサーバーに直接話しかけていました。バーチャルホストは存在せず、プロキシについても用意されていませんでした。単一のサーバは自分のホスト名とポートを知っているため、冗長な情報を送信しないように、クライアントはスキームとホスト(およびポート)を除いた部分的なURIを送信しました。
- プロキシが登場すると、この部分的なURIが問題になりました。プロキシは、サーバへの独自の接続を確立するために、宛先サーバの名前を知る必要がありました。また、プロキシベースのゲートウェイは、FTPリソースやその他のスキームに接続するために、URIのスキームを必要としました。HTTP/1.0は、プロキシのリクエストに完全なURIを要求することで問題を解決しましたが、サーバーのリクエストには部分的なURIを保持していました（完全なURIをサポートするためにすべてのサーバーを変更するには、すでに多くのサーバーが配備されていました）[8]。
  - [8] HTTP/1.1では、サーバーはプロキシリクエストとサーバーリクエストの両方で完全なURIを処理することが要求されていますが、実際には、多くのデプロイされたサーバーはまだ部分的なURIしか受け付けません。
- そのため、部分的なURIをサーバーに、完全なURIをプロキシに送る必要があります。クライアントのプロキシ設定が明示的に設定されている場合、クライアントはどのようなタイプのリクエストを発行すべきかを知っています。

  - クライアントがプロキシを使用するように設定されていない場合は、部分URIを送信します（図6-15a）。
  - クライアントがプロキシを使用するように設定されている場合は、完全なURIを送信します（図6-15b）。

### 6.5.2 The Same Problem with Virtual Hosting
- プロキシの「scheme/host/port が見つからない」問題は、仮想ホスト型のウェブサーバーが直面する問題と同じです。仮想ホスト型ウェブサーバは、多くのウェブサイトで同じ物理的ウェブサーバを共有しています。部分的なURI /index.htmlに対するリクエストが来ると、仮想ホスト型Webサーバは目的のWebサイトのホスト名を知る必要があります(詳細はセクション5.7.1.1とセクション18.2を参照)。
- これらの問題は類似しているにもかかわらず、異なる方法で解決されました。
  - 明示的なプロキシは、リクエストメッセージに完全なURIを要求することで問題を解決しています。
  - 仮想ホストされているウェブサーバーは、ホストとポートの情報を伝えるためにHostヘッダーを必要とします。

### 6.5.3 Intercepting Proxies Get Partial URIs
- クライアントがHTTPを適切に実装している限り、クライアントは明示的に設定されたプロキシへのリクエストで完全なURIを送信します。これで問題の一部は解決しましたが、プロキシの中にはクライアントからは見えないものもあるため、クライアントがプロキシと話していることを常に知ることはできません。また、クライアントがプロキシを使用するように設定されていなくても、クライアントのトラフィックがサロゲートプロキシやインターセプトプロキシを経由する場合があります。どちらの場合も、クライアントはWebサーバーと通信していると思い込み、完全なURIを送信しません。
  - サロゲートとは、前述したように、通常はホスト名やIPアドレスを仮定してオリジンサーバーの代わりを務めるプロキシサーバーのことです。サロゲートはウェブサーバーのリクエストを受け取り、キャッシュされたレスポンスを提供したり、実際のサーバーへのプロキシリクエストを提供したりします。クライアントはサロゲートをウェブサーバと区別できないため、部分的なURIを送信します（図6-15c）。
  - 傍受プロキシとは、ネットワークの流れの中にあるプロキシサーバで、クライアントからサーバへのトラフィックをハイジャックし、キャッシュされた応答を提供するか、プロキシするものである。インターセプティングプロキシは、クライアントからサーバへのトラフィックをハイジャックするため、Webサーバに送信されるURIを部分的に受信することになる（図6-15d）[9]。
  - [9] 傍受プロキシも、状況によってはクライアントからプロキシへのトラフィックを傍受するかもしれません。その場合、傍受プロキシは完全なURIを受け取り、それを処理する必要があるかもしれません。明示的なプロキシは通常、HTTPで使用されるポートとは異なるポート(通常は80ではなく8080)で通信し、傍受するプロキシは通常、ポート80のみを傍受するので、このようなことはあまり起こらない。

### 6.5.4 Proxies Can Handle Both Proxy and Server Requests
- トラフィックがプロキシサーバにリダイレクトされる方法は様々なので、汎用プロキシサーバはリクエストメッセージ中の完全なURIと部分的なURIの両方をサポートするべきである。プロキシは、明示的なプロキシ要求であれば完全なURIを使用し、Webサーバの要求であれば部分URIと仮想Hostヘッダを使用すべきである。
- 完全なURIと部分的なURIを使用するためのルールは以下の通りです。

  - 完全なURIが提供された場合，プロキシはそれを使用すべきである。
  - 部分的なURIが提供され、Hostヘッダーが存在する場合、オリジンサーバー名とポート番号を決定するためにHostヘッダーが使用されるべきである。
  - 部分的なURIが提供され、Hostヘッダーが存在しない場合、オリジンサーバーは何らかの他の方法で決定される必要がある。
    - プロキシがオリジンサーバーの代わりに立つサロゲートである場合、 プロキシは実際のサーバーのアドレスとポート番号で設定できる。
    - トラフィックが傍受され、傍受者がオリジナルのIPアドレスとポートを利用可能にしている場合、プロキシは傍受技術から得たIPアドレスとポート番号を使用することができる(第20章参照)。
    - 他のすべてが失敗した場合、プロキシはオリジンサーバーを決定するのに十分な情報を持っていないので、エラーメッセージを返さなければならない(多くの場合、ユーザーにHostヘッダーをサポートする最新のブラウザにアップグレードするよう提案する)[10] 。
    - [10] これは気軽に行うべきではありません。ユーザーは、今まで受けたことのない不可解なエラーページを受け取ることになります。

### 6.5.5 In-Flight URI Modification
- プロキシサーバは、メッセージを転送する際にリクエストURIを変更することに十分注意する必要があります。URIのわずかな変更は、たとえそれが無害に見えたとしても、下流のサーバとの相互運用性の問題を引き起こす可能性があります。
- 一般的に、プロキシサーバは可能な限り寛容であるように努めるべきです。なぜなら、これまで機能していたサービスが大きく破壊される可能性があるからです。
- 特に、HTTP仕様では、一般的な傍受プロキシがURIを転送する際にURIの絶対パス部分を書き換えることを禁止しています。唯一の例外は、空のパスを「/」で置き換えることができることです。

### 6.5.6 URI Client Auto-Expansion and Hostname Resolution
- ブラウザは、プロキシが存在するかどうかによって、リクエストURIの解決方法が異なります。プロキシがない場合、ブラウザは入力されたURIを受け取り、対応するIPアドレスを探そうとします。ホスト名が見つかれば、ブラウザは接続に成功するまで対応するIPアドレスを探します。
- しかし、ホストが見つからない場合、多くのブラウザは、ホストの「短縮形」の略語を入力した場合に備えて、ホスト名を自動的に「拡張」しようとします（第2.3.2節に戻って参照してください）[11]。
- [11] ほとんどのブラウザでは、「yahoo」と入力すると自動的に「www.yahoo.com」と展開されます。同様に、ブラウザは "http://"というプレフィックスを省略したり、不足していれば挿入したりすることができます。
  - 多くのブラウザは、一般的なウェブサイト名の中間部分を入力した場合に備えて、「www.」のプレフィックスと「.com」のサフィックスを追加しようとします（例えば、「www.yahoo.com」の代わりに「yahoo」と入力させる場合など）。
  - 一部のブラウザでは、解決できなかったURIをサードパーティのサイトに渡して、スペルミスを修正したり、意図していたかもしれないURIを提案しようとするものもあります。
  - さらに、ほとんどのシステムのDNS設定では、ホスト名のプレフィックスだけを入力すると、DNSが自動的にドメインを検索します。ドメイン「oreilly.com」にいるときに、ホスト名「host7」と入力すると、DNSは自動的に「host7.oreilly.com」と一致させようとします。これは完全で有効なホスト名ではありません。

### 6.5.7 URI Resolution Without a Proxy
- 図6-16は、プロキシを使わずにブラウザのホスト名を自動展開した例である。ステップ2a～3cでは 有効なホスト名が見つかるまで、ブラウザはホスト名のバリエーションを検索します。
  - oreillyと入力した場合、ホストoreillyでDNS名前解決しようとして失敗し、その後ブラウザは自動拡張でoreillyをwww.oreilly.comに変換し、これでDNS名前解決を試みて成功し、通信を開始する。

### 6.5.8 URI Resolution with an Explicit Proxy
- 明示的なプロキシを使用すると、ユーザの URI がプロキシに直接渡されるため、ブラウザはこれらの便宜的な展開を行わなくなります。図 6-17 に示すように、明示的なプロキシがある場合、ブラウザは部分的なホスト名を自動展開しません。その結果、ユーザがブラウザのロケーションウィンドウに「oreilly」と入力すると、プロキシには「http://oreilly/」が送られる（ブラウザはデフォルトのスキームとパスを追加するが、ホスト名は入力したままにする）。
- このため、プロキシの中には、「www...com」の自動展開やローカルドメインのサフィックスの追加など、ブラウザの便利なサービスをできるだけ真似しようとするものがあります[12]。

### 6.5.9 URI Resolution with an Intercepting Proxy
- ホスト名の解決は、目に見えない傍受プロキシの場合は少し異なります。というのも、クライアントにとってはプロキシがないからです。挙動はサーバの場合とほぼ同じで、DNSが成功するまでブラウザはホスト名を自動展開します。しかし、図6-18が示すように、サーバへの接続時に大きな違いが生じます。
- 傍受プロキシは、ブラウザが提供するのと同等のフォールト・トレランスを提供するために、Hostヘッダ内のホスト名を解決するか、IPアドレスのDNS逆引きを行うかして、他のIPアドレスを試す必要がある。ブラウザが明示的プロキシを使用する設定の場合、プロキシにフォールト・トレランスを頼ることになるので、傍受プロキシと明示的プロキシの両方が、死んだサーバーへのDNS解決に関するフォールト・トレランスをサポートすることが重要である。

## 6.6 Tracing Messages
- 今日では、Webリクエストがクライアントからサーバーに届くまでに、2つ以上のプロキシを連鎖的に経由することも珍しくありません（図6-19）。今日、ウェブリクエストのかなりの割合がプロキシを経由しています。
- プロキシが普及するにつれ、異なるスイッチやルーターを経由したIPパケットの流れを追跡することが重要であるのと同様に、プロキシを経由したメッセージの流れを追跡し、問題を検出する能力が必要となります。

### 6.6.1 The Via Header
- Via ヘッダーフィールドには，メッセージが通過する各中間ノード（プロキシやゲートウェイ）に関する情報がリストアップされています。メッセージが他のノードを経由するたびに、その中間ノードをViaリストの最後に追加する必要があります。
- Viaヘッダーフィールドは、メッセージの転送を追跡し、メッセージのループを診断し、リクエス ト/レスポンスチェーンに沿ったすべての送信者のプロトコル機能を特定するために使用される（図 6-20）。

#### 6.6.1.1 Via syntax
- Viaヘッダーフィールドは、コンマで区切られたウェイポイントのリストを含む。各ウェイポイントは、個々のプロキシサーバまたはゲートウェイのホップを表し、その中間ノードのプロトコルとアドレスに関する情報を含みます。
  - Protocol name
  - Protocol version
  - Node name
  - Node comment

#### 6.6.1.2 Via request and response paths
- リクエストとレスポンスは通常同一のTCPコネクション上を通る。そのため、レスポンスのViaヘッダーは、ほぼ常にリクエストのViaヘッダーを反転したものになる。

#### 6.6.1.3 Via and gateways]
- プロキシはゲートウェイの機能を提供する場合があり、その場合HTTP以外のプロトコル変換についてもViaヘッダーに記録される。クライアントはレスポンスのViaヘッダーを見て、プロキシチェーンに沿ったプロトコル機能と変換について気づくことができる。

#### 6.6.1.4 The Server and Via headers
- サーバーは、サーバーレスポンスヘッダフィールドにオリジンサーバーで使われるソフトウェアを記述する。
- 応答メッセージがプロキシ経由で転送される場合は、プロキシが Server ヘッダを変更しないようにしてください。Serverヘッダーはオリジンサーバーのためのものです。代わりに、プロキシは Via エントリを追加する必要があります。

#### 6.6.1.5 Privacy and security implications of Via
- Via文字列に正確なホスト名を入れたくない場合もあります。一般的に、この動作が明示的に有効になっていない限り、プロキシサーバがネットワークのファイアウォールの一部である場合、ファイアウォールの背後にあるホストの名前とポートを転送すべきではありません。これは、ファイアウォールの背後にあるネットワークアーキテクチャの知識が悪意のある当事者に利用される可能性があるからです[14]。
- Viaノード名転送が有効でない場合、セキュリティ境界の一部であるプロキシは、ホスト名をそのホストの適切な偽名に置き換える必要があります。しかし、一般的には、プロキシは、実名が不明瞭であっても、各プロキシサーバのViaウェイポイントエントリを保持するようにすべきである。
- 内部ネットワークアーキテクチャのデザインやトポロジーを不明瞭に することに非常に強いプライバシー要求を持つ組織の場合、プロキ シは(同一のreceived-protocol値を持つ)順番に並んだViaウェイポ イントエントリを単一の結合されたエントリにまとめることができる。例えば、以下のようにである。

```
Via: 1.0 foo, 1.1 devirus.company.com, 1.1 accesslogger.company.com
->
Via: 1.0 foo, 1.1 concealed-stuff 
```

- 同じ組織の管理下にあり、ホストがすでに偽名に置き換えられている場合を除き、複数のエントリを結合しないでください。また、received-protocolの値が異なるエントリーを結合してはいけません。

### 6.6.2 The TRACE Method
- プロキシネットワークを簡単に診断するためには、HTTPプロキシネットワークを介してホップごとに転送されるメッセージがどのように変化するかを便利に観察する方法が必要です。
- HTTP/1.1 の TRACE メソッドは、プロキシの連鎖を通してリクエストメッセージを追跡し、メッセージがどのプロキシを通過し、各プロキシがリクエストメッセージをどのように変更するかを観察することができます。TRACEはプロキシの流れをデバッグするのに非常に便利です[15]。
  - [15] 残念ながら、これはまだ広く実装されていません。
- TRACEリクエストが宛先サーバに到達すると[16]、リクエストメッセージ全体がHTTPレスポンスのボディにまとめられて送信者に返信されます（図6-23参照）。TRACEレスポンスが到着すると、クライアントは、サーバが受信した正確なメッセージと、そのメッセージが通過したプロキシのリスト（Viaヘッダ内）を確認することができます。TRACEの応答は、Content-Typeがmessage/httpで、ステータスが200 OKです。
  - [16] 最終的な受信者は、オリジンサーバーか、リクエストでMax-Forwards値0を受け取った最初のプロキシまたはゲートウェイのいずれかである。

#### 6.6.2.1 Max-Forwards
- 通常、TRACEメッセージは、介在するプロキシの数に関係なく、デスティネーションサーバまで全行程を移動します。Max-Forwardsヘッダを使用して、TRACEとOPTIONSリクエストのプロキシホップ数を制限することができます。これは、無限ループでメッセージを転送するプロキシのチェーンをテストしたり、チェーンの途中にある特定のプロキシサーバーの影響をチェックするのに便利です。MaxForwardsはまた、OPTIONSメッセージの転送を制限します（セクション6.8参照）。
- Max-Forwardsリクエストヘッダーフィールドは、このリクエストメッセージ が転送される可能性のある残り回数を示す一つの整数を含む(図6-24)。Max-Forwardsの値がゼロの場合(MaxForwards: 0)、受信者がオリジンサーバーでなくても、TRACEメッセージをそれ以上転送せずにクライアントに向けて反映させなければならない。
- 受信したMax-Forwardsの値がゼロより大きい場合、転送されたメッセージには、値を1つ減らした更新されたMax-Forwardsフィールドが含まれていなければなりません。すべてのプロキシとゲートウェイはMax-Forwardsをサポートする必要があります。Max-Forwardsを使って、プロキシチェーンのどのホップでもリクエストを見ることができます。

## 6.7 Proxy Authentication
- プロキシは、アクセス制御装置として機能します。HTTPはプロキシ認証と呼ばれるメカニズムを定義しており、ユーザーがプロキシに有効なアクセス許可の認証情報を提供するまで、コンテンツへのリクエストをブロックします。
  - 制限されたコンテンツへのリクエストがプロキシサーバーに到着すると、プロキシサーバーはアクセス資格を要求する407 Proxy Authorization Requiredステータスコードを返し、その資格を提供する方法を記述したProxy-Authenticateヘッダーフィールドを添付することができる(図6-25b)。
  - 407応答を受信したクライアントは、ローカルデータベースから、またはユーザにプロンプトを表示して、必要な資格情報を収集しようとする。
  - クレデンシャルが取得されると、クライアントはProxy-Authorizationヘッダーフィールドに必要なクレデンシャルを提供してリクエストを再送する。
  - クレデンシャルが有効であれば、プロキシはオリジナルのリクエストをチェーンに沿って渡し(図6-25c)、そうでなければ別の407応答が送られる。
- 一般に、プロキシ認証は、複数のプロキシが連鎖し、それぞれが認証に参加する場合には、うまく機能しません。認証情報をプロキシチェーン内の特定のウェイポイントに関連付けるためのHTTPの拡張機能が提案されていますが、これらの拡張機能は広く実装されていません。
- HTTP の認証メカニズムの詳細については、第 12 章を参照してください。

## 6.8 Proxy Interoperation
- クライアント、サーバー、プロキシは、複数のベンダーによって、異なるバージョンのHTTP仕様で構築されています。また、様々な機能をサポートし、様々なバグを持っています。プロキシサーバは、クライアント側とサーバ側の機器の間を取り持つ必要がありますが、これらの機器は異なるプロトコルを実装していたり、面倒な癖があったりします。

### 6.8.1 Handling Unsupported Headers and Methods
- プロキシサーバは、プロキシサーバを通過するすべてのヘッダフィールドを理解しているわけではありません。いくつかのヘッダーはプロキシ自身よりも新しいかもしれないし、 他のヘッダーは特定のアプリケーションに固有のカスタマイズされたヘッダーフィールドかもしれない。プロキシは認識できないヘッダーフィールドを転送(forward)しなければならず、同じ名前のヘッダーフィールドの相対的な順番を維持しなければ ならない[17]。同様に、プロキシがあるメソッドを知らない場合、プロキシは可能であればメッセージを次のホップに転送(forward)することを 試みるべきである。
- [17] 同じフィールド名を持つ複数のメッセージヘッダーフィールドがメッ セージ中に存在するかもしれないが、それらが存在する場合、それらは等価にカンマ区切りのリストに結合できなければならない。同じフィールド名を持つヘッダーフィールドを受け取る順番は、そ れゆえに結合されたフィールド値の解釈にとって重要である。そのためプロキシはメッセージを転送(forward)するときにこれらの同じ名 称のフィールド値の相対的な順番を変更できない。
- Microsoft Outlookを介したHotmailへのアクセスでは、HTTP拡張メソッドが多用されているため、サポートされていないメソッドをトンネリングできないプロキシは、今日のほとんどのネットワークでは実行できないかもしれません。

### 6.8.2 OPTIONS: Discovering Optional Feature Support
- HTTP OPTIONS メソッドは、クライアント（またはプロキシ）に、ウェブサーバのサポートされている機能（サポートされているメソッドなど）や、ウェブサーバ上の特定のリソースを発見させます（図6-26）。クライアントはOPTIONSを使ってサーバの機能を事前に確認できるため、機能レベルの異なるプロキシやサーバとの相互運用が容易になります。
- OPTIONSリクエストのURIにアスタリスク(*)が付いている場合、そのリクエストはサーバーがサポートする機能全体に関わるものです。
- 成功すると、OPTIONS メソッドは、サーバーでサポートされている、またはリソースで利用可能なオプション機能を記述するさまざまなヘッダーフィールドを含む 200 OK 応答を返します。HTTP/1.1 が応答で指定する唯一のヘッダーフィールドは Allow ヘッダーであり、これはサーバー（またはサーバー上の特定のリソース）がどのようなメソッドをサポートしているかを記述している。[18] OPTIONS はより多くの情報を含むオプションの応答ボディを許可しているが、これは未定義である。

### 6.8.3 The Allow Header
- Allow entityヘッダーフィールドは、リクエストURIで特定されるリソース、またはリクエストURIが*の場合はサーバー全体でサポートされるメソッドのセットを列挙します。
- Allowヘッダーは、新しいリソースでサポートされるべきメソッドを推奨するために、リクエストヘッダーとして使用することができます。サーバーはこれらのメソッドをサポートする必要はなく、実際にサポートされているメソッドを列挙したAllowヘッダーをマッチング レスポンスに含めるべきである。
- プロキシは、たとえ指定されたすべてのメソッドを理解していなくても、 Allowヘッダーフィールドを修正することはできない。なぜなら、クライア ントはオリジンサーバーと話すための他のパスを持っているかもしれないから である。

# Chapter7. Caching
- Webキャッシュは、頻繁にアクセスがあるドキュメントのコピーを自動的に保持するHTTPデバイスです。Webリクエストがキャッシュに到達したとき、ローカルに「キャッシュ」されたコピーがあれば、オリジンサーバーからではなく、ローカルストレージからドキュメントが提供されます。キャッシュには次のような利点があります。
  - キャッシュは冗長なデータ転送を減らし、ネットワーク料金を節約できる。
  - キャッシュはネットワークのボトルネックを解消します。帯域幅を増やさなくてもページの読み込みが速くなります。
  - キャッシュはオリジンサーバーへの要求を軽減します。サーバーの応答が速くなり、過負荷を回避できます。
  - キャッシュは、距離による遅延を軽減します。（ページが遠くにあると読み込みが遅くなるため。）
- この章では、キャッシュがどのようにしてパフォーマンスを向上させ、コストを削減するのか、その効果をどのように測定するのか、そしてキャッシュの効果を最大化するためにはどこに配置すればよいのかを説明します。また、HTTPがキャッシュのコピーを新鮮に保つ方法や、キャッシュが他のキャッシュやサーバーとどのように相互作用するかについても説明します。

## 7.1 Redundant Data Transfers - 冗長なデータ転送
- 複数のクライアントが人気のあるオリジンサーバーのページにアクセスすると、サーバーは同じドキュメントを複数回、各クライアントに送信します。同じデータが何度もネットワーク上を行き来することになるのです。このような冗長なデータ転送は、高価なネットワーク帯域を消費し、転送速度を低下させ、ウェブサーバーに負荷をかけます。キャッシュでは、サーバーからの最初のレスポンスのコピーを保持します。後続のリクエストは、キャッシュされたコピーから実行することができ、オリジンサーバーとの間の無駄な重複したトラフィックを削減します。

## 7.2 Bandwidth Bottlenecks
- また、キャッシュはネットワークのボトルネックを解消します。多くのネットワークでは、リモートサーバーよりもローカルネットワークのクライアントに多くの帯域を提供しています（図7-1）。クライアントは、途中にある最も遅いネットワークの速度でサーバーにアクセスします。クライアントが高速なLAN上のキャッシュからコピーを取得する場合、キャッシュはパフォーマンスを向上させることができます（特に大きなドキュメントの場合）。

## 7.3 Flash Crowds
- キャッシングは、フラッシュクラウドを解消するために特に重要です。フラッシュ・クラウドとは、ニュース速報、電子メールの大量送信、有名人のイベントなどの突発的な出来事により、多くの人がほぼ同時にウェブ・ドキュメントにアクセスすることをいいます（図7-2）。その結果、冗長なトラフィックが急増し、ネットワークやWebサーバが壊滅的な状態に陥ることがあります。

## 7.4 Distance Delays
- 帯域が問題にならなくても、距離が問題になることがあります。すべてのネットワークルーターは、インターネットトラフィックに遅延をもたらします。また、クライアントとサーバーの間にそれほど多くのルーターがなくても、光の速さだけでかなりの遅延が発生します。
- 近くのマシンルームにキャッシュを配置することで、ドキュメントの移動距離を数千マイルから数十ヤードにまで縮めることができます。

## 7.5 Hits and Misses
- だから、キャッシュは役に立つ。しかし、キャッシュは世界中のすべてのドキュメントのコピーを保存するものではありません[3]。
- キャッシュに届いたリクエストの中には、利用可能なコピーを提供できる場合があります。これをキャッシュヒットと呼ぶ（図7-4a）。他のリクエストはキャッシュに到着しても、利用可能なコピーがないためにオリジン・サーバーに転送されるだけです。これはキャッシュミスと呼ばれる（図7-4b）。

### 7.5.1 Revalidations
- オリジンサーバーのコンテンツは変更される可能性があるため、キャッシュは時々、自分のコピーがサーバーとの間で最新の状態であるかどうかをチェックする必要があります。このような「新鮮さのチェック」をHTTPの再検証（revalidations）と呼びます（図7-4c）。再検証を効率的に行うために、HTTP は、サーバーからオブジェクト全体を取得することなく、コンテンツがまだ新鮮であるかどうかを迅速に確認できる特別なリクエストを定義しています。
- キャッシュは、いつでも好きなときに好きなだけコピーを再検証することができます。しかし、キャッシュには何百万ものドキュメントが含まれていることが多く、また、ネットワークの帯域幅も限られているため、ほとんどのキャッシュでは、クライアントからの要求があり、コピーが十分に古くなってから、コピーの再検証を行っています。新鮮さをチェックするための HTTP ルールについては、この章の後半で説明します。
- キャッシュがキャッシュされたコピーを再検証する必要があるときは、オリジンサーバーに小さな再検証リクエストを送ります。コンテンツが変更されていなければ、サーバーは小さな 304 Not Modified 応答を返します。キャッシュはコピーがまだ有効であることを知るとすぐに、コピーを一時的に新鮮であるとマークし、そのコピーをクライアントに提供します（図7-5a）。これを再検証ヒットまたはスローヒットと呼びます。オリジンサーバーに確認する必要があるため、純粋なキャッシュヒットよりは遅くなりますが、サーバーからオブジェクトデータを取得しないため、キャッシュミスよりは速くなります。
- HTTP には、キャッシュされたオブジェクトを再検証するためのツールがいくつかありますが、最も一般的なのは If-Modified-Since ヘッダです。GETリクエストに追加されたこのヘッダは、コピーがキャッシュされた時点以降にオブジェクトが変更された場合にのみ、そのオブジェクトを送信するようにサーバに指示します。
- ここでは、サーバーのコンテンツが変更されていない場合、サーバーのコンテンツが変更されている場合、サーバーが削除されている場合、の3つの状況で、GET If-Modified-Sinceリクエストがサーバーに到着した場合にどうなるかを説明します。

  - Revalidate hit
    - サーバーオブジェクトが変更されていない場合、サーバーはクライアントに小さな HTTP 304 Not Modified レスポンスを送信する。これを図7-6に示す。
  - Revalidate miss
    - サーバーオブジェクトが変更されている（キャッシュされたコピーと異なる）場合、サーバーはクライアントに対して、完全なコンテンツを含む通常のHTTP 200 OKレスポンスを送信します。
  - Object deleted
    - サーバーオブジェクトが削除されている場合、サーバーは404 Not Foundレスポンスを返送し、キャッシュはそのコピーを削除します。

### 7.5.2 Hit Rate
- キャッシュから処理されたリクエストの割合は、キャッシュヒット率（またはキャッシュヒットレシオ）[4]、またはドキュメントヒット率（またはドキュメントヒットレシオ）と呼ばれることもあります。ヒット率の範囲は 0 から 1 ですが、しばしばパーセンテージで表現され、0% はすべてのリクエストがミス（ネットワーク上でドキュメントを取得しなければならなかった）であり、100% はすべてのリクエストがヒット（キャッシュ内にコピーがあった）であることを意味します[5]。
  - [5] 再検証ヒットをヒット率に含めることもありますが、ヒット率と再検証ヒット率を別々に測定することもあります。ヒット率を調べる際には、何をもって「ヒット」とするかを確認してください。
- キャッシュ管理者は、キャッシュのヒット率を100%に近づけたいと考えています。実際に得られるヒット率は、キャッシュの大きさ、キャッシュユーザーの関心事がどれだけ似ているか、キャッシュデータの変更やパーソナライズの頻度、キャッシュの構成などによって異なります。ヒット率を予測するのは難しいのですが、今日のささやかなウェブキャッシュでは40％のヒット率が妥当なところでしょう。キャッシュの良いところは、控えめなサイズのキャッシュであっても、パフォーマンスを大幅に向上させ、トラフィックを削減するのに十分な人気のあるドキュメントが含まれている場合があることです。キャッシュは、有用なコンテンツがキャッシュに残るように努力します。

### 7.5.3 Byte Hit Rate
- ドキュメントのヒット率がすべてを物語るわけではありません。なぜなら、ドキュメントはすべて同じサイズではないからです。大きなオブジェクトは、そのサイズのために、アクセス頻度は低くても、データトラフィック全体への貢献度は高い場合があります。このような理由から、バイトヒット率という指標を好む人もいます（特に、1バイトごとのトラフィックに課金される人は要注意！）。
- バイトヒット率とは、転送された全バイトのうち、キャッシュから提供されたバイトの割合を表します。この指標は、トラフィック削減の度合いを表しています。バイトヒット率が100％の場合、すべてのバイトがキャッシュから提供され、インターネット上にトラフィックが流れないことを意味します。
- ドキュメント・ヒット・レートとバイト・ヒット・レートは、どちらもキャッシュのパフォーマンスを測るのに有効です。ドキュメントヒット率は、どれだけ多くのウェブトランザクションが送信ネットワークから遮断されたかを表します。トランザクションには固定された時間があり、その時間はしばしば大きくなります（例えば、サーバーへのTCP接続の設定など）。
- ドキュメントヒット率を向上させることで、全体的なレイテンシー（遅延）の削減が最適化されます。バイト・ヒット・レートは、インターネットからどれだけ多くのバイトが削除されたかを示します。バイトヒット率を向上させることで、帯域幅の節約が最適化されます。

### 7.5.4 Distinguishing Hits and Misses
- 残念ながら、HTTPではクライアントがレスポンスがキャッシュヒットなのかオリジンサーバーからのアクセスなのかを判別する方法がありません。どちらの場合も、レスポンスコードは200 OKで、レスポンスにボディがあることを示します。商用のプロキシキャッシュの中には、キャッシュで何が起こったかを説明するために、Viaヘッダーに追加情報を付けるものもあります。
- 通常、クライアントが応答がキャッシュから来たかどうかを検知する方法の一つは、Dateヘッダを使用することです。応答中のDateヘッダの値を現在の時刻と比較することで、クライアントは多くの場合、その古い日付の値によってキャッシュされた応答を検出することができる。また、クライアントがキャッシュされた応答を検知する別の方法として、応答の古さを示すAgeヘッダーがあります（「Age」を参照）。

## 7.6 Cache Topologies
- キャッシュには、1人のユーザー専用のものと、数千人のユーザーで共有するものがあります。専用キャッシュは、プライベートキャッシュと呼ばれます。プライベート・キャッシュは個人用のキャッシュであり、1人のユーザーに人気のあるページが含まれる（図7-7a）。共有キャッシュはパブリックキャッシュと呼ばれます。パブリック・キャッシュには、ユーザー・コミュニティで人気のあるページが含まれています（図7-7b）。

### 7.6.1 Private Caches
- プライベート・キャッシュは馬力や記憶容量をあまり必要としないため、小型で安価に作ることができます。ウェブブラウザには、プライベートキャッシュが組み込まれています。ほとんどのブラウザは、人気のあるドキュメントをパソコンのディスクやメモリにキャッシュし、キャッシュのサイズや設定を変更することができます。また、ブラウザのキャッシュの中を覗いて、何が入っているかを確認することもできます。

### 7.6.2 Public Proxy Caches
- パブリックキャッシュは、キャッシングプロキシサーバー、あるいはより一般的にはプロキシキャッシュと呼ばれる、特別な共有プロキシサーバーです（プロキシについては第6章で説明しました）。プロキシキャッシュは、ローカルキャッシュからドキュメントを提供したり、ユーザーに代わってサーバーに問い合わせたりします。パブリックキャッシュは複数のユーザーからのアクセスを受けるため、冗長なトラフィックを排除する機会が多い[6]。
  - [6] パブリック・キャッシュはユーザ・コミュニティの多様な関心事をキャッシュするので、個々のユーザの関心事に振り回されることなく、人気のあるドキュメントのセットを保持するのに十分な大きさが必要である。
- 図7-8aでは、各クライアントが新しい「ホット」なドキュメント(まだプライベート・キャッシュに入っていない)に冗長にアクセスしています。各プライベート・キャッシュは同じドキュメントをフェッチし、ネットワークを何度も横断する。図7-8bのように、共有されたパブリック・キャッシュでは、キャッシュは人気のあるオブジェクトを一度だけフェッチする必要があり、共有コピーを使用してすべてのリクエストを処理するため、ネットワーク・トラフィックが減少する。
- プロキシ・キャッシュは、第6章で説明したプロキシのルールに従います。プロキシキャッシュを使用するようにブラウザを設定するには、手動でプロキシを指定するか、プロキシの自動設定ファイルを設定します（6.4.1項参照）。また、インターセプトプロキシを使用すると、ブラウザを設定せずに、キャッシュを介してHTTPリクエストを強制的に実行することができます（第20章参照）。

### 7.6.3 Proxy Cache Hierarchies
- 実際には、多くの場合、キャッシュの階層を展開することが理にかなっています。小さなキャッシュでのキャッシュミスは、残りの「蒸留」トラフィックを処理するより大きな親キャッシュに流されます。図7-9は、2階層のキャッシュ階層を示しています[7]。このアイデアは、クライアントの近くに小さくて安価なキャッシュを使用し、多くのユーザーが共有するドキュメントを保持するために、上位の階層にはより強力なキャッシュを使用するというものです[8]。
  - [7] クライアントがブラウザキャッシュを持つブラウザの場合、図7-9は技術的には3階層のキャッシュ階層を示しています。
  - [8] 親キャッシュは、より多くのユーザに人気のあるドキュメントを保持するために、より大きく、より高性能である必要があるかもしれません。なぜなら、親キャッシュは、関心事が多様である可能性のある多くの子のトラフィックを集約して受け取るからです。
- うまくいけば、ほとんどのユーザは近くのレベル1キャッシュでキャッシュ・ヒットを得ることができます（図7-9aに示すように）。そうでない場合は、より大きな親キャッシュでリクエストを処理できるかもしれません(図7-9b)。深いキャッシュ階層では、キャッシュの長いチェーンを経由することが可能ですが、介在する各プロキシは、プロキシのチェーンが長くなると顕著になる、いくつかのパフォーマンスペナルティを課します[9]。
  - [9] 実際には、ネットワークアーキテクトは、連続するプロキシを2つまたは3つに制限しようとする。しかし、新世代の高性能プロキシサーバが登場すれば、プロキシチェーンの長さが問題にならなくなるかもしれない。

### 7.6.4 Cache Meshes, Content Routing, and Peering
- ネットワークアーキテクトの中には、単純なキャッシュ階層ではなく、複雑なキャッシュメッシュを構築する人もいる。
キャッシュメッシュ内のプロキシキャッシュは、より洗練された方法で相互に通信し、どの親キャッシュと通信するか、あるいはキャッシュを完全にバイパスしてオリジンサーバに直接接続するかなど、動的なキャッシュ通信の決定を行います。このようなプロキシキャッシュは、コンテンツへのアクセス、管理、および配信の方法についてルーティングの決定を行うため、コンテンツルータと表現することができます。
- キャッシュメッシュ内のコンテンツルーティングのために設計されたキャッシュは、以下のすべてを行うことができます（中略）。
  - URL に基づいて、親キャッシュまたはオリジンサーバを動的に選択する。
  - URLに基づいて、親キャッシュを動的に選択する。
  - 親キャッシュにアクセスする前に、ローカルエリアのキャッシュを検索してキャッシュされたコピーを探す。
  - 他のキャッシュに、自らにキャッシュされたコンテンツの一部へのアクセスを許可するが、自らのキャッシュを経由したインターネットの通過は許可しない。
- このようなキャッシュ間のより複雑な関係により、異なる組織が相互にピアリングを行い、相互利益のためにキャッシュを接続することができます。選択的なピアリングサポートを提供するキャッシュは、兄弟キャッシュと呼ばれます（図7-10）。HTTPは兄弟キャッシュをサポートしていないため、インターネット・キャッシュ・プロトコル(ICP)やハイパーテキスト・キャッシュ・プロトコル(HTCP)などのプロトコルでHTTPを拡張しています。これらのプロトコルについては、第20章で説明します。

## 7.7 Cache Processing Steps
- 最近の商用プロキシキャッシュは非常に複雑です。非常に高性能で、HTTPやその他の技術の高度な機能をサポートするように作られています。しかし、微妙な違いはあっても、ウェブキャッシュの基本的な動作はほとんど単純です。HTTP GET メッセージの基本的なキャッシュ処理シーケンスは、7 つのステップで構成されています（図 7-11 に図示）。

1. 受信-キャッシュはネットワークから到着したリクエストメッセージを読み込みます。
2. 解析-キャッシュはメッセージを解析し、URL とヘッダを抽出します。
3. ルックアップ-キャッシュは、ローカルコピーが利用可能かどうかをチェックし、利用可能でない場合はコピーをフェッチします（そしてローカルに保存します）。
4. 新鮮さチェック-キャッシュされたコピーが十分に新鮮であるかどうかをチェックし、新鮮でない場合はサーバーに更新を要求します。
5. レスポンスの作成-キャッシュは、新しいヘッダとキャッシュされた本文を含むレスポンスメッセージを作成します。
6. 送信-キャッシュはネットワーク経由でクライアントにレスポンスを送信します。
7. ロギング-オプションとして、キャッシュはトランザクションを説明するログファイルエントリを作成します。

### 7.7.1 Step 1: Receiving
- ステップ1では、キャッシュがネットワーク接続のアクティビティを検出し、受信データを読み取る。高性能なキャッシュは、複数の受信接続から同時にデータを読み取り、メッセージ全体が到着する前にトランザクションの処理を開始します。

### 7.7.2 Step 2: Parsing
- 次に、キャッシュはリクエストメッセージを断片的に解析し、ヘッダ部分を操作しやすいデータ構造に配置します。これにより、キャッシュソフトウェアがヘッダーフィールドを処理したり、いじったりすることが容易になります[10]。

### 7.7.3 Step 3: Lookup
- ステップ3では、キャッシュがURLを受け取り、ローカルコピーをチェックします。ローカルコピーは、メモリに保存されている場合もあれば、ローカルディスクに保存されている場合もありますし、近くの別のコンピュータに保存されている場合もあります。プロ仕様のキャッシュでは、高速なアルゴリズムを使用して、オブジェクトがローカルキャッシュで利用可能かどうかを判断します。ドキュメントがローカルで利用できない場合は、状況や設定に応じて、オリジンサーバーや親プロキシから取得したり、失敗を返したりします。
- キャッシュされたオブジェクトには、サーバーのレスポンスボディとオリジナルのサーバーレスポンスヘッダーが含まれているため、キャッシュヒット時に正しいサーバーヘッダーを返すことができます。また、キャッシュされたオブジェクトにはメタデータが含まれており、オブジェクトがキャッシュに置かれていた期間や使用回数などを記録するために使用されます[11]。

### 7.7.4 Step 4: Freshness Check
- HTTPでは、キャッシュにサーバーのドキュメントのコピーを一定期間保持させることができます。この期間中、ドキュメントは「新鮮」とみなされ、キャッシュはサーバーに問い合わせることなくドキュメントを提供することができます。しかし、キャッシュされたコピーが長時間放置され、ドキュメントの新鮮さの限界を超えると、オブジェクトは「陳腐化」したとみなされ、キャッシュはサーバーとの再検証を行い、ドキュメントの変更をチェックしてから提供する必要があります。さらに事を複雑にしているのは、クライアントがキャッシュに送信するリクエストヘッダであり、これはキャッシュに対して再検証の試行、または完全な検証回避を強制したりすることができます。
- HTTP には鮮度チェックのための非常に複雑なルールがありますが、これはキャッシュ製品がサポートする多数の設定オプションや、HTTP 以外の鮮度標準との相互運用の必要性によって悪化しています。この章の残りの部分のほとんどを、鮮度計算の説明に充てることにします。

### 7.7.5 Step 5: Response Creation
- キャッシュされたレスポンスはオリジンサーバーから来たように見せたいので、キャッシュは、キャッシュされたサーバーのレスポンスヘッダーをレスポンスヘッダーの出発点として使用します。これらのベースとなるヘッダーは、キャッシュによって修正され、補強されます。
- キャッシュは、クライアントに合わせてヘッダを適応させる役割を果たします。例えば、クライアントが HTTP/1.1 レスポンスを期待しているのに、サーバが HTTP/1.0 レスポンス (あるいは HTTP/0.9 レスポンス) を返すことがあります。この場合、キャッシュはそれに合わせてヘッダを変換する必要があります。キャッシュはまた、キャッシュの鮮度情報 (Cache-Control、Age、Expires ヘッダ) を挿入し、多くの場合、プロキシキャッシュがリクエストを処理したことを示す Via ヘッダを含めます。
- キャッシュは Date ヘッダーを調整してはいけないことに注意してください。Date ヘッダーは、オブジェクトがオリジンサーバーで最初に生成されたときの日付を表します。

### 7.7.6 Step 6: Sending
- レスポンスヘッダーの準備ができると、キャッシュはレスポンスをクライアントに送り返します。他のプロキシサーバと同様に、プロキシキャッシュはクライアントとの接続を管理する必要がある。高性能のキャッシュは、データを効率的に送信するために懸命に働き、しばしばローカルストレージとネットワークI/Oバッファ間でのドキュメントコンテンツのコピーを回避します。

### 7.7.7 Step 7: Logging
- ほとんどのキャッシュは、キャッシュの使用状況に関するログファイルと統計情報を保持します。各キャッシュトランザクションが完了すると、キャッシュはキャッシュのヒット数とミス数 (およびその他の関連メトリクス) をカウントする統計情報を更新し、リクエストタイプ、URL、および何が起こったかを示すエントリをログファイルに挿入します。
- 最も一般的なキャッシュログフォーマットは、SquidログフォーマットとNetscape拡張コモンログフォーマットですが、多くのキャッシュ製品では、カスタムログファイルを作成することができます。ログファイルのフォーマットについては、第21章で詳しく説明します。

### 7.7.8 Cache Processing Flowchart
- 図 7-12 は、キャッシュが URL を GET するリクエストをどのように処理するかを簡略化して示したものである[12]。
- [12] 図 7-12 で説明したリソースの再検証とフェッチは、条件付きリクエストで一度に行うことができる（セクション 7.8.4 参照）。

## 7.8 Keeping Copies Fresh
- キャッシュされたコピーは、サーバー上のドキュメントとすべてが一致するとは限りません。キャッシュは、常に古いデータを提供していては意味がありません。キャッシュされたデータは、サーバーのデータとの一貫性を保つ必要があります。
- HTTP には、どのキャッシュにドキュメントのコピーがあるかをサーバーが覚えていなくても、キャッシュデータとサーバーとの一貫性を十分に保つための簡単なメカニズムがあります。HTTP はこれらの単純な仕組みを、ドキュメントの失効とサーバーの再検証と呼んでいます。

### 7.8.1 Document Expiration

### 7.8.2 Expiration Dates and Ages

### 7.8.3 Server Revalidation

### 7.8.4 Revalidation with Conditional Methods

### 7.8.5 If-Modified-Since: Date Revalidation

### 7.8.6 If-None-Match: Entity Tag Revalidation

### 7.8.7 Weak and Strong Validators

### 7.8.8 When to Use Entity Tags and Last-Modified Dates

## 7.9 Controlling Cachability
- HTTPでは、サーバーがドキュメントの有効期限が切れるまでのキャッシュ期間を指定する方法がいくつか定義されています。優先度の高い順に、サーバーは以下の方法を取ることができます。
  - レスポンスに Cache-Control: no-store ヘッダを付加する。
  - Cache-Control: must-revalidate ヘッダを応答に添付する。
  - レスポンスに Cache-Control: no-cache ヘッダーを付ける。
  - レスポンスに Cache-Control: max-age ヘッダーを付けます。
  - レスポンスに Expires date ヘッダーを付けます。
  - 有効期限情報を添付せず、キャッシュに自身の発見的な有効期限を決定させます。
- このセクションでは、キャッシュ制御ヘッダーについて説明します。次の 7.10 節では、コンテンツごとに異なるキャッシュ情報を割り当てる方法について説明します。

### 7.9.1 No-Cache and No-Store Headers
- HTTP/1.1 には、キャッシュされないオブジェクトをマークする方法がいくつかあります。技術的には、これらのキャッシュ不可能なページは、決してキャッシュに保存されるべきではなく、したがって、新鮮さを計算する段階に達することはありません。ここでは、ドキュメントをキャッシュ不能にするHTTPヘッダをいくつか紹介します。

```
Pragma: no-cache
Cache-Control: no-cache
Cache-Control: no-store
```

- RFC 2616 では、キャッシュが「no-cache」とマークされたレスポンスを保存することを許可していますが、キャッシュはレスポンスを提供する前にオリジンサーバーでレスポンスを再検証する必要があります。no-store "とマークされた応答は、キャッシュがその応答のコピーを作成することを禁止します。キャッシュはこのレスポンスを保存してはいけません。
- Pragma: no-cache ヘッダーは、HTTP 1.0+ との下位互換性のために HTTP 1.1 に含まれています。技術的には HTTP リクエストに対してのみ有効で定義されていますが、HTTP 1.0 と 1.1 の両方のリクエストおよびレスポンスの拡張ヘッダとして広く使用されています。HTTP 1.1 アプリケーションは、CacheControl: no-cache を使用する必要があります。ただし、Pragma: no-cache しか理解できない HTTP 1.0 アプリケーションを扱う場合は例外です。

### 7.9.2 Max-Age Response Headers
- Cache-Control: max-age ヘッダーは、ドキュメントがサーバーから送られてきてから何秒経ったら新鮮だと判断できるかを示します。また、s-maxageヘッダー（"maxage "にハイフンがないことに注意）もあります。これはmax-ageと同じように動作しますが、共有（パブリック）キャッシュにのみ適用されます。

```
Cache-Control: max-age=3600
Cache-Control: s-maxage=3600
```

- サーバーは、最大経過時間をゼロに設定することで、ドキュメントをキャッシュしないか、またはアクセスごとにリフレッシュするようキャッシュに要求できます。

```
Cache-Control: max-age=0
Cache-Control: s-maxage=0
```

### 7.9.3 Expires Response Headers
- 非推奨の Expires ヘッダーは、秒単位の時間ではなく、実際の有効期限を指定します。後にHTTPの設計者は、多くのサーバの時計は同期していないか不正確であるため、期限切れを絶対時間ではなく経過秒数で表現する方が良いと判断しました。同様の新鮮さの寿命は、expires値とdate値の差の秒数を計算することで算出できます。

```
Expires: Fri, 05 Jul 2002, 05:00:00 GMT
```

- サーバによっては、Expires: 0 応答ヘッダを送り返して、ドキュメントが常に期限切れになるようにしようとするものもありますが、この構文は違法であり、一部のソフトウェアで問題が発生する可能性があります。この構文を入力としてサポートするようにすべきですが、生成すべきではありません。

### 7.9.4 Must-Revalidate Response Headers
- Cache-Control: must-revalidate レスポンスヘッダーは、新鮮さの計算メカニズムをバイパスし、アクセスごとに再検証するようキャッシュに指示します。

```
Cache-Control: must-revalidate
```

- レスポンスにこのヘッダを付けることは、実際には Cache-Control: no-cache を使うよりも強力なキャッシュ制限となります。というのも、このヘッダはキャッシュに対して、キャッシュされたコピーを提供する前に必ずレスポンスを再検証するように指示するからです。これは、サーバが利用できない場合でも同様です。その場合、キャッシュはレスポンスを再検証できないので、キャッシュされたコピーを提供すべきではありません。no-store" 指示だけがキャッシュの動作をより制限します。 no-store 指示はキャッシュにリソースのコピーを作成しないように指示するからです (それによって、キャッシュは常にリソースを取得することになります)。

### 7.9.5 Heuristic Expiration
- 応答が Cache-Control: max-age ヘッダーも Expires ヘッダーも含んでいない場合、キャッシュはヒューリスティックな最大ageを計算してもよい。どのようなアルゴリズムを使ってもよいが、結果として最大年齢が24時間を超える場合には、応答ヘッダにHeuristic Expiration Warning (Warning 13)ヘッダを追加すべきである。私たちの知る限り、この警告情報をユーザーに提供しているブラウザはほとんどありません。
- 一般的なヒューリスティック満了アルゴリズムのひとつである LM-Factor アルゴリズムは、ドキュメントに最終更新日が含まれている場合に使用できる。LM-Factor アルゴリズムでは、last-modified の日付をドキュメントの変化しやすさの推定値として使用する。

### 7.9.6 Client Freshness Constraints

### 7.9.7 Cautions

## 7.10 Setting Cache Controls
- Webサーバによって、HTTPのcache-controlやexpirationヘッダを設定する仕組みが異なります。このセクションでは、人気の高い Apache ウェブサーバがどのようにキャッシュコントロールをサポートしているかについて簡単に説明します。具体的な詳細については、ウェブサーバのマニュアルを参照してください。

### 7.10.1 Controlling HTTP Headers with Apache
- Apache ウェブサーバは、HTTP キャッシュを制御するヘッダを設定するためのいくつかのメカニズムを提供しています。これらのメカニズムの多くは、デフォルトでは有効になっていませんので、それらを有効にする必要があります（場合によっては、最初にApacheの拡張モジュールを入手する必要があります）。ここでは、Apache の機能のいくつかについて簡単に説明します。

- mod_headers
  - mod_headers モジュールは個々のヘッダを設定することができます。このモジュールがロードされると、個々の HTTP ヘッダを設定するためのディレクティブで Apache の設定ファイルを補強することができます。また、これらの設定を Apache の正規表現やフィルタと組み合わせて使うことで、 ヘッダを個々のコンテンツに関連付けることができます。以下は、ディレクトリ内のすべての HTML ファイルを読めないようにする設定の例です。
  ```
  <Files *.html>
   Header set Cache-control no-cache
  </Files>
  ```

- mod_expires
  - mod_expires モジュールは、正しい有効期限を持つ Expires ヘッダを 自動的に生成するプログラムロジックを提供します。このモジュールを使うと、ドキュメントが最後にアクセスされてから、 あるいは最後に修正されてから、ある期間の有効期限を設定することが できます。また、このモジュールでは、ファイルタイプごとに異なる有効期限を設定したり、「access plus 1 month」のような便利な冗長記述を使ってキャッシュ可能性を表現したりすることができます。以下にいくつかの例を示します。

- mod_cern_meta
  - mod_cern_meta モジュールは、HTTP ヘッダのファイルを特定のオブジェクトと関連付けることができます。このモジュールを有効にすると、制御したいドキュメントごとに「メタファイル」のセットを作成し、各メタファイルに必要なヘッダを追加します。

### 7.10.2 Controlling HTML Caching Through HTTP-EQUIV

## 7.11 Detailed Algorithms

## 7.12 Caches and Advertising

# Chapter8.  Integration Points: Gateways, Tunnels, and Relays
- HTTP は、Web のすべてのリソースのためのプロトコルとして使用されていますが、他のアプリケーションやアプリケーションプロトコルが仕事をするために利用するプロトコルでもあります。本章では、開発者がHTTPを使ってさまざまなリソースにアクセスするために考え出した手法を概観し、他のプロトコルやアプリケーションの通信を可能にするフレームワークとして、開発者がどのようにHTTPを使用しているかを検証します。
- 本章では、以下について説明します。

  - ゲートウェイ: HTTPと他のプロトコルやアプリケーションとのインターフェースを提供します。
  - 異なる種類のウェブアプリケーション同士の通信を可能にするアプリケーションインターフェース
  - トンネル：HTTP 接続を介して非HTTPトラフィックを送信することができます。
  - リレー：HTTP プロキシを簡略化したもので、データを 1 ホップずつ転送するために使用されます。

## 8.1 Gateways
- HTTPの拡張機能やインターフェースの歴史は、人々のニーズに支えられてきました。より複雑なリソースをWebに載せたいという要望が出てきたとき、想像しうるすべてのリソースを1つのアプリケーションで扱うことは不可能であることがすぐに明らかになりました。
- この問題を解決するために、開発者たちは、リソースへのアクセス方法を抽象化し、一種のインタープリタとしての役割を果たすゲートウェイという概念を考え出しました。ゲートウェイは、リソースとアプリケーションをつなぐ役割を果たします。アプリケーションは、HTTPやその他の定義されたインターフェイスを通じて、ゲートウェイにリクエストの処理を依頼し、ゲートウェイはレスポンスを提供することができます。ゲートウェイは、データベースへのクエリ言語を話したり、ダイナミックコンテンツを生成したりすることができ、ポータルのような役割を果たします（リクエストが入り、レスポンスが出る）。
- 図8-1は、リソース・ゲートウェイの一例を示したものである。ここでは、Joe's Hardwareサーバーが、データベース・コンテンツへのゲートウェイとして機能しています。クライアントは、単にHTTPでリソースを要求しているだけであり、Joe's Hardwareサーバーは、リソースを得るためにゲートウェイと連携していることがわかります。
- ゲートウェイの中には、HTTPトラフィックを他のプロトコルに自動的に変換するものがあり、クライアントが他のプロトコルを知らなくても、HTTPクライアントが他のアプリケーションとインタフェースできるようになっている（図8-2）。
- 図8-2にゲートウェイの3つの例を示す。
  - 図8-2aでは、ゲートウェイはFTPのURLに対するHTTPリクエストを受信する。その後、ゲートウェイはFTP接続を開き、FTPサーバに適切なコマンドを発行する。ドキュメントは、正しいHTTPヘッダーとともに、HTTPで送り返される。
  - 図8-2bでは、ゲートウェイはSSLを介して暗号化されたWebリクエストを受信し、リクエストを復号化して[1]、通常のHTTPリクエストを宛先サーバに転送しています。これらのセキュリティ・アクセラレータは、ウェブ・サーバの前に直接（通常は同じ敷地内に）設置することで、オリジン・サーバに高性能な暗号化を提供することができます。
    - [1] ゲートウェイには、適切なサーバ証明書をインストールする必要がある。
  - 図8-2cでは、ゲートウェイは、アプリケーション・サーバー・ゲートウェイAPIを介して、HTTPクライアントをサーバーサイドアプリケーション・プログラムに接続している。ウェブ上の電子商取引店で購入したり、天気予報を確認したり、株価を入手したりするときには、アプリケーション・サーバー・ゲートウェイにアクセスしていることになる。

### 8.1.1 Client-Side and Server-Side Gateways
- Webゲートウェイは、一方ではHTTPを話し、もう一方では異なるプロトコルを話します[2]。
  - [2] 異なるバージョンのHTTP間で変換を行うWebプロキシは、当事者間で交渉するための高度なロジックを実行するため、ゲートウェイのようなものです。しかし、双方でHTTPを使用しているため、技術的にはプロキシとなります。
- ゲートウェイは、クライアント側とサーバー側のプロトコルをスラッシュで区切って記述します。

```
<client-protocol>/<server-protocol>
```

- つまり、HTTPクライアントとNNTPニュースサーバーをつなぐゲートウェイは、HTTP/NNTPゲートウェイということになります。ここでは、ゲートウェイのどの側で変換が行われるかを表すために、「サーバーサイドゲートウェイ」と「クライアントサイドゲートウェイ」という言葉を使っています。

  - サーバーサイド・ゲートウェイは、クライアントとはHTTP、サーバーとはHTTP以外のプロトコル（HTTP/*）で会話します。
  - クライアントサイド・ゲートウェイは、クライアントとはHTTP以外のプロトコルを、サーバーとはHTTPを話す（*/HTTP）。

## 8.2 Protocol Gateways
- HTTPトラフィックをゲートウェイに誘導するには、トラフィックをプロキシに誘導するのと同じ方法があります。一般的には、ゲートウェイを使用するようにブラウザを明示的に設定したり、透過的にトラフィックをインターセプトしたり、サロゲート（リバース・プロキシ）としてゲートウェイを設定したりします。
- 図8-3は、サーバー側のFTPゲートウェイを使用するようにブラウザを設定するためのダイアログ・ボックスを示しています。図の構成では、ブラウザは、すべてのFTP URLに対してHTTP/FTPゲートウェイとしてgw1.joes-hardware.comを使用するように構成されている。FTPサーバーにFTPコマンドを送信する代わりに、ブラウザはポート8080のHTTP/FTPゲートウェイgw1.joes-hardware.comにHTTPコマンドを送信します。
- このゲートウェイ構成の結果を図8-4に示す。通常のHTTPトラフィックは影響を受けず、オリジンサーバに直接流れ続ける。しかし，FTPのURLに対するリクエストは，HTTPリクエストの中でゲートウェイ gw1.joes-hardware.comに送られる．ゲートウェイはクライアントに代わってFTPトランザクションを実行し，結果をHTTPでクライアントに返す．
- 次のセクションでは、一般的なゲートウェイの種類である、サーバー・プロトコル・コンバータ、サーバーサイド・セキュリティ・ゲートウェイ、クライアントサイド・セキュリティ・ゲートウェイ、アプリケーション・サーバーについて説明します。

### 8.2.1 HTTP/*: Server-Side Web Gateways
- サーバ側のWebゲートウェイは、クライアント側のHTTPリクエストをforeignプロトコルに変換して、オリジンサーバに送信します（図8-5参照）。
- 図8-5では、ゲートウェイはFTPリソースに対するHTTPリクエストを受信する。

```
ftp://ftp.irs.gov/pub/00-index.txt
```

- ゲートウェイは、オリジン・サーバの FTP ポート（ポート 21）への FTP 接続を開き、オブジェクトを取得するために FTP プロトコルを使用する。ゲートウェイは次のことを行う．

  - サーバーにログインするためにUSERコマンドとPASSコマンドを送信する。
  - CWDコマンドを発行し、サーバ上の適切なディレクトリに変更する。
  - ダウンロードタイプをASCIIに設定する
  - MDTMでドキュメントの最終更新時刻を取得する。
  - PASVでパッシブ・データ・リトリーバル・リクエストを期待するようにサーバーに伝える
  - RETRを使ってオブジェクトの検索を要求する
  - コントロール・チャネルで返されたポートでFTPサーバへのデータ接続を開き、データ・チャネルが開かれると同時に、オブジェクト・コンテンツがゲートウェイに戻ってくる

- 検索が完了すると、オブジェクトはHTTPレスポンスでクライアントに送信されます。

### 8.2.2 HTTP/HTTPS: Server-Side Security Gateways
- ゲートウェイは、すべての受信ウェブ・リクエストを暗号化することにより、組織のプライバシーとセキュリティを強化するために使用することができる。クライアントは通常の HTTP を使用して Web を閲覧することができるが、ゲートウェイはユーザのセッションを自動的に暗号化する（図 8-6）。

### 8.2.3 HTTPS/HTTP: Client-Side Security Accelerator Gateways
- 最近では、セキュリティ・アクセラレータとして HTTPS/HTTP ゲートウェイが普及している。これらの HTTPS/HTTP ゲートウェイは、通常は目に見えないインターセプト・ゲートウェイまたはリバース・プロキシとして、ウェブ・サーバの前に置かれる。これらのゲートウェイは安全な HTTPS トラフィックを受信し、その安全なトラ フィックを復号化し、ウェブ・サーバに対して通常の HTTP リクエストを行う（図 8-7）。
- これらのゲートウェイはしばしば特別な復号化ハードウェアを含み、オリジン・サーバよりもはるかに効率的にセキュ ア・トラフィックを復号化し、オリジン・サーバの負荷を軽減する。これらのゲートウェイは、ゲートウェイとオリジン・サーバー間で暗号化されていないトラフィックを送信するため、ゲートウェイとオリジン・サーバー間のネットワークが安全であることを確認するために注意を払う必要があります。

## 8.3 Resource Gateways
- これまでは、ネットワーク上でクライアントとサーバーをつなぐゲートウェイについて説明してきました。しかし、ゲートウェイの最も一般的な形態であるアプリケーションサーバーは、宛先サーバーとゲートウェイを1つのサーバーにまとめたものです。アプリケーション・サーバーは、クライアントとHTTPを話し、サーバー側のアプリケーション・プログラムに接続するサーバー側のゲートウェイである（図8-8参照）。
- 図8-8では、2つのクライアントがHTTPを使ってアプリケーション・サーバに接続している。しかし、アプリケーション・サーバは、サーバからファイルを送り返すのではなく、ゲートウェイ・アプリケーション・プログラミング・インターフェース（API）を介して、サーバ上で実行されているアプリケーションにリクエストを渡している。

  - クライアントAのリクエストを受信し、URIに基づいて、APIを介してデジタルカメラアプリケーションに送信されます。結果として得られたカメラ画像は、HTTPレスポンスメッセージにまとめられてクライアントに返送され、クライアントのブラウザで表示されます。
  - クライアントBのURIは、電子商取引アプリケーション用です。クライアントBのリクエストは、サーバ・ゲートウェイAPIを介して電子商取引ソフトウェアに送信され、その結果がブラウザに返信されます。電子商取引ソフトウェアはクライアントと対話し、ユーザーに一連のHTMLページを表示させ、購入を完了させます。

- アプリケーションゲートウェイ用の最初の一般的なAPIは、CGI（Common Gateway Interface）でした。CGIは、ウェブサーバーが特殊なURLに対するHTTPリクエストに応じてプログラムを起動し、プログラムの出力を収集してHTTPレスポンスで送り返すために使用する標準化されたインターフェースのセットです。ここ数年、商用のウェブサーバーは、ウェブサーバーとアプリケーションを接続するための、より洗練されたインターフェースを提供してきました。
- 初期のウェブサーバは非常にシンプルな作りで、ゲートウェイ用のインターフェースを実装するために取られたシンプルなアプローチは、今日まで受け継がれています。
- ゲートウェイを必要とするリソースへのリクエストが来ると、サーバーはそのリクエストを処理するヘルパーアプリケーションを起動します。ヘルパーアプリケーションは、必要なデータを渡されます。多くの場合、これはリクエスト全体か、ユーザーがデータベース上で実行したいクエリのようなものです（URLのクエリ文字列から）。
- そして、レスポンスやレスポンスデータをサーバーに返し、サーバーはそれをクライアントに伝えます。サーバーとゲートウェイは別のアプリケーションなので、責任の所在が明確になっています。図8-9は、サーバーとゲートウェイのアプリケーションのやり取りの基本的な仕組みを示しています。
- この単純なプロトコル（リクエストイン、ハンドオフ、レスポンス）は、最も古く、最も一般的なサーバー拡張インターフェースの1つであるCGIの背後にある本質である。

### 8.3.1 Common Gateway Interface (CGI)
- Common Gateway Interfaceは、最初の、そしておそらく今でも最も広く使われているサーバー拡張機能です。CGIは、動的なHTML、クレジットカードの処理、データベースへの問い合わせなど、ウェブ全体で使用されています。
- CGIアプリケーションはサーバーから独立しているため、Perl、Tcl、C、各種シェル言語など、ほとんどの言語で実装することができます。また、CGIはシンプルなので、ほとんどのHTTPサーバーがサポートしています。CGIモデルの基本的な仕組みを図8-9に示します。
- CGIの処理はユーザーには見えません。クライアントの視点では、通常のリクエストを行っているだけです。クライアントは、サーバーとCGIアプリケーションの間で行われているハンドオフ手順を全く知りません。クライアントの唯一のヒントは、CGIアプリケーションが関与しているかもしれないということであり、URLに「cgi」という文字と「?」があることです。
- では、CGIは素晴らしいものですよね？まあ、イエスでもありノーでもあります。CGIは、サーバーとほとんどすべてのタイプのリソースとの間にシンプルで機能的な形態の接着剤を提供し、発生する必要のあるすべての翻訳を処理します。また、このインターフェースは、バグのある拡張機能からサーバーを保護するという点でも優れています（拡張機能がサーバー自体に組み込まれていると、エラーが発生してサーバーがクラッシュしてしまう可能性があります）。
- しかし、この分離はパフォーマンスの面でコストがかかります。CGIリクエストごとに新しいプロセスを生成するためのオーバーヘッドは非常に高く、CGIを使用するサーバーのパフォーマンスを制限し、サーバーマシンのリソースに負担をかけます。この問題を回避するために、新しい形式のCGI - 正確にはFast CGIと呼ばれる - が開発されました。このインターフェイスはCGIに似ていますが、永続的なデーモンとして実行されるため、リクエストごとに新しいプロセスを立ち上げたり削除したりすることによるパフォーマンスの低下を防ぐことができます。

### 8.3.2 Server Extension APIs
- CGIプロトコルは、外部のインタープリタと純正のHTTPサーバとをインタフェースするためのクリーンな方法を提供します。しかし、サーバ自体の動作を変更したい場合や、サーバから得られるパフォーマンスの最後の一滴まで引き出したい場合はどうしたらよいでしょうか？この2つのニーズに対して、サーバ開発者はサーバ拡張APIを提供しています。これは、ウェブ開発者が自分のモジュールとHTTPサーバを直接インタフェースするための強力なインタフェースです。拡張APIは、プログラマが独自のコードをサーバに移植したり、サーバのコンポーネントを完全に交換して独自のものに置き換えたりすることを可能にします。一般的なサーバは、開発者向けに1つ以上の拡張APIを提供しています。
- これらの拡張機能は、サーバー自体のアーキテクチャに関連していることが多いため、ほとんどの場合、1つのサーバータイプに固有のものとなっています。Microsoft、Netscape、Apacheなどのサーバーは、開発者がサーバーの動作を変更したり、さまざまなリソースへのカスタムインターフェースを提供するためのAPIインターフェースを備えています。これらのカスタムインターフェースは、開発者にとって強力なインターフェースとなります。
- サーバ拡張機能の例としては、Microsoft の FrontPage Server Extension (FPSE) があります。これは FrontPage 作者のための Web 出版サービスをサポートするものです。FPSE は、FrontPage クライアントが送信するリモート・プロシージャ・コール(RPC)コマンドを解釈することができます。これらのコマンドは HTTP におんぶに抱っこです(具体的には、HTTP POST メソッドに重ねられています)。詳細については、セクション19.1を参照してください。

## 8.4 Application Interfaces and Web Services よく分からない
- これまで、ウェブサーバとアプリケーションの通信手段として、リソースゲートウェイについて説明してきました。より一般的には、ウェブアプリケーションが提供するサービスの種類が増えていることから、HTTPがアプリケーションをつなぐための基盤の一部となることが明らかになっています。アプリケーションの連携で難しいのは、2つのアプリケーションがデータを交換できるようにするために、2つのアプリケーション間のプロトコルインターフェースをネゴシエートすることですが、多くの場合、これはアプリケーションごとに行われます。
- アプリケーション同士が連携するためには、通常、HTTPヘッダーで表現できる以上の複雑な情報を交換する必要があります。第19章では、カスタマイズされた情報を交換するために、HTTPを拡張したり、HTTPの上にプロトコルを重ねたりするいくつかの例を紹介します。19.1 節では HTTP POST メッセージに RPC を重ねることについて、19.2 節では HTTP ヘッダに XML を追加することについて説明します。
- インターネットコミュニティでは、ウェブアプリケーションが相互に通信できるように、一連の標準およびプロトコルを開発しています。これらの標準は、漠然とWebサービスと呼ばれていますが、この用語は、スタンドアロンのWebアプリケーション（ビルディングブロック）そのものを意味することもあります。Webサービスの前提は新しいものではありませんが、アプリケーションが情報を共有するための新たな仕組みとなっています。Webサービスは、HTTPなどの標準的なWeb技術をベースに構築されています。
- Webサービスは、SOAPによるXMLを使って情報を交換する。XML（Extensible Markup Language）は、データオブジェクトに関するカスタマイズされた情報を作成・解釈する方法を提供する。SOAP（Simple Object Access Protocol）は、HTTPメッセージにXML情報を追加するための標準規格である[3]。
  - [3] 詳細は http://www.w3.org/TR/2001/WD-soap12-part0-20011217/ をご覧ください。 また、Doug Tidwell、James Snell、Pavel Kulchenko著の「Programming Web Services with SOAP」（O'Reilly）も、SOAPプロトコルに関する優れた情報源です。

## 8.5 Tunnels
- これまでに、さまざまな種類のリソースへのアクセスを可能にしたり、アプリケーション間の通信を可能にしたりするために、HTTP を使用するさまざまな方法について説明してきました。このセクションでは、HTTP のもう一つの用途である Web トンネルについて説明します。Web トンネルは、HTTP アプリケーションを介して非 HTTP プロトコルを使用するアプリケーションへのアクセスを可能にします。
- Webトンネルは、HTTP接続を通じて非HTTPトラフィックを送信し、他のプロトコルが HTTP の上にピギーバックすることを可能にします。Web トンネルを使用する最も一般的な理由は、HTTP 接続の中に非 HTTP トラフィックを埋め込み、Web トラフィックのみを許可するファイアウォールを介して送信できるようにすることです。

### 8.5.1 Establishing HTTP Tunnels with CONNECT
- Webトンネルは、HTTPのCONNECTメソッドを用いて確立される。CONNECTプロトコルは、HTTP/1.1のコア仕様には含まれていないが[4]、広く実装されている拡張機能である。技術的な仕様は、Ari Luotonen氏の失効したインターネットドラフト仕様「Tunneling TCP based protocols through Web proxy servers」や、同氏の著書「Web Proxy Servers」に記載されており、いずれも本章の最後に引用されている。
  - [4] HTTP/1.1の仕様では、CONNECTメソッドが予約されているが、その機能については記述されていない。
- CONNECTメソッドは、トンネルゲートウェイに対して、任意の宛先サーバーとポートへのTCP接続の作成と、クライアントとサーバーの間で後続のデータを盲目的に中継することを求めるものである。
- 図8-10は、ゲートウェイへのトンネルを確立するためのCONNECTメソッドの動作を示す。
  - 図8-10aでは、クライアントがトンネル・ゲートウェイにCONNECTリクエストを送信している。クライアントの CONNECT メソッドは、トンネル・ゲートウェイに TCP 接続を開くように要求します（ここでは、 orders.joes-hardware.com という名前のホストにポート 443（通常の SSL ポート）で接続します）。
  - 図8-10bと図8-10cでは、TCP接続が作成されています。
  - TCP 接続が確立されると、ゲートウェイは HTTP 200 Connection Established レスポンスを送信してクライアントに通知します（図 8-10d）。
  - この時点で、トンネルの設定が完了します。クライアントからHTTPトンネル経由で送信されたデータは、直接、送信用のTCPコネクションに中継され、サーバーから送信されたデータは、HTTPトンネル経由でクライアントに中継されます。
- 図 8-10 の例では、SSL トラフィックが HTTP 接続を介して送信される SSL トンネルについて説明していますが、CONNECT メソッドを使用して、任意のプロトコルを使用する任意のサーバーへの TCP 接続を確立することができます。

#### 8.5.1.1 CONNECT requests
- CONNECTの構文は、開始行を除いて他のHTTPメソッドと同じ形式です。リクエストURIは、ホスト名、コロン、ポート番号の順に置き換えられます。ホストとポートの両方を指定する必要があります。

```
CONNECT home.netscape.com:443 HTTP/1.0
User-agent: Mozilla/4.0
```

- 開始行の後には、他のHTTPメッセージと同様に、0個以上のHTTPリクエストヘッダフィールドがあります。いつものように、行末はCRLFで終わり、ヘッダのリストはむき出しのCRLFで終わります。

#### 8.5.1.2 CONNECT responses
- リクエストが送信された後，クライアントはゲートウェイからのレスポンスを待ちます。通常の HTTP メッセージと同様に，200 応答コードは成功を示す．慣習的に、レスポンスの reason フレーズは通常「Connection Established」に設定されます。

```
HTTP/1.0 200 Connection Established
Proxy-agent: Netscape-Proxy/1.1
```

- 通常のHTTPレスポンスとは異なり、レスポンスにはContent-Typeヘッダーを含める必要はありません。コネクションがメッセージキャリアではなく、生のバイトリレーとなるため、コンテンツタイプは必要ありません[5]。
  - [5] 将来の仕様では、統一性を持たせるために、トンネル用のメディアタイプ（例：application/tunnel）を定義することができる。

### 8.5.2 Data Tunneling, Timing, and Connection Management
- トンネル化されたデータはゲートウェイにとって不透明であるため、ゲートウェイはパケットの順序や流れを推測することができない。トンネルが確立されると、データはいつでもどの方向にでも自由に流れます[6]。
  - [6] トンネルの2つのエンドポイント（クライアントとゲートウェイ）は、いつでもどちらかの接続からパケットを受け入れる準備をし、そのデータを直ちに転送しなければなりません。トンネル化されたプロトコルにはデータ依存性が含まれている可能性があるため、トンネルのどちらの端も入力データを無視することはできません。トンネルの一方でデータが消費されないと、トンネルのもう一方の端にあるプロデューサーがハングアップし、デッドロックにつながる可能性があります。
- パフォーマンスの最適化として、クライアントはCONNECTリクエストを送信した後、レスポンスを受信する前にトンネルデータを送信することができます。これにより、サーバへのデータ送信は高速化されるが、ゲートウェイはリクエストに続くデータを適切に処理できなければならないことになる。特に、ゲートウェイは、ネットワークI/Oリクエストがヘッダーデータだけを返すと仮定することはできず、ゲートウェイは、接続の準備ができたときに、ヘッダーで読み取ったデータを必ずサーバーに転送しなければならない。リクエストの後にデータをパイプラインで送るクライアントは、レスポンスが認証チャレンジやその他の非200、非致命的なステータスとして戻ってきた場合、リクエストデータを再送する準備をしなければならない。[7]
  - [7] リクエストのTCPパケットの残りの部分に収まる以上のデータをパイプライン化しないようにしてください。パイプライン化されたすべてのTCPパケットを受信する前にゲートウェイが接続を閉じた場合、より多くのデータをパイプライン化すると、クライアントのTCPリセットが発生する可能性があります。TCPリセットが起こると、クライアントは受信したゲートウェイの応答を失うことになり、失敗の原因がネットワークエラーなのか、アクセスコントロールなのか、認証チャレンジなのかがわからなくなります。
- トンネルのエンドポイントのいずれかが切断された場合、そのエンドポイントから来た未処理のデータは、もう一方のエンドポイントに渡され、その後、もう一方の接続もプロキシによって終了されます。終了するエンドポイントに未配送のデータがある場合、そのデータは破棄されます。

### 8.5.3 SSL Tunneling
- Webトンネルは、暗号化されたSSLトラフィックをファイアウォールを経由して伝送するために開発されました。多くの企業では、セキュリティ強化のために、すべてのトラフィックをパケットフィルタリングルータやプロキシサーバに通しています。しかし、暗号化されたSSLのような一部のプロトコルは、情報が暗号化されているため、従来のプロキシサーバではプロキシできません。トンネルでは、SSL トラフィックを HTTP 接続で伝送することにより、ポート 80 の HTTP ファイアウォールを通過させることができます（図 8-11）。
- SSL トラフィックが既存のプロキシ・ファイアウォールを通過できるようにするために、HTTP にトンネリング機能が追加されました。これは、生の暗号化されたデータを HTTP メッセージ内に配置し、通常の HTTP チャンネルで送信するものです（図 8-12）。
- 図 8-12a では、SSL トラフィックは安全な Web サーバー（SSL ポート 443）に直接送信されます。図 8-12b では、SSL トラフィックは HTTP メッセージにカプセル化され、HTTPポート 80 接続で送信されますが、その後通常の SSL 接続にカプセル化解除されます。
- トンネルは、HTTP 以外のトラフィックがポートフィルタリングを行うファイアウォールを通過するためによく使用されます。この機能は、例えば、安全な SSL トラフィックがファイアウォールを通過できるようにするなど、有効に活用することができます。しかし、この機能が悪用されると、悪意のあるプロトコルが HTTP トンネルを介して組織内に流入する可能性があります。

### 8.5.4 SSL Tunneling Versus HTTP/HTTPS Gateways
- HTTPS プロトコル（HTTP over SSL）は、他のプロトコルと同じ方法でゲートウェイすることもできます。（クライアントの代わりに）ゲートウェイがリモート HTTPS サーバとの SSL セッションを開始し、クライアント側で HTTPS トランザクションを実行します。レスポンスはプロキシが受信して復号化し、（安全でない）HTTPでクライアントに送信される。これは、ゲートウェイが FTP を処理する方法である。しかし、この方法にはいくつかのデメリットがある。

  - クライアントからゲートウェイへの接続は、通常の安全でないHTTPです。
  - クライアントは、リモートサーバに対してSSLクライアント認証（X509
  証明書に基づく認証）を行うことができません。
  - ゲートウェイは完全なSSL実装をサポートする必要があります。

- なお、このメカニズムをSSLトンネリングに使用する場合、プロキシでのSSLの実装は必要ありません。SSLセッションは、リクエストを生成したクライアントと宛先の（安全な）ウェブサーバの間で確立されます。その間にあるプロキシサーバは、単に暗号化されたデータをトンネリングするだけで、安全なトランザクションには関与しません。

### 8.5.5 Tunnel Authentication
- HTTPの他の機能は、必要に応じてトンネルと共に使用することができます。特に、プロキシ認証のサポートは、クライアントがトンネルを使用する権利を認証するためにトンネルと一緒に使用することができます（図8-13）。

### 8.5.6 Tunnel Security Considerations
- 一般的に、トンネルゲートウェイは、話されているプロトコルが本当にトンネルすることになっているかどうかを確認することができません。例えば、悪意のあるユーザが、SSL 用のトンネルを使用して、企業のファイアウォールを通過するインターネット・ゲー ム・トラフィックをトンネリングしたり、悪意のあるユーザが、トンネルを使用して Telnet セッションを開いたり、企業の電子メール・スキャナーを迂回して電子メールを送信したりすることがあります。
- トンネルの悪用を最小限に抑えるために、ゲートウェイは、HTTPS用の 443 など、特定のウェルノウンポートに対してのみトンネルを開くべきである。

## 8.6 Relays
- HTTPリレーは、HTTPの仕様に完全に準拠していないシンプルなHTTPプロキシです。リレーは、接続を確立するのに十分な HTTP を処理した後、バイトを盲目的に転送します。
- HTTP は複雑なので、ヘッダやメソッドのロジックをすべて実行せずに、トラフィックをブラインドで転送するだけの素朴なプロキシを実装すると便利な場合があります。ブラインドリレーは実装が簡単なので、単純なフィルタリング、診断、コンテンツ変換などに使用されることがあります。しかし、相互運用性に問題が発生する可能性があるため、導入には十分な注意が必要です。
- 単純なブラインドリレーの実装でよく見られる（そして有名な）問題のひとつに、Connectionヘッダーを適切に処理しないために、キープアライブ接続がハングアップするというものがあります。この状況を図8-14に示します。
- 省略
- このようなリスクを取り除くために、リレーを少し賢くする方法がありますが、プロキシを単純化すると、相互運用の問題が発生するリスクがあります。特定の目的のためにシンプルなHTTPリレーを構築する場合、その使い方には注意が必要です。大規模に展開する場合は、代わりに本物のHTTP準拠のプロキシサーバーを使用することを強く検討する必要があります。
- リレーと接続管理の詳細については、セクション4.5.6を参照してください。

# Chapter 9. Web Robots
- ここでは、Webロボットと呼ばれる自力で動くユーザーエージェントに焦点を当てて、HTTPアーキテクチャのツアーを続けます。
- ウェブロボットとは、人間の手を借りずに一連のウェブ取引を自動化するソフトウェアプログラムのことである。多くのロボットは、WebサイトからWebサイトへと移動し、コンテンツを取得し、ハイパーリンクをたどり、見つけたデータを処理する。この種のロボットには、「クローラー」「スパイダー」「ワーム」「ボット」などのカラフルな名前が付けられていますが、これは、ロボットが自動的にWebサイトを探索し、まるで自分の意思を持っているかのように見えるためです。
ここでは、Webロボットの例をいくつか紹介します。

  - 株価グラフ作成ロボットは、数分ごとに株式市場のサーバーにHTTP GETを発行し、そのデータを使って株価のトレンドグラフを作成する。
  - ウェブセンサス・ロボットは、WWWの規模と進化に関する「センサス（census）」情報を収集する。ウェブを歩き回ってページ数を数え、各ページのサイズ、言語、メディアタイプを記録している[1]。
    - [1] http://www.netcraft.com は，ウェブ上のサイトでどのような種類のサーバが使用されているかという，優れたセンサスメトリクスを収集しています．
  - 検索エンジンのロボットは、見つけた文書をすべて収集して検索データベースを作成します。
  - 比較ショッピングロボットは、オンラインストアのカタログからウェブページを収集し、製品とその価格のデータベースを構築する。

## 9.1 Crawlers and Crawling

## 9.2 Robotic HTTP

## 9.3 Misbehaving Robots

## 9.4 Excluding Robots

## 9.5 Robot Etiquette

## 9.6 Search Engines

# Chapter 10. HTTP-NG
- 本書が完成する頃、HTTPは10回目の誕生日を迎えています。このインターネットプロトコルにとって、この10年間は非常に充実したものでした。現在、HTTPは世界中のデジタルトラフィックの絶対的な大部分を動かしています。
- しかし、HTTPが10代になるにつれ、いくつかの課題に直面しています。ある意味では、HTTPの採用ペースがその設計よりも先に進んでしまったのです。今日、人々は、さまざまなネットワーク技術上で、多くの多様なアプリケーションの基盤としてHTTPを使用しています。
- 本章では、HTTP の将来に関する動向と課題、および HTTP-NG と呼ばれる次世代アーキテクチャの提案について説明します。HTTP-NG のワーキンググループは解散し、その迅速な採用は難しいと思われますが、それでも、HTTP の将来の方向性の可能性を示しています。

# Part III: Identification, Authorization, and Security
- 第3部の4つの章では、アイデンティティを追跡し、セキュリティを強化し、コンテンツへのアクセスを制御するための一連の技術とテクノロジーが紹介されています。
  - 第11章では、コンテンツをユーザーに合わせてパーソナライズできるように、ユーザーを識別する技術について説明します。
  - 第12章では、ユーザのアイデンティティを確認するための基本的なメカニズムを紹介します。また、この章では、HTTP認証とデータベースとのインターフェースについても説明します。
  - 第13章では、ダイジェスト認証について説明しています。ダイジェスト認証は、セキュリティを大幅に強化するために提案されたHTTPの複雑な拡張機能です。
  - 第14章では、インターネット暗号、デジタル証明書、SSL（Secure Sockets Layer）について詳しく説明します。

# Chapter 11. Client Identification and Cookies
- ウェブサーバは、何千もの異なるクライアントと同時に会話することがあります。これらのサーバは、すべてのリクエストを匿名のクライアントからのものとして扱うのではなく、誰と話しているのかを追跡する必要がある場合があります。この章では、サーバーが誰と会話しているかを特定するために使用できる技術について説明します。

## 11.1 The Personal Touch
- HTTPは、匿名かつステートレスなリクエスト/レスポンスプロトコルとして誕生しました。クライアントから送られてきたリクエストはサーバーで処理され、レスポンスがクライアントに返されます。どのユーザーがリクエストを送信したのか、あるいは訪問したユーザーからの一連のリクエストを追跡するための情報は、ウェブサーバーにはほとんどありませんでした。
- 最近のWebサイトは、personal touchを提供したいと考えています。接続先のユーザーのことをもっと知りたいし、閲覧中のユーザーを追跡できるようにしたいのです。Amazon.comのような人気のあるオンラインショッピングサイトでは、いくつかの方法でユーザーのためにサイトをパーソナライズしています。

- Personal greetings
  - ウェルカムメッセージやページの内容をユーザーのために特別に作成し、よりパーソナルなショッピング体験を提供しています。
- Targeted recommendations
  - お客様の関心事を知ることで、お客様が喜ぶであろう商品を提案することができます。また、お客さまの誕生日などに合わせて、バースデー・スペシャルを実施することも可能です。
- Administrative information on file
  - オンラインで買い物をする人は、住所やクレジットカードなどの面倒な情報を何度も入力することを嫌います。サイトによっては、これらの管理情報をデータベースに保存しています。これにより、お客様の身元を確認した上で、登録されている管理情報を利用することができ、より便利にショッピングを楽しむことができます。
- Session tracking
  - HTTPトランザクションはステートレスです。それぞれのリクエスト/レスポンスは独立して行われます。多くのWebサイトでは、お客様がサイトを操作する際に、徐々に状態を構築していきたいと考えています（例えば、オンラインショッピングカートを満たす場合など）。そのためには、異なるユーザーによるHTTPトランザクションを区別する方法が必要です。

- この章では、HTTPでユーザーを識別するために使われているいくつかの技術をまとめています。HTTP自体は、豊富な識別機能を備えて生まれたわけではありません。初期のWebサイト設計者(彼らは実用的な人々でした)は、ユーザーを識別するために独自の技術を構築しました。それぞれの技術には長所と短所があります。
この章では、ユーザーを識別するための次のような仕組みについて説明します。

  - ユーザーのアイデンティティに関する情報を伝えるHTTPヘッダ
  - クライアントのIPアドレスを追跡することで、IPアドレスによってユーザを識別する。
  - 認証を利用してユーザーを識別するユーザーログイン
  - Fat URL：URL にアイデンティティを埋め込む技術
  - Cookies：永続的な ID を維持するための強力かつ効率的な技術である。

## 11.2 HTTP Headers
- 表11-1は、ユーザーに関する情報を最も一般的に伝える7つのHTTPリクエストヘッダーを示しています。ここでは、最初の3つのヘッダについて説明します。後の4つのヘッダは、後述するより高度な識別技術に使用されます。
- Fromヘッダーには、ユーザーの電子メールアドレスが含まれています。本来であれば、ユーザーごとに異なる電子メールアドレスを持つことで、ユーザーを識別する有効な手段となります。しかし、悪質なサーバーが電子メールアドレスを収集し、ジャンクメールの配信に使用することが懸念されるため、Fromヘッダーを送信するブラウザはほとんどありません。実際には、Fromヘッダーは自動化されたロボットやスパイダーによって送信され、何か問題が発生した場合にウェブマスターが怒りのメールを送る場所を確保できるようになっています。
- User-Agentヘッダーは、ユーザーが使用しているブラウザに関する情報をサーバーに伝えるもので、プログラムの名前やバージョン、しばしばオペレーティングシステムに関する情報も含まれます。この情報は、特定のブラウザやその属性に合わせてコンテンツをカスタマイズするのに役立つこともありますが、特定のユーザーを意味のある形で識別するのにはあまり役立ちません。
- Referer ヘッダーは、ユーザーがアクセスしてきたページの URL を提供します。Refererヘッダーだけでは、ユーザーを直接識別することはできませんが、ユーザーが以前にどのページを訪れたかを知ることができます。これを利用して、ユーザーの閲覧行動やユーザーの興味をより深く理解することができます。例えば、ユーザーが野球のサイトからウェブサーバーにアクセスした場合、サーバーはユーザーが野球ファンであることを推測することができます。
- From、User-Agent、Refererの各ヘッダは、確実な識別を目的とするには不十分です。残りのセクションでは、特定のユーザーを識別するための、より正確なスキームについて説明します。

## 11.3 Client IP Address
- 初期のウェブ開発者たちは、クライアントのIPアドレスを識別の手段として利用しようとしました。この方式が有効なのは、各ユーザーが個別のIPアドレスを持ち、IPアドレスがめったに（あったとしても）変更されず、ウェブサーバーがリクエストごとにクライアントのIPアドレスを特定できる場合です。クライアントのIPアドレスは通常、HTTPヘッダーには含まれていませんが[1]、ウェブサーバーはHTTPリクエストを伝送するTCP接続の相手側のIPアドレスを見つけることができます。
- [1] 後述するように、プロキシの中にはClient-ipヘッダーを追加するものもありますが、これはHTTP標準には含まれていません。
- 例えば，Unix システムでは，getpeername 関数コールは，送信側マシンのクライアント IP アドレスを返します。

```
status = getpeername(tcp_connection_socket,...);
```

- 残念ながら、クライアントのIPアドレスを使ってユーザーを識別する方法には、ユーザー識別技術としての有効性を制限する多くの弱点があります。

  - クライアントIPアドレスは、ユーザーではなく、使用しているコンピュータのみを表します。複数のユーザーが同じコンピュータを共有している場合、それらは区別できません。
  - 多くのインターネットサービスプロバイダーは、ユーザーがログインしたときにIPアドレスを動的に割り当てます。ユーザーがログインするたびに異なるアドレスが割り当てられるため、ウェブサーバーは、IPアドレスがログインセッション間でユーザーを識別できると仮定することはできません。
  - セキュリティの強化と不足するアドレスの管理のため、多くのユーザーはNAT（Network Address Translation）ファイアウォールを経由してインターネットを利用しています。これらのNATデバイスは、ファイアウォールの背後にある実際のクライアントのIPアドレスを不明瞭にし、実際のクライアントのIPアドレスを単一の共有ファイアウォールIPアドレス（および異なるポート番号）に変換します。
  - HTTPプロキシやゲートウェイは、通常、オリジンサーバーへの新しいTCP接続を開きます。Webサーバーには、クライアントのIPアドレスではなく、プロキシサーバーのIPアドレスが表示されます。一部のプロキシは、特別な Client-ip または X-Forwarded-For HTTP 拡張ヘッダを追加して元の IP アドレスを保持することで、この問題を回避しようとします (図 11-1)。しかし、すべてのプロキシがこの動作をサポートしているわけではありません。

- 一部のWebサイトでは、セッション間でユーザーを追跡するためにクライアントのIPアドレスを使用していますが、多くはありません。IPアドレスのターゲティングがうまく機能しない場所が多すぎるのです。
- いくつかのサイトでは、クライアントIPアドレスをセキュリティ機能として使用し、特定のIPアドレスからのユーザーにのみドキュメントを提供しています。これは、イントラネット内では適切かもしれませんが、インターネット上では、IPアドレスのスプーフィング（偽造）が容易であることが主な理由です。また、経路上に傍受用プロキシが存在する場合も、この方式は破綻します。第14章では、特権文書へのアクセスを制御する、より強力なスキームについて説明します。

## 11.4 User Login
- Webサーバは、IPアドレスからユーザの身元を推測するのではなく、ユーザ名とパスワードによる認証（ログイン）を要求することで、ユーザに明示的に身元を尋ねることができます。
- Webサイトへのログインを容易にするために、HTTPにはWWW-AuthenticateおよびAuthorizationヘッダを用いて、Webサイトにユーザ名情報を渡す仕組みが組み込まれています。一度ログインすると、ブラウザはサイトへのリクエストごとにこのログイン情報を継続的に送信するので、情報は常に有効です。このHTTP認証については、第12章でもっと詳しく説明しますが、今は簡単に見てみましょう。
- サーバーが、サイトへのアクセスを提供する前にユーザー登録をしてほしい場合、HTTP 401 Login Requiredのレスポンスコードをブラウザに送り返すことができます。ブラウザはログインダイアログボックスを表示し、次のブラウザへのリクエストでAuthorizationヘッダを使用して情報を供給します[2]。 これを図11-2に示します。
  - [2] ユーザーがリクエストのたびにログインする手間を省くために、ほとんどのブラウザはサイトのログイン情報を記憶し、サイトへのリクエストのたびにログイン情報を渡します。
- この図では何が起こっているのかを説明します。
  - 図11-2aでは、ブラウザが www.joes-hardware.com サイトからリクエストを行う。
  - サイトはユーザのアイデンティティを知らないので、図11-2bでは、サーバは401 Login Required HTTP応答コードを返してログインを要求し、WWW-Authenticateヘッダを追加する。これにより、ブラウザはログイン・ダイアログ・ボックスをポップアップする。
  - ユーザがユーザ名とパスワードを入力すると（ユーザの身元を確認するため）、ブラウザは元のリクエストを繰り返します。今回は、ユーザー名とパスワードを指定するAuthorizationヘッダーを追加します。ユーザー名とパスワードはスクランブルされており、ネットワーク上の偶然の訪問者からは見えないようになっています[3]。
    - [3] 第14章で説明するように、HTTP基本認証のユーザー名とパスワードは、最小限の手間をかければ、誰でも簡単にスクランブルを解除することができます。より安全な技術については後述します。
  - これで、サーバーはユーザーのアイデンティティを知ることができました。
  - それ以降のリクエストでは、ブラウザは求められれば自動的に保存されているユーザー名とパスワードを発行し、求められていない場合でもサイトに送信することもしばしばあります。これにより、ブラウザがサーバーへの各リクエストで自分のアイデンティティのトークンとしてAuthorizationヘッダーを送信することで、サイトに一度だけログインし、セッションを通じて自分のアイデンティティを維持することが可能になります。

- しかし、Webサイトへのログインは面倒なものです。フレッドさんがあちこちのサイトを閲覧すると、サイトごとにログインする必要があります。さらに悪いことに、かわいそうなフレッドは、サイトごとに異なるユーザー名とパスワードを覚えておかなければならない可能性が高い。彼のお気に入りのユーザー名「fred」は、彼が多くのサイトを訪れる頃にはすでに誰かに選ばれてしまっているだろうし、サイトによってはユーザー名やパスワードの長さや構成について異なる規則があるだろう。すぐにフレッドはインターネットに見切りをつけて、オプラの視聴に戻ってしまうだろう。次のセクションでは、この問題を解決する方法について説明します。

## 11.5 Fat URLs
- Webサイトの中には、ユーザーごとに特別なバージョンのURLを生成することで、ユーザーのアイデンティティを把握しているものがあります。通常、実際のURLは、URLパスの最初または最後に何らかの状態情報を追加することで拡張されます。ユーザーがサイトを閲覧すると、ウェブサーバは、URLの状態情報を維持し続けるハイパーリンクを動的に生成します。
- ユーザーの状態情報を含むように変更されたURLは、ファットURLと呼ばれます。以下は、Amazon.comのeコマースサイトで使用されているファットURLの例です。各URLの末尾には、ユーザー固有の識別番号（この例では002-1145265-8016838）が付けられており、ユーザーがストアを閲覧した際に追跡できるようになっています。
- ファットURLを使用すると、ウェブサーバーとの独立したHTTPトランザクションを1つの「セッション」または「訪問」に結びつけることができます。ユーザーが初めてWebサイトにアクセスしたときには、一意のIDが生成され、それがサーバーに認識される方法でURLに追加され、サーバーはクライアントをこのファットURLにリダイレクトします。サーバーは、ファットURLへのリクエストを受け取るたびに、そのユーザーIDに関連するあらゆる増分状態（ショッピングカート、プロファイルなど）を調べることができ、ユーザーIDを維持するために、発信するハイパーリンクをすべてファットに書き換えます。
- ファットURLは、サイトを閲覧するユーザーを識別するために使用することができます。しかし、この技術にはいくつかの重大な問題があります。その問題点には次のようなものがあります。

  - Ugly URLs
    - ブラウザに表示されるfat URLは、初めて使う人には分かりにくいものです。
  - URLの共有ができない
    - ファットURLには、特定のユーザーやセッションに関する状態情報が含まれています。そのURLを誰かにメールすると、蓄積された個人情報を誤って共有してしまう可能性があります。
  - キャッシングが壊れる
    - 各URLのユーザー固有のバージョンを生成するということは、キャッシュするためによくアクセスされるURLがなくなってしまうということです。
  - 余分なサーバー負荷
    - URLをファットにするためには、サーバー側でHTMLページを書き換える必要があります。
  - Escape hatches
    - 別のサイトにジャンプしたり、特定のURLをリクエストしたりして、ファットURLセッションから誤って「脱出」してしまうことがあります。ファットURLは、ユーザーが事前に修正されたリンクを厳密にたどった場合にのみ機能します。ユーザーが逃げてしまうと、進行状況(ショッピングカートの中身など)が失われ、最初からやり直さなければならないことがあります。
  - セッションを超えて永続的ではない
    - ユーザーがログアウトすると、特定のファットURLをブックマークしない限り、すべての情報が失われます。

## 11.6 Cookies
- クッキーは、ユーザーを識別し、永続的なセッションを可能にする現在の最良の方法です。これまでの技術が抱えていた多くの問題はありませんが、これらの技術と組み合わせて使用することで、さらに価値を高めることができます。クッキーはNetscapeによって開発されましたが、現在ではすべての主要ブラウザでサポートされています。
- クッキーは重要であり、新しいHTTPヘッダーを定義するものでもあるので、これまでのテクニックよりも詳しく説明します。クッキーの存在はキャッシングにも影響を与え、ほとんどのキャッシュやブラウザは、クッキーを使用したコンテンツのキャッシングを禁止しています。次のセクションでは、より詳細な情報を紹介します。

### 11.6.1 Types of Cookies
- クッキーは、セッション・クッキーとパーシステント・クッキーの2種類に大別できます。セッション・クッキーは、ユーザーがサイトを閲覧する際の設定や好みを記録する一時的なクッキーです。セッション・クッキーは、ユーザーがブラウザを終了すると削除されます。パーシステント・クッキーは、より長く保存することができます。ディスクに保存され、ブラウザの終了やコンピュータの再起動後も保存されます。パーシステント・クッキーは、ユーザーが定期的にアクセスするサイトの構成プロファイルやログイン名を保持するためによく使われます。
- セッション・クッキーとパーシステント・クッキーの唯一の違いは、期限切れの時期です。後述するように、Discardパラメータが設定されているか、有効期限の延長を示すExpiresやMaxAgeパラメータがない場合、クッキーはセッションクッキーです。

### 11.6.2 How Cookies Work
- クッキーとは、サーバーがユーザーに貼り付けた「Hello, My Name Is」というシールのようなものです。ユーザーがWebサイトを訪問すると、Webサイトは、そのサーバーがユーザーに貼り付けたシールをすべて読むことができます。
- ユーザーが初めてWebサイトを訪れたとき、Webサーバーはそのユーザーについて何も知りません（図11-3a）。ウェブサーバは、この同じユーザが再び戻ってくることを期待しているので、将来このユーザを識別できるように、一意のクッキーをユーザに「叩きつけ（slap）」たいと考えています。クッキーには、任意の名前と値の情報のリストが含まれており、Set-CookieまたはSet-Cookie2 HTTP応答（拡張）ヘッダを使用してユーザに添付されます。
- クッキーには任意の情報を含めることができますが、多くの場合、追跡目的でサーバーが生成した固有の識別番号だけが含まれています。例えば、図11-3bでは、サーバーはid="34294 "というクッキーをユーザーに叩きつけています。サーバは、この番号を使って、サーバが蓄積している訪問者のデータベース情報（購入履歴、住所情報など）を調べることができます。
- しかし、クッキーはID番号だけに限定されるものではありません。多くのウェブサーバーは、クッキーに直接情報を保持することを選択しています。例えば 

```
Cookie: name="Brian Totty"; phone="555-1212" 
```

- ブラウザは、Set-CookieやSet-Cookie2ヘッダでサーバから送り返されたクッキーの内容を記憶し、ブラウザのクッキーデータベース（様々な国のステッカーが貼られたスーツケースのようなものだと思ってください）に一連のクッキーを保存します。将来、ユーザーが同じサイトに戻ってきたとき（図11-3c）、ブラウザは、そのサーバーからユーザーに叩きつけられたクッキーを選択し、クッキー・リクエスト・ヘッダーで送り返すことになります。

### 11.6.3 Cookie Jar: Client-Side State
- クッキーの基本的な考え方は、ブラウザにサーバー固有の情報を蓄積させ、ユーザーがアクセスするたびにその情報をサーバーに提供するというものです。クッキーの情報を蓄積するのはブラウザであることから、この仕組みをクライアントサイドステートと呼びます。なお、クッキー仕様の正式名称は「HTTP State Management Mechanism」です。

#### 11.6.3.1 Netscape Navigator cookies
- ブラウザによって、クッキーの保存方法は異なります。Netscape Navigatorは、cookies.txtという1つのテキストファイルにクッキーを保存します。たとえば、以下のようになります。

#### 11.6.3.2 Microsoft Internet Explorer cookies
- Microsoft Internet Explorer は、Cookie を個々のテキストファイルとして cache ディレクトリに保存します。図11-4に示すように、このディレクトリを参照してクッキーを表示することができます。Internet Explorer の Cookie ファイルの形式は独自のものですが、フィールドの多くは簡単に理解できます。各クッキーは、ファイル内に次々と保存され、各クッキーは複数の行で構成されています。
- ファイル内の各クッキーの最初の行には、クッキーの変数名が書かれています。次の行には、変数の値が書かれています。3 行目はドメインとパスを含みます。残りの行は、日付やその他のフラグを含むと思われる、独自のデータです。

### 11.6.4 Different Cookies for Different Sites
- ブラウザは内部のクッキージャーに何百、何千ものクッキーを持つことができますが、ブラウザはすべてのクッキーをすべてのサイトに送るわけではありません。実際には、通常、各サイトには2つまたは3つのクッキーしか送りません。
その理由は以下の通りです。

  - クッキーのバイトをすべて移動させると、パフォーマンスが劇的に低下します。ブラウザは、実際のコンテンツ・バイトよりも多くのクッキー・バイトを移動させることになります。
  - これらのクッキーのほとんどは、サーバー固有の名前と値のペアを含んでいるので、ほとんどのサイトにとっては認識できないちんぷんかんぷんなものです。
  - すべてのクッキーをすべてのサイトに送信すると、信頼していないサイトが別のサイトだけに意図した情報を得ることになり、プライバシーの問題が生じる可能性があります。

- 一般的には、ブラウザは、サーバーが生成したクッキーだけをサーバーに送信します。joes-hardware.comによって生成されたクッキーは、joes-hardware.comに送信され、bob-books.comやmarysmovies.comには送信されません。
- 多くのWebサイトでは、第三者であるベンダーと契約して広告を管理しています。これらの広告は、ウェブサイトの一部であるかのように見せかけ、パーシステントクッキーを押します。ユーザーが同じ広告会社のサービスを受けている別のウェブサイトにアクセスすると、以前に設定されたパーシステント・クッキーがブラウザによって再び送信されます（ドメインが一致しているため）。
- マーケティング会社は、この技術をRefererヘッダーと組み合わせて使用することで、ユーザーのプロファイルやブラウジングの習慣に関する網羅的なデータセットを構築できる可能性があります。最近のブラウザでは、プライバシー設定を行ってサードパーティのクッキーを制限することができます。

#### 11.6.4.1 Cookie Domain attribute
- クッキーを生成するサーバは、Set-Cookie 応答ヘッダに Domain 属性を追加することで、どのサイトがそのクッ キーを見ることができるかを制御できます。たとえば、次の HTTP レスポンスヘッダーは、クッキー user="mary17" をドメイン .airtravelbargains.com のすべてのサイトに送信するようブラウザに指示しています。

```
Set-cookie: user="mary17"; domain="airtravelbargains.com"
```

- ユーザーが www.airtravelbargains.com、species.airtravelbargains.com、または .airtravelbargains.com で終わるサイトにアクセスすると、次のような Cookie ヘッダーが発行されます。

```
Cookie: user="mary17" 
```

#### 11.6.4.2 Cookie Path attribute
- クッキーの仕様では、クッキーをウェブサイトの一部と関連付けることもできます。これには Path 属性を使用します。Path 属性は、各クッキーが有効な URL パスプレフィックスを示します。
- 例えば、1つのウェブサーバを2つの組織で共有し、それぞれが別々のクッキーを持つような場合です。www.airtravelbargains.com のサイトは、ウェブサイトの一部を自動車のレンタルに充てているかもしれません。例えば、http://www.airtravelbargains.com/autos/-using は、ユーザの好みの車のサイズを追跡するために別のクッキーを使用しています。特別な自動車レンタル用のクッキーは次のように生成されるかもしれません。

```
Set-cookie: pref=compact; domain="airtravelbargains.com";
path=/autos/
```

- ユーザーが http://www.airtravelbargains.com/specials.html にアクセスすると、このクッキーだけを取得します。

```
Cookie: user="mary17" 
```

- しかし、彼女が http://www.airtravelbargains.com/autos/cheapo/index.html に行けば、この二つのクッキーを手に入れることができます。

```
Cookie: user="mary17"
Cookie: pref=compact
```

- つまり、クッキーは、サーバがクライアントに叩きつけ、クライアントが維持し、適切なサイトだけに送り返す、状態の断片なのです。クッキーの技術と規格について、もう少し詳しく見てみましょう。

### 11.6.5 Cookie Ingredients
- クッキーの仕様には、2つの異なるバージョンが使用されています。バージョン0のクッキー（「Netscapeクッキー」と呼ばれることもあります）と、バージョン1（「RFC2965」）のクッキーです。バージョン1のクッキーは、バージョン0のクッキーを拡張したもので、あまり広く使われていません。
- バージョン0とバージョン1のどちらのクッキー仕様も、HTTP/1.1仕様の一部としては文書化されていません。クッキーの使用を最もよく説明する2つの主要な付属文書があり、表11-2にまとめられています。
  - Ver 0: http://home.netscape.com/newsref/std/cookie_spec.html
  - Ver 1: http://www.ietf.org/rfc/rfc2965.txt

### 11.6.6 Version 0 (Netscape) Cookies
- 最初のクッキーの仕様は、Netscapeによって定義されました。この「バージョン0」のクッキーは、Set-Cookie応答ヘッダ、Cookie要求ヘッダ、およびクッキーを制御するためのフィールドを定義していました。
- バージョン0のクッキーは次のようなものです。

```
Set-Cookie: name=value [; expires=date] [; path=path] [;domain=domain] [; secure]

Cookie: name1=value1 [; name2=value2] ... 
```

#### 11.6.6.1 Version 0 Set-Cookie header
- Set-Cookie ヘッダーは、必須の Cookie 名と Cookie 値を持ちます。このヘッダーには、セミコロンで区切られたオプションのクッキー属性を続けることができます。Set-Cookieのフィールドを表11-3に示します。

|  Set-Cookie attribute  |  Description and examples  |
| ---- | ---- |
|  NAME=VALUE  |  必須項目。NAMEとVALUEは共に、二重引用符で囲まれていない限り、セミコロン、コンマ、等号、空白を除いた文字の並びである。<br>ウェブサーバは、任意のNAME=VALUEの関連付けを行うことができ、次回以降のサイト訪問時にウェブサーバに送り返されることになる。<br>Set-Cookie: customer=Mary  |
|  Expires  |  オプションです。この属性は、そのクッキーの有効期限を定義する日付文字列を指定します。有効期限に達すると、そのクッキーは保存されなくなり、配布されなくなります。日付は以下のようにフォーマットされています。<br>Weekday, DD-Mon-YY HH:MM:SS GMT<br>合法なタイムゾーンはGMTのみで、日付の要素間のセパレータはダッシュでなければなりません。Expiresが指定されていない場合は、ユーザーのセッションが終了した時点でクッキーが失効します。<br>Set-Cookie: foo=bar; expires=Wednesday, 09-Nov-99
23:12:40 GMT  |
|  Domain  |  オプションです。ブラウザは、指定されたドメイン内のサーバーホスト名にのみクッキーを送信します。これにより、サーバーは特定のドメインのみにクッキーを制限することができます。ドメインが「acme.com」の場合、ホスト名「anvil.acme.com」と「shipping.crate.acme.com」にはマッチしますが、「www.cnn.com」にはマッチしません。指定されたドメイン内のホストのみがドメインにクッキーを設定することができ、ドメインは「.com」、「.edu」、「va.us」という形式のドメインを防ぐために、少なくとも2つまたは3つのピリオドを持たなければなりません。<br>ここに挙げた特別なトップレベルドメインの固定セットに該当するドメインは、ピリオドを2つだけ必要とします。それ以外のドメインは、少なくとも3つのピリオドが必要です。特別なトップレベルドメインとは .com、.edu、.net、.org、.gov、.mil、.int、.biz、.info、.name、.museum、.coop、.aero、.proです。<br>ドメインが指定されていない場合は、Set-Cookieレスポンスを生成したサーバーのホスト名がデフォルトとなります。<br>Set-Cookie: SHIPPING=FEDEX; domain="joeshardware.com"  |
|  Path  |  オプションです。この属性により、サーバー上の特定のドキュメントにクッキーを割り当てることができます。Path属性がURLパスの接頭辞であれば、クッキーを付けることができます。パス「/foo」は「/foobar」と「/foo/bar.html」にマッチします。パス「/」は、ドメイン内のすべてにマッチします。<br>パスが指定されていない場合は、SetCookie レスポンスを生成した URL のパスが設定されます。<br>Set-Cookie: lastorder=00183; path=/orders  |
|  Secure  |  オプションです。この属性が含まれている場合、HTTPがSSLセキュア接続を使用している場合にのみ、Cookieが送信されます。<br>Set-Cookie: private_id=519; secure  |

#### 11.6.6.2 Version 0 Cookie header
- クライアントがリクエストを送信する際には、サイトへのドメイン、パス、セキュアフィルターに一致するすべての期限が切れていないクッキーが含まれます。すべての Cookie は Cookie ヘッダーにまとめられます。

```
Cookie: session-id=002-1145265-8016838; session-idtime=1007884800
```

### 11.6.7 Version 1 (RFC 2965) Cookies
- クッキーの拡張版はRFC2965（旧RFC2109）で定義されている。このバージョン1の規格では、Set-Cookie2とCookie2のヘッダが導入されていますが、バージョン0のシステムとも相互運用が可能です。
- RFC2965のクッキー規格は、元々のNetscapeの規格よりも少し複雑で、まだ完全にはサポートされていません。RFC2965クッキーの主な変更点は以下の通りです。

  - それぞれのクッキーに目的を説明するテキストを関連付ける
  - 有効期限に関わらず、ブラウザ終了時のクッキーの強制破棄をサポート
  - 絶対的な日付ではなく、相対的な秒数でのCookieのMax-Ageエージング
  - ドメインやパスだけでなく、URLのポート番号でクッキーを制御する機能
  - Cookie ヘッダは、ドメイン、ポート、およびパスのフィルタがあれば、それを反映します。
  - 相互運用のためのバージョン番号
  - 追加のキーワードをユーザ名と区別するためのCookieヘッダの$プレフィックス

- バージョン1のクッキー構文は以下の通りです。

#### 11.6.7.1 Version 1 Set-Cookie2 header
- バージョン1のクッキー規格では、Netscape規格よりも多くの属性が利用できる。表11-4に属性の簡単な概要を示します。より詳しい説明はRFC2965を参照してください。

|  Set-Cookie2 attribute  |  Description and examples  |
| ---- | ---- |
|  NAME=VALUE  |  必須項目。ウェブサーバは任意のNAME=VALUE関連を作成することができ、次回以降のサイト訪問時にウェブサーバに送り返されます。"$"は予約済みの文字なので、名前は"$"で始まってはならない。  |
|  Version  |  必須項目。この属性の値は、クッキー仕様のバージョンに対応する整数です。RFC2965はバージョン1です。<br>Set-Cookie2: Part="Rocket_Launcher_0001"; Version="1"  |
|  Comment  |  オプション項目。この属性は、サーバーがどのようにクッキーを使用しようとしているかを文書化します。ユーザーはこのポリシーを確認して、このクッキーを使用したセッションを許可するかどうかを決定できます。この値は、UTF-8エンコーディングでなければなりません。  |
|  CommentURL  |  オプション項目。この属性は、クッキーの目的とポリシーに関する詳細な文書へのURLポインタを提供します。ユーザーはこのポリシーを確認して、このクッキーを使用したセッションを許可するかどうかを決定できます。  |
|  Discard  |  オプション項目。この属性が存在する場合、クライアントプログラムの終了時にクッキーを破棄するように指示します。  |
|  Domain  |  オプション項目。ブラウザは、指定されたドメイン内のサーバーホスト名にのみクッキーを送信します。これにより、サーバーは特定のドメインのみにクッキーを制限することができます。acme.com "というドメインは、"anvil.acme.com "と "shipping.crate.acme.com "というホストネームにはマッチしますが、"www.cnn.com "にはマッチしません。ドメインマッチングのルールは基本的にNetscapeのクッキーと同じですが、いくつかの追加ルールがあります。詳細はRFC2965を参照してください。  |
|  Max-Age  |  オプション項目。この属性の値は、クッキーの有効期限を秒単位で設定する整数です。クライアントは、HTTP/1.1の年齢計算のルールに従って、クッキーの年齢を計算する必要があります。クッキーの年齢がMax-Ageよりも大きくなった場合、クライアントはそのクッキーを破棄する必要があります。値がゼロの場合、その名前のクッキーは直ちに廃棄されるべきです。  |
|  Path  |  オプション項目。この属性により、サーバー上の特定のドキュメントにクッキーを割り当てることができます。Path属性がURLパスの接頭辞であれば、クッキーを付けることができます。パス「/foo」は、「/foobar」と「/foo/bar.html」にマッチします。パス「/」は、ドメイン内のすべてにマッチします。パスが指定されていない場合は、Set-Cookie レスポンスを生成した URL のパスが設定されます。  |
|  Port  |  オプション項目。この属性は、キーワードとして単独で使用することもできますし、クッキーを適用できるポートのコンマで区切ったリストを含めることもできます。ポートリストがある場合、クッキーは、ポートがリスト内のポートと一致するサーバにのみ提供できます。Port キーワードを単独で指定した場合、クッキーは現在応答しているサーバのポート番号にのみ適用されます。<br> Set-Cookie2: foo="bar"; Version="1"; Port="80,81,8080"<br>Set-Cookie2: foo="bar"; Version="1"; Port  |
|  Secure  |  オプション項目。この属性が含まれている場合、HTTPがSSLセキュア接続を使用している場合にのみ、Cookieが送信されます。  |

#### 11.6.7.2 Version 1 Cookie header
- バージョン 1 の Cookie は、配信された各 Cookie に関する追加情報を持ち帰り、各 Cookie が通過したフィルタを記述します。一致する各クッキーには、対応するSet-Cookie2ヘッダからのDomain、Port、Path属性が多く含まれます。
- 例えば、クライアントが過去に www.joes-hardware.com の Webサイトから以下の 5 つの Set-Cookie2 レスポンスを受け取ったとします。

```
Set-Cookie2: ID="29046"; Domain=".joes-hardware.com"
Set-Cookie2: color=blue
Set-Cookie2: support-pref="L2"; Domain="customer-care.joeshardware.com"
Set-Cookie2: Coupon="hammer027"; Version="1"; Path="/tools"
Set-Cookie2: Coupon="handvac103"; Version="1"; Path="/tools/cordless"
```

- クライアントがパス /tools/cordless/specials.html に対して別のリクエストを行うと、次のような長い Cookie2 ヘッダーが渡されます。

```
Cookie: $Version="1";
        ID="29046"; $Domain=".joes-hardware.com";
        color="blue";
        Coupon="hammer027"; $Path="/tools";
        Coupon="handvac103"; $Path="/tools/cordless" 
```

- 一致するすべてのCookieがSet-Cookie2フィルターとともに配信され、予約されたキーワードはドル記号（$）で始まっていることに注意してください。

#### 11.6.7.3 Version 1 Cookie2 header and version negotiation
- Cookie2 リクエスト・ヘッダは、クッキー仕様の異なるバージョンを理解するクライアントとサーバの間で、相互運用性を取り決めるために使用されます。Cookie2ヘッダは、ユーザエージェントが新スタイルのクッキーを理解していることをサーバに通知し、サポートしているクッキー規格のバージョンを提供します（Cookie-Versionと呼ぶ方が理にかなっているでしょう）。

```
Cookie2: $Version=“1"
```

- サーバが新スタイルのクッキーを理解していれば、クッキー2ヘッダを認識し、（Set-Cookieではなく）SetCookie2レスポンスヘッダを送信するはずです。クライアントが同じクッキーに対して Set-Cookie と SetCookie2 の両方のヘッダーを取得した場合、クライアントは古い Set-Cookie ヘッダーを無視します。
- クライアントがバージョン0とバージョン1の両方のクッキーをサポートしているが、サーバからバージョン0のSet-Cookieヘッダを受け取った場合、クライアントはバージョン0のCookieヘッダでクッキーを送るべきである。ただし、クライアントは Cookie2: $Version="1 "も送信して、サーバーにアップグレード可能であることを示す必要があります。

### 11.6.8 Cookies and Session Tracking
- クッキーは、ユーザーがウェブサイトで複数の取引を行う際に、その取引を追跡するために使用することができます。E-コマースのWebサイトでは、ユーザーがショッピングカートを利用した際の追跡にセッション・クッキーを使用しています。人気のショッピングサイトであるAmazon.comを例にとってみましょう。ユーザーがブラウザに「http://www.amazon.com」と入力すると、一連のリダイレクト、URLの書き換え、クッキーの設定によってウェブサーバーが識別情報を付加する一連のトランザクションが開始されます。
- 図11-5は、Amazon.comにアクセスしたときのトランザクション・シーケンスを示しています。

### 11.6.9 Cookies and Caching
- クッキーのトランザクションに関係するドキュメントをキャッシュする際には、注意が必要です。あるユーザーに過去のユーザーのクッキーを割り当てたり、最悪の場合、あるユーザーに他の人のパーソナライズされたドキュメントの内容を見せたりすることは避けなければなりません。
- クッキーとキャッシングのルールは、あまり確立されていません。ここでは、キャッシュを扱うための指針となる原則をいくつか紹介します。

- 以下の場合、ドキュメントをキャッシュ不能にします。
  - ドキュメントがキャッシュできないかどうかは、ドキュメントの所有者が一番よく知っています。具体的には、ドキュメントが Set-Cookie ヘッダを除いてキャッシュ可能な場合は Cache-Control: no-cache="Set-Cookie" を使用してください。より一般的な方法として、キャッシュ可能なドキュメントに Cache-Control: public を使用すると、Web の帯域幅を節約できます。
- Set-Cookieヘッダーのキャッシュには注意が必要です。
  - レスポンスに Set-Cookie ヘッダがある場合、（他に指示がない限り）ボディをキャッシュすることができますが、Set-Cookie ヘッダのキャッシュについては特に注意が必要です。同じ SetCookie ヘッダを複数のユーザに送信すると、ユーザターゲティングができなくなる可能性があります。
  - キャッシュの中には、応答をキャッシュに保存する前に Set-Cookie ヘッダを削除するものがありますが、これも問題を引き起こす可能性があります。なぜなら、キャッシュからサービスを受けるクライアントは、キャッシュがない場合に通常受けるはずのクッキーを受け取らなくなるからです。この状況は、キャッシュにオリジンサーバへのすべてのリクエストの再検証を行わせ、返されたSet-Cookieヘッダをクライアントのレスポンスにマージすることで改善できます。オリジンサーバは、キャッシュのコピーにこのヘッダを追加することで、このような再検証を指示することができます。
  - Cache-Control: must-revalidate, max-age=0
  - 保守的なキャッシュでは、コンテンツが実際にキャッシュ可能であっても、Set-Cookieヘッダを持つレスポンスのキャッシュを拒否することがあります。一部のキャッシュでは、Set-Cookie された画像がキャッシュされるモードを許可していますが、テキストは許可していません。
- Cookieヘッダーを持つリクエストには注意が必要
  - リクエストに Cookie ヘッダーが付いている場合、結果として得られるコンテンツがパーソナライズされている可能性があることを示唆しています。パーソナライズされたコンテンツにはキャッシュ不可のフラグを立てる必要がありますが、サーバーによっては誤ってこのコンテンツをキャッシュ不可にしない場合があります。
  - 保守的なキャッシュは、Cookieヘッダを持つリクエストに応答してくるドキュメントをキャッシュしないように選択することができます。また、一部のキャッシュでは、Cookied 画像がキャッシュされるモードを許可していますが、テキストは許可していません。より一般的なポリシーは、クッキーヘッダで画像をキャッシュし、有効期限をゼロに設定して、毎回再検証を行うことです。

### 11.6.10 Cookies, Security, and Privacy
- クッキー自体は無効にすることができ、また、追跡の多くはログ解析などで行うことができるため、セキュリティ上の大きなリスクにはならないと考えられています。実際、個人情報を遠隔地のデータベースに保持するための標準化された精査された方法を提供し、匿名のクッキーを鍵として使用することで、クライアントからサーバーへの機密データの通信頻度を減らすことができます。
- それでも、プライバシーやユーザーの追跡を扱う際には、常に悪用される可能性があるため、慎重になるのがよいでしょう。最大の悪用例は、第三者のウェブサイトが永続的なクッキーを使ってユーザーを追跡することです。この方法は、IPアドレスやRefererヘッダーの情報と組み合わせることで、マーケティング会社がかなり正確なユーザープロファイルや閲覧パターンを構築することを可能にしています。
- 否定的な意見もありますが、個人情報を提供する相手に注意を払い、サイトのプライバシーポリシーを確認すれば、クッキーによるセッション処理やトランザクションの利便性は、ほとんどのリスクを上回るというのが従来の考え方です。
- The Computer Incident Advisory Capability（米国エネルギー省の一部）は、1998年に、過剰に取り上げられたクッキーの危険性についての評価を書いています。以下はその報告書からの抜粋です。

# Chapter 12. Basic Authentication 
- 何百万人もの人々がウェブを利用して個人的な取引を行い、個人的なデータにアクセスしています。ウェブを使えば、これらの情報に簡単にアクセスできますが、簡単なだけでは十分ではありません。誰が私たちの機密データを見ることができるのか、誰が私たちの特権的な取引を行うことができるのかについて、保証が必要です。すべての情報が一般大衆向けではありません。
- 承認されていないユーザーが、私たちのオンライン旅行プロフィールを閲覧したり、私たちのウェブサイトに文書を公開したりすることができないという安心感が必要なのです。また、企業の最も重要な計画文書が、権限のない、潜在的に悪意のある組織のメンバーに利用されないようにする必要があります。また、子供や配偶者、秘密の恋人との個人的なWebコミュニケーションも、プライバシーが守られていると安心して行うことができます。
- サーバーには、ユーザーが誰であるかを知る方法が必要です。サーバーは、ユーザーが誰であるかを知ることができれば、そのユーザーがアクセスできるトランザクションやリソースを決定することができます。認証とは、自分が誰であるかを証明することです。通常、ユーザー名と秘密のパスワードを入力することで認証を行います。HTTPは、HTTP認証のためのネイティブな機能を提供しています。HTTP フォームや Cookie の上に「独自の」認証機能を実装することも可能ですが、多くの状況では HTTP のネイティブ認証がうまく機能しています。
- 本章では、HTTP認証について説明し、HTTP認証の最も一般的な形式であるベーシック認証について掘り下げます。次の章では、より強力な技術であるダイジェスト認証について説明します。

## 12.1 Authentication
- 認証とは、自分の身元を証明するものを示すことです。パスポートや運転免許証などの写真付き身分証明書を提示することは、自分が主張する人物であることを証明していることに他なりません。現金自動預け払い機に暗証番号を入力したり、コンピュータのダイアログボックスに秘密のパスワードを入力したりすることも、自分が主張する人物であることを証明していることになります。
- しかし、これらの方式はいずれも確実なものではありません。パスワードは推測されたり、盗み聞きされたり、IDカードは盗まれたり偽造されたりする可能性があります。しかし、それぞれの証拠は、あなたが自分自身であるという合理的な信頼を築くのに役立ちます。

### 12.1.1 HTTP's Challenge/Response Authentication Framework
- HTTPは、ネイティブなチャレンジ/レスポンスフレームワークを提供し、ユーザーの認証を容易にします。HTTPの認証モデルを図12-1に示します。
- WebアプリケーションがHTTPリクエストメッセージを受信した場合、サーバーはリクエストに対応する代わりに、「認証チャレンジ」で応答することができます。これは、いくつかの秘密の情報を提供することで、ユーザーが自分自身であることを示すようにチャレンジするものです。
- ユーザーは、リクエストを繰り返す際に、秘密の認証情報（ユーザー名とパスワード）を添付する必要があります。認証情報が一致しない場合、サーバーはクライアントに再度チャレンジするか、エラーを生成します。認証情報が一致した場合、リクエストは正常に完了します。

### 12.1.2 Authentication Protocols and Headers
- HTTP は、カスタマイズ可能な一連の制御ヘッダにより、さまざまな認証プロトコルに対応する拡張可能なフレームワークを提供します。表 12-1 に示すヘッダの形式と内容は、認証プロトコルによって異なります。また、認証プロトコルは、HTTP認証ヘッダで指定されます。
- HTTPでは，Basic認証とDigest認証の2つの公式認証プロトコルが定義されている。将来的には、HTTPのチャレンジ／レスポンスの枠組みを利用した新しいプロトコルを自由に考案することができます。この章の残りの部分では、Basic認証について説明します。ダイジェスト認証については、第13章を参照してください。

| Phase | Headers | Description | Method/Status |
| ---- | ---- | ---- | ---- |
| Request |  | 最初のリクエストには認証がありません。 | GET |
| Challenge | WWW-Authenticate | サーバーは、ユーザーが自分のユーザー名とパスワードを提供する必要があることを示す401ステータスでリクエストを拒否します。<br>サーバには，異なる領域があり，それぞれにパスワードが設定されている可能性があるため，サーバはWWW-Authenticateヘッダに保護領域を記述する。<br>また，認証アルゴリズムもWWW-Authenticateヘッダで指定する。 | 401 Unauthorized |
| Authorization | Authorization | クライアントはリクエストを再試行しますが、今度は認証アルゴリズム、ユーザー名、パスワードを指定したAuthorizationヘッダーを添付します。 | GET |
| Success | Authentication-Info | 認証証明が正しければ、サーバーはドキュメントを返します。認証アルゴリズムの中には、認証セッションに関する追加情報を、オプションの Authentication-Info ヘッダで返すものがあります。 | 200 OK |

- 具体的には、図12-2を見てみましょう。
- サーバがユーザにチャレンジするときは、401 Unauthorized応答を返し、WWW-Authenticateヘッダに認証方法と認証先を記述する（図12-2b）。
- クライアントがサーバーの処理を承認すると、リクエストを再送するが、エンコードされたパスワードと他の認証パラメータをAuthorizationヘッダーに添付する（図12-2c）。
- 認可されたリクエストが正常に完了すると、サーバーは通常のステータスコード（例：200 OK）を返し、高度な認証アルゴリズムの場合は、Authentication-Infoヘッダーに追加情報を添付することがある（図12-2d）。

### 12.1.3 Security Realms
- Basic認証の詳細を説明する前に、HTTPによってサーバが異なるリソースに異なるアクセス権を関連付ける方法を説明する必要があります。図12-2bのWWWAuthenticateチャレンジには、realmディレクティブが含まれていることにお気づきでしょうか。ウェブ・サーバは、保護されたドキュメントをセキュリティ・レルムにグループ化します。それぞれのセキュリティ・レルムには、異なる許可されたユーザのセットを持つことができる。
- 例えば、あるウェブ・サーバに2つのセキュリティ・レルムが設定されているとする。1つは企業の財務情報用、もう1つは個人の家族の文書用である（図12-3を参照）。それぞれの領域へのアクセス権は、ユーザによって異なります。あなたの会社のCEOは、おそらく売上予測へのアクセス権を持つべきですが、あなたの家族の休暇の写真へのアクセス権は与えないかもしれません。
- 以下は、レルムを指定した仮想的なベーシック認証のチャレンジです。

```
HTTP/1.0 401 Unauthorized
WWW-Authenticate: Basic realm="Corporate Financials”
```

- レルムには「Corporate Financials」のような説明的な文字列名を付けて、ユーザがどのユーザ名とパスワードを使用すればよいかを理解できるようにします。また、「executive-committee@bigcompany.com」のように、サーバのホスト名をレルム名に含めると便利です。

## 12.2 Basic Authentication
- ベーシック認証は、最も普及しているHTTP認証プロトコルです。ほとんどすべての主要なクライアントとサーバーがBasic認証を実装しています。ベーシック認証は、もともとHTTP/1.0の仕様書に記載されていましたが、その後、RFC2617に移され、HTTP認証の詳細が記載されています。
- ベーシック認証では、ウェブサーバーは、クライアントに有効なユーザー名とパスワードの入力を求めてトランザクションを拒否することができます。サーバーは、200ではなく401ステータスコードを返すことで認証チャレンジを開始し、WWW-Authenticateレスポンスヘッダでアクセスしているセキュリティレルムを指定します。ブラウザはチャレンジを受信すると、このレルムのユーザ名とパスワードを要求するダイアログボックスを開きます。ユーザ名とパスワードは、Authorizationリクエスト・ヘッダの中で、わずかにスクランブルされた形式でサーバに送り返されます。

## 12.3 The Security Flaws of Basic Authentication
- 本章前半の図12-2では、基本認証の詳細な例を示した。

  - 図12-2aでは、ユーザが個人の家族写真/family/jeff.jpgを要求している。
  - 図12-2bでは、サーバはWWW-Authenticateヘッダとともに、個人の家族写真に対する401 Authorization Requiredのパスワード・チャレンジを送り返す。このヘッダは、Familyという名前のレルムに対する基本認証を要求している。
  - 図 12-2c では、ブラウザは 401 チャレンジを受け取り、Family レルムのユーザ名とパスワードを尋ねるダイアログ・ボックスを開く。ユーザがユーザ名とパスワードを入力すると、ブラウザはそれらをコロンでつなぎ、「スクランブルされた」Base-64表現（次のセクションで説明）にエンコードして、Authorizationヘッダで送り返します。
  - 図12-2dでは、サーバはユーザ名とパスワードを解読し、それらが正しいことを確認して、要求されたドキュメントをHTTP 200 OKメッセージで返す。

- HTTP基本認証のWWW-AuthenticateおよびAuthorizationヘッダの概要を表12-2に示します。

| Challenge/Response | Header syntax and description |
| ---- | ---- |
| Challenge (server to client) | サイトのさまざまな部分に異なるパスワードが設定されている場合があります。レルムは、要求されているドキュメントのセットの名前を示す引用文字列で、ユーザーはどのパスワードを使用すべきかを知ることができます。<br>WWW-Authenticate: Basic realm=quoted-realm |
| Response (client to server) | ユーザー名とパスワードをコロン（：）でつなぎ、Base-64エンコーディングに変換することで、ユーザー名やパスワードに国際的な文字を含めることが少し容易になり、ネットワークトラフィックを見ていても、ざっと調べただけではユーザー名やパスワードが出てこないようになっています。<br>Authorization: Basic base64-username-and-password |

- なお、Basic認証プロトコルでは、表12-1で示したAuthentication-Infoヘッダーは使用していません。

### 12.2.2 Base-64 Username/Password Encoding
- HTTP基本認証では、ユーザー名とパスワードをコロンで区切ってまとめ、base-64エンコード方式でエンコードします。base-64エンコーディングが何か分からなくても、心配は要りません。もし興味があれば、付録Eに詳しく書かれています。簡単に言うと、base-64エンコーディングは、8ビットのバイト列を6ビットのチャンクに分割します。各6ビットの塊は、主に文字と数字で構成された64文字の特別なアルファベットの中から文字を選ぶのに使われます。
- 図12-4は、Basic認証にBase-64エンコーディングを使用した例である。ここでは、ユーザ名が「brian-totty」、パスワードが「Ow!」。 ブラウザはユーザ名とパスワードをコロンで結合し、「brian-totty:Ow!」という圧縮文字列を生成する。この文字列はbase64でエンコードされ、次のような言葉になります。"YnJpYW4tdG90dHk6T3ch". 
- Base-64エンコーディングは、バイナリ、テキスト、国際文字データなどの文字列（一部のシステムでは問題が発生していた）を、一時的に携帯可能なアルファベットに変換して伝送するために考案されたものである。これにより、元の文字列をリモート側で復号する際に、伝送の破損を心配する必要がなくなりました。
- Base-64エンコーディングは、国際文字やHTTPヘッダーでは不正な文字（引用符、コロン、キャリッジリターンなど）を含むユーザー名やパスワードに有効です。また、base-64エンコーディングは、ユーザー名とパスワードをごちゃごちゃにスクランブルするため、管理者がサーバーやネットワークを管理する際に、誤ってユーザー名やパスワードを見てしまうことを防ぐのにも役立ちます。

### 12.2.3 Proxy Authentication
- 認証は、中間的なプロキシサーバーによっても行われます。組織によっては、プロキシサーバーを使ってユーザーを認証してから、サーバーやLAN、無線ネットワークにアクセスさせているところもある。プロキシサーバーは、アクセスポリシーをプロキシサーバー上で集中管理できるため、組織のリソース全体で統一されたアクセスコントロールを提供する便利な手段となります。このプロセスの最初のステップは、プロキシ認証によるアイデンティティの確立である。
- プロキシ認証の手順は、Webサーバの識別と同じである。ただし、ヘッダとステータスコードは異なります。表12-3は、Webサーバ認証とプロキシ認証で使用されるステータスコードとヘッダを対比させたものである。

| Web server | Proxy server |
| Unauthorized status code: 401 | Unauthorized status code: 407 |
| WWW-Authenticate | Proxy-Authenticate |
| Authorization | Proxy-Authorization |
| Authentication-Info | Proxy-Authentication-Info |

## 12.3 The Security Flaws of Basic Authentication
- 基本認証はシンプルで便利ですが、安全ではありません。基本認証は、悪意のない第三者からの意図しないアクセスを防ぐためにのみ使用するか、SSLなどの暗号化技術と組み合わせて使用する必要があります。
- 以下のようなセキュリティ上の欠陥があると考えられます。

  1. Basic認証では、ユーザ名とパスワードを、簡単に解読できる形式でネットワーク上に送信します。事実上、秘密のパスワードは、誰もが読み取って取得できる平文で送信されます。ベース64エンコーディングでは、ユーザー名とパスワードが不明瞭になるため、ネットワークを偶然観察した友人がパスワードを盗み出す可能性は低くなります。しかし、ベース64エンコーディングされたユーザー名とパスワードが与えられた場合、エンコーディングのプロセスを逆にすることで、デコーディングは些細なことで実行できます。復号化は、鉛筆と紙を使って手作業で数秒で行うこともできます。Base 64で暗号化されたパスワードは、事実上、"平文 "で送信されます。基本認証で送信されたユーザー名とパスワードを、動機のある第三者が傍受することを想定してください。これが懸念される場合は、すべてのHTTPトランザクションをSSL暗号化チャネルで送信するか、ダイジェスト認証など、より安全な認証プロトコルを使用してください。
  2. たとえ秘密のパスワードが、解読するのがより複雑なスキームでエンコードされていたとしても、第三者は文字化けしたユーザー名とパスワードをキャプチャし、オリジンサーバーに文字化けした情報を何度もリプレイしてアクセスすることができます。このようなリプレイアタックを防ぐための工夫はなされていない。
  3. 企業のイントラネットへのアクセス制御やパーソナライズされたコンテンツなど、重要ではないアプリケーションに基本認証が使用されている場合でも、社会的な行動がこれを危険なものにしています。多くのユーザは、パスワードで保護された多数のサービスに圧倒されて、ユーザ名とパスワードを共有しています。巧妙な悪意のある者は、例えば、インターネットの無料電子メールサイトからユーザー名とパスワードを平然と入手し、同じユーザー名とパスワードで重要なオンラインバンキングサイトにアクセスできることに気づくかもしれません。
  4. ベーシック認証は、認証ヘッダーはそのままにして、メッセージの残りの部分を変更して、取引の性質を大幅に変えてしまうような、プロキシや仲介者のような行為に対する保護はありません。
  5. Basic認証は、偽造サーバによるスプーフィングに弱い。ユーザーがBasic認証で保護された有効なホストに接続していると思わせておいて、実際には敵対的なサーバーやゲートウェイに接続している場合、攻撃者はパスワードを要求し、後で使用するためにパスワードを保存し、エラーを装うことができます。

- とはいえ、基本認証は、親しみやすい環境で文書を便利にパーソナライズしたり、アクセス制御したりする場合や、プライバシーが要求されるが絶対に必要ではない場合には有用である。このように、ベーシック認証は、好奇心旺盛なユーザーによる偶発的またはカジュアルなアクセスを防ぐために使用されます[1]。
  - [1] 基本認証のユーザー名／パスワードが、より安全なシステムのパスワードと同じでないことに注意してください。そうでないと、悪意のあるユーザーが、安全なアカウントに侵入するために使用することができます。
- 例えば、企業内では、製品管理者が将来の製品計画をパスワードで保護して、早急な配布を制限することがあります。ベーシック認証を利用すれば、第三者がこのデータにアクセスすることを十分に防ぐことができます[2]。同様に、個人的な写真や個人的なウェブサイトをパスワードで保護することもあるでしょう。これらは極秘でもなければ、貴重な情報を含んでいるわけでもありませんが、他の人には関係のないことです。
  - [2] あまり安全ではありませんが、通常、会社の内部の従業員は、悪意を持ってパスワードを取得しようとは思いません。とはいえ、企業スパイは発生しますし、復讐心に燃えた不満を持つ従業員も存在しますので、悪意を持って取得されると非常に有害となるデータは、より強固なセキュリティスキームの下に置くことが賢明です。
- 基本認証は、ユーザー名とパスワードを悪意のある人物から隠すために、暗号化されたデータ伝送（SSLなど）と組み合わせることで、安全性を高めることができます。これは一般的な手法です。
- 安全な暗号化については第14章で説明します。次の章では、Basic認証よりも強力なセキュリティ特性を持つ、より洗練されたHTTP認証プロトコルであるDigest認証について説明します。

# Chapter 13. Digest Authentication

## 13.1 The Improvements of Digest Authentication

## 13.2 Digest Calculations

## 13.3 Quality of Protection Enhancements

## 13.4 Practical Considerations

## 13.5 Security Considerations

# Chapter 14. Secure HTTP
- これまでの3つの章では、ユーザーの識別や認証に役立つHTTPの機能について説明しました。これらの技術は、友好的なコミュニティではうまく機能しますが、意欲的で敵対的な敵のコミュニティから重要なトランザクションを保護するには十分な強度がありません。
- 本章では、デジタル暗号を用いて、盗聴や改ざんからHTTPトランザクションを保護するための、より複雑で積極的な技術を紹介します。

## 14.1 Making HTTP Safe
- 人々はウェブでの取引を重大なことに使っています。強力なセキュリティがなければ、人々は安心してオンラインショッピングやバンキングを行うことができません。また、アクセスを制限できなければ、企業は重要な文書をウェブサーバに置くことができません。ウェブには、安全な形式のHTTPが必要です。
- 前の章では、認証（基本認証とダイジェスト認証）とメッセージの整合性（ダイジェスト qop="auth-int"）を提供するいくつかの軽量な方法について説明しました。これらのスキームは多くの目的に適していますが、大規模な買い物や銀行取引、機密データへのアクセスには十分な強度が得られない場合があります。このような深刻な取引のために、私たちはHTTPとデジタル暗号化技術を組み合わせています。
- HTTPの安全なバージョンは、効率的で、ポータブルで、管理が容易で、世界の変化に対応できるものでなければなりません。また、社会的、政府的な要求を満たす必要があります。私たちは、HTTPセキュリティのために、以下を提供する技術を必要としています。

  - サーバー認証（クライアントは、偽物のサーバーではなく、本物のサーバーと話していることを知っている）
  - クライアント認証（サーバーは、偽物ではなく本物のユーザーと話していることを知っている）
  - 完全性（クライアントとサーバーは、データが変更される心配をすることがない）
  - 暗号化 (クライアントとサーバーは、盗聴の心配なく内密に会話できる)
  - 効率性（安価なクライアントやサーバーでも使用できるほど、高速なアルゴリズムであること）
  - ユビキタス性（ほぼすべてのクライアントやサーバがサポートするプロトコル）
  - 管理的な拡張性（誰でも、どこでも、すぐに安全な通信ができる）
  - 適応性 (その時点で最もよく知られているセキュリティ手法をサポート)
  - 社会的実現性（社会の文化的・政治的なニーズに対応していること） 

### 14.1.1 HTTPS
- HTTPSは、HTTPの最も一般的な安全な形式です。これは Netscape Communications Corporation が開発したもので、すべての主要なブラウザとサーバーでサポートされています。
- ウェブページがHTTPではなくHTTPSでアクセスされているかどうかは、URLが http:// ではなく https:// というスキームで始まることでわかります（一部のブラウザでは、図14-1に示すように、アイコニックなセキュリティキューが表示されます）。
- HTTPSを使用すると、HTTPリクエストとレスポンスのデータがすべて暗号化されてネットワーク上に送信されます。HTTPS は、SSL（Secure Sockets Layer）またはその後継である TLS（Transport Layer Security）のいずれかを使用して、トランスポートレベルの暗号化セキュリティ層を HTTP の下に提供することで機能します（図 14-2）。SSL と TLS は非常によく似ているため、本書では SSL と TLS の両方を表すために「SSL」という用語を大まかに使用しています。
- ほとんどのハードなエンコーディングやデコーディングはSSLライブラリで行われるため、ウェブクライアントやサーバは、セキュアなHTTPを使用するために、プロトコル処理ロジックの多くを変更する必要はありません。ほとんどの場合、TCPの入出力コールをSSLコールに置き換え、セキュリティ情報を設定・管理するためのいくつかのコールを追加するだけです。

## 14.2 Digital Cryptography
- HTTPS について詳しく説明する前に、SSL と HTTPS で使用されている暗号エンコーディング技術について少し説明する必要があります。次のいくつかのセクションでは、デジタル暗号技術の要点を手短に説明します。デジタル暗号の技術や用語について既にご存知の方は、遠慮なくセクション 14.7 に進んでください。
- このデジタル暗号の入門章では、以下のことについて説明します。

  - Ciphers
    - 覗き見されても読めないようにテキストをエンコードするアルゴリズム。
  - Keys
    - 暗号の動作を変化させる数値的なパラメータ
  - Symmetric-key cryptosystems - 対称鍵暗号方式
    - 符号化と復号化に同じ鍵を使用するアルゴリズム
  - asynmmetric-key cryptosystems - 非対称鍵暗号方式
    - 符号化と復号化に異なる鍵を使用するアルゴリズム
  - Public-key cryptography - 公開鍵暗号方式
    - 数百万台のコンピュータから秘密のメッセージを簡単に送信できるシステム
  - Digital signatures - デジタル署名
    - メッセージが偽造または改ざんされていないことを確認するチェックサム
  - Digital certificates - デジタル証明書
    - 信頼できる組織によって検証、署名された識別情報 

### 14.2.1 The Art and Science of Secret Coding
- 暗号とは、メッセージを符号化・復号化する技術と科学のことです。何千年も前から、人々は秘密のメッセージを送るために暗号技術を利用してきました。しかし、暗号技術は単にメッセージを暗号化しておせっかいな人に読まれないようにするだけでなく、メッセージの改ざんを防ぐためにも使用することができます。また、小切手への手書きのサインや封筒に押された封印のように、メッセージや取引を行ったことを証明するためにも、暗号技術を利用することができます。

### 14.2.2 Ciphers
- 暗号技術は「暗号」と呼ばれる秘密のコードに基づいています。暗号とは、メッセージを符号化するための特定の方法と、後に秘密を解読するための付随する方法とを組み合わせたものです。符号化される前の元のメッセージは、多くの場合、plaintextまたはcleartext（両方とも日本語では平文）と呼ばれます。暗号が適用された後のコード化されたメッセージは、多くの場合、暗号文と呼ばれます。図14-3に簡単な例を示します。
- 暗号は何千年も前から秘密のメッセージを作るために使われてきました。伝説によると，ジュリアス・シーザーは，メッセージの各文字をアルファベット3文字分前方の文字に置き換える3文字回転暗号を使用していたそうです。現代のアルファベットで言えば、「A」は「D」に、「B」は「E」に、というように置き換えられます。
- 例えば，図14-4では，「meet me at the pier at midnight」というメッセージは，rot3（rotate by 3 characters）暗号を用いて，「phhw ph dw wkh slhu dw plgqljkw」という暗号文に変換される[1]．この暗号文は，アルファベットの3文字を回転させる逆の暗号を適用することで，元の平文メッセージに復号することができる。
  - [1] 例を簡単にするために、ここでは句読点や空白を回転させていませんが、可能です。

### 14.2.3 Cipher Machines
- 暗号は、人間が自分で符号化と復号化を行う必要があったため、比較的単純なアルゴリズムとして始まりました。暗号が単純だったので、人間は鉛筆と紙と暗号書を使って暗号を解くことができた。しかし、賢い人間は簡単に暗号を解読することができた。
- 技術が進歩すると、より複雑な暗号を使って、メッセージを素早く正確に暗号化・解読できる機械が作られるようになった。これらの暗号機は、単純な回転を行うだけでなく、文字を入れ替えたり、文字の順序を入れ替えたり、メッセージを切り刻んだりして、暗号の解読をはるかに困難にしました[2]。
  - [2] 最も有名な機械式暗号機は、第二次世界大戦中のドイツのエニグマ暗号機でしょう。エニグマ暗号の複雑さにもかかわらず、アラン・チューリングらは1940年代初頭に、初期のデジタルコンピュータを使ってエニグマ暗号を解読することに成功した。

### 14.2.4 Keyed Ciphers
- 暗号のアルゴリズムや機械が敵の手に渡る可能性があるため、ほとんどの機械には、暗号の仕組みを変えるためのダイヤルを大量に設定できるようになっていた。たとえ機械が盗まれても、正しいダイヤル設定（キーの値）がなければ、デコーダーは機能しないのだ[3]。
  - [3] 実際には、機械のロジックを持っていると、暗号を解くのに役立つことがあります。なぜなら、機械のロジックは、利用できるパターンを示しているかもしれないからです。最近の暗号アルゴリズムは、たとえアルゴリズムが公開されていても、悪人がコードを解読するのに役立つパターンを思いつくのが難しいように設計されているのが普通です。実際、現在使用されている最強の暗号の多くは、そのソースコードが公開されており、誰でも見て研究することができます。
- これらの暗号パラメータは鍵と呼ばれていた。解読プロセスが正しく機能するためには、正しい鍵を暗号機に入力する必要があります。暗号鍵は、1台の暗号機を多数の仮想的な暗号機の集合のように動作させ、それぞれの暗号機は異なる鍵の値を持つため、異なる動作をします。
- 現在では、ほぼすべての暗号アルゴリズムに鍵が使われている。

### 14.2.5 Digital Ciphers
- デジタル計算機の登場により、2つの大きな進歩がありました。
  - 機械的な速度や機能の制限から解放され、複雑な符号化・復号化のアルゴリズムが可能になったこと。
  - 非常に大きな鍵を扱えるようになり、1つの暗号アルゴリズムから、鍵の値によって異なる何兆もの仮想的な暗号アルゴリズムを作り出すことができるようになった。鍵が長ければ長いほど、より多くの暗号の組み合わせが可能となり、ランダムに鍵を推測して暗号を解読することが難しくなります。
- デジタルキーは、物理的な金属キーや機械装置のダイヤル設定とは異なり、単なる数字です。これらのデジタルキーの値は、符号化アルゴリズムと復号化アルゴリズムの入力となる。符号化アルゴリズムは、データの塊を、アルゴリズムと鍵の値に基づいて符号化／復号化する関数である。
- Pという平文メッセージ、Eという符号化関数、eというデジタル符号化鍵があれば、符号化された暗号文メッセージC（C=E(P,e)）を生成することができる（図14-6）。暗号文Cは，復号関数Dと復号鍵dを用いて，元の平文Pに復号することができる．もちろん，復号関数と符号化関数は互いに逆であり，Pの符号化を復号すると，元のメッセージPが戻ってくる。

## 14.3 Symmetric-Key Cryptography
- ここでは、鍵と暗号の関係について、もう少し詳しく説明します。多くのデジタル暗号アルゴリズムは、対称鍵暗号と呼ばれています。これは、暗号化にも復号化にも同じ鍵の値を使用するからです（e = d）。ここでは、鍵を「k」と呼ぶことにします。
- 対称鍵暗号では、送信者と受信者の両方が、同じ共有秘密鍵kを持っていないと通信できません。送信者は、共有秘密鍵を使ってメッセージを暗号化し、得られた暗号文を受信者に送ります。受信者はその暗号文を受け取り，同じ共有秘密鍵とともに復号化関数を適用して元の平文を復元する（図14-7）。
- 一般的な共通鍵暗号アルゴリズムには、DES、Triple-DES、RC2、RC4などがあります。

### 14.3.1 Key Length and Enumeration Attacks
- 秘密鍵が秘密のままであることは非常に重要です。ほとんどの場合、符号化と復号化のアルゴリズムは公知であるため、秘密なのは鍵だけなのです。
- 優れた暗号アルゴリズムでは、敵は暗号を解読するために、宇宙に存在する可能性のあるすべての鍵値を試すことになります。全力ですべての鍵盤を試すことを「列挙型攻撃」といいます。可能性のある鍵盤が数個しかない場合、悪者は総当りですべての鍵盤を試し、最終的に暗号を解読することができます。しかし、可能性のある鍵の値がたくさんある場合、悪者がすべての鍵を調べて、暗号を破る鍵を探すには、何日も、何年も、あるいは宇宙の一生さえもかかるかもしれません。
- 可能な鍵の数は、鍵のビット数と、可能な鍵のうちいくつが有効かに依存する。対称鍵暗号では、通常、すべての鍵が有効である[4]。8ビットの鍵では256個、40ビットの鍵では240個（約1兆個）、128ビットの鍵では約262,000,000,000,000,000,000,000,000個の鍵が有効である。
  - [4] 鍵の一部しか有効でない暗号もある。例えば，最も有名な非対称鍵暗号方式であるRSAでは，有効な鍵は素数と一定の関係がなければならない．このような性質を持つ鍵は、限られた数しかありません。
- 従来の共通鍵暗号では、40ビットの鍵は、小規模で重要でない取引には十分安全だと考えられている。しかし、現在の高速ワークステーションでは、1秒間に数十億回の計算を行うことができるため、40ビットの鍵は破られてしまう。
- 一方、128ビットの鍵は、対称鍵暗号の中では非常に強い鍵とされている。
- TripleDESキーに似たサイズの128ビットDESキーは、ブルートフォース攻撃を使っても、誰でも、どんなコストでも、事実上解読できないと考えられています[6]。
  - [6] しかし、鍵が大きいからといって、その暗号が絶対に安全というわけではありません。暗号のアルゴリズムや実装に気づかれない欠陥があり、それが攻撃者にとっての弱点になっている可能性があります。また、攻撃者が鍵の生成方法について何らかの情報を持っていて、ある鍵が他の鍵よりも可能性が高いことを知っていて、ブルートフォース攻撃の焦点となっている可能性もあります。あるいは、ユーザーが秘密鍵を攻撃者が盗めるような場所に置いてしまうこともあるでしょう。

### 14.3.2 Establishing Shared Keys
- 対称鍵暗号の欠点は、送信者と受信者の両方が、お互いに会話をする前に共有秘密鍵を持っていなければならないことです。
- 例えば、テレビの家庭用品番組を見て木工用品を注文するために、Joe's Hardware Storeと安全に会話をしたいとしたら、安全に商品を注文する前に、あなたとwww.joes-hardware.com の間で秘密鍵を確立しなければなりません。そのためには、秘密鍵を生成し、それを記憶する方法が必要です。あなたも Joe's Hardware も、そして他のすべてのインターネットユーザーも、生成して覚えておくべき何千もの鍵を持つことになります。
- 例えば、アリス(A)、ボブ(B)、クリス(C)の3人が、ジョーズ・ハードウェア(J)と話したいとします。A、B、Cはそれぞれ、Jとの間で秘密鍵を確立する必要があります。Aには鍵kAJ、Bには鍵kBJ、Cには鍵kCJが必要です。通信相手のペアには、それぞれの秘密鍵が必要です。N個のノードがあり、各ノードが他のN-1個のノードと安全に通信しなければならない場合、秘密鍵の総数はおよそN2個となり、管理上の悪夢となります。

## 14.4 Public-Key Cryptography
- 公開鍵暗号方式では、ホストのペアごとに1つのエンコード鍵/デコード鍵を使用するのではなく、2つの非対称鍵を使用します。1つはホストのメッセージをエンコードするための鍵、もう1つはホストのメッセージをデコードするための鍵です。
- 符号化用の鍵は世界中に公開されていますが（これが公開鍵暗号方式という名称の由来です）、復号用の秘密鍵はホストのみが知っています（図14-8参照）。これにより、特定のホストの公開鍵を誰でも見つけることができるため、鍵の確立が非常に容易になります。しかし、復号鍵は秘密にされているので、受信者だけがそのホストに送られたメッセージを復号することができます。
- ノードXはそのエンコーディング・キーをexとして公開することができます[7]。これで、ノードXにメッセージを送りたい人は誰でも同じ、よく知られた公開キーを使うことができます。公開鍵暗号方式では、各ホストに誰もが使用するエンコーディング・キーが割り当てられるため、ペアになった対称鍵のN2爆発を避けることができます（図14-9参照）。
  - [7] 後述するように、公開鍵の検索は実際にはデジタル証明書を通じて行われますが、公開鍵の検索方法の詳細は今はあまり重要ではありません。
- 誰もが同じ鍵を使ってXへのメッセージをエンコードできても、X以外の人はメッセージをデコードできません。鍵を分割すると、誰もがメッセージをエンコードできますが、メッセージをデコードできるのは所有者だけに制限されます。これにより、ノードはサーバの公開鍵を調べればよいので、サーバにメッセージを安全に送信することが容易になります。公開鍵暗号化技術は、世界中のすべてのコンピュータユーザーにセキュリティプロトコルを展開することを可能にします。公開鍵技術を標準化することは非常に重要であるため、PKI（Public-Key Infrastructure）標準化のための大規模な取り組みが10年以上前から行われている。

### 14.4.1 RSA
- 公開鍵型非対称暗号システムの課題は、次のような手がかりをすべて持っていたとしても、悪人が秘密鍵を計算できないようにすることです。
  - 公開鍵（公開されているため、誰でも入手できる）
  - 傍受した暗号文（ネットワークを盗聴して得たもの）
  - メッセージとそれに関連する暗号文（任意のテキストにエンコーダを実行して得られる）
- このようなニーズに応える代表的な公開鍵暗号方式として、MITで発明され、RSA Data Securityが商品化したRSAアルゴリズムがあります。公開鍵、任意の平文、平文を公開鍵で暗号化したときの暗号文、RSAアルゴリズムそのもの、さらにはRSA実装のソースコードがあっても、暗号を解読して対応する秘密鍵を見つけることは、巨大な素数を計算するのと同じくらい難しい問題だと考えられています（コンピュータサイエンスの中でも最も難しい問題の一つとされています）。つまり、大きな数字を素数に分解する高速な方法を見つけられれば、スイスの銀行口座に侵入できるだけでなく、チューリング賞を受賞することも可能なのです。
- RSA暗号の詳細については難しい数学が必要なので、ここでは触れません。数論の博士号を持っていなくても、RSAアルゴリズムを実行するためのライブラリはたくさんあります。

### 14.4.2 Hybrid Cryptosystems and Session Keys
- 非対称公開鍵暗号方式は、公開鍵を知っているだけで、誰でも安全なメッセージを公開サーバーに送ることができるという点で、とても便利です。2つのノードが安全に通信するために秘密鍵をネゴシエートする必要はありません。
- しかし、公開鍵暗号のアルゴリズムは、計算時間が長くなる傾向があります。実際には、対称型と非対称型の両方の方式が混在しています。例えば、公開鍵暗号方式でノード間のセキュアな通信を簡単に設定した後、そのセキュアなチャネルを使って一時的にランダムな対称鍵を生成・通信し、より高速な対称暗号方式で残りのデータを暗号化するという方法が一般的です。

## 14.5 Digital Signatures
- 暗号システムは、メッセージの暗号化と復号化に加えて、メッセージに署名することもできます。これにより、誰がメッセージを書いたのかということや、メッセージが改ざんされていないことを証明できます。この技術はデジタル署名と呼ばれ、次のセクションで説明するインターネットセキュリティ証明書に重要な役割を果たしています。

### 14.5.1 Signatures Are Cryptographic Checksums
- デジタル署名は、メッセージに添付される特別な暗号チェックサムです。デジタル署名には2つの利点があります。
  - 署名は、作者がメッセージを書いたことを証明します。作者の秘密鍵を持っているのは作者だけなので[8]、このチェックサムを計算できるのも作者だけです。チェックサムは、作者の個人的な「署名」の役割を果たします。
    - [8] これは、秘密鍵が盗まれていないことを前提としています。ほとんどの秘密鍵は、しばらくすると失効します。また、盗まれた鍵や漏洩した鍵を追跡する「失効リスト」もあります。
  - 署名は、メッセージの改ざんを防ぎます。悪意のある攻撃者がネットワーク内でメッセージを改ざんすると、チェックサムが一致しなくなります。また、チェックサムには作者の秘密鍵が含まれているため、侵入者は改ざんされたメッセージに対して正しいチェックサムを作成することができません。
- デジタル署名は、多くの場合、非対称の公開鍵技術を用いて生成されます。秘密鍵は所有者のみが知っているため、作成者の秘密鍵は一種の「サムプリント」として使用されます。図14-10は、ノードAがノードBにメッセージを送信し、それに署名する方法の例を示しています。
  - ノードAは、可変長のメッセージを固定サイズのダイジェストに変換します。
  - ノードAは、ユーザの秘密鍵をパラメータとして使用する「署名」関数をダイジェストに適用します。秘密鍵を知っているのはユーザだけなので、正しい署名関数は、署名者が所有者であることを示します。図14-10では、ユーザの秘密鍵を含むデコーダ関数Dを署名関数として使用しています[9]。
    - [9]  RSA暗号システムでは、デコーダ関数Dはすでに秘密鍵を入力としているため、署名関数として使用されます。なお、デコーダ関数は単なる関数であるため、どのような入力に対しても使用可能である。また、RSA暗号では、D関数とE関数はどちらを適用しても機能し、お互いに打ち消し合う。つまり、D(E(stuff))=stuffであるのと同様に、E(D(stuff))=stuffなのです。
  - 署名が計算されると、ノードAはその署名をメッセージの末尾に追加し、メッセージと署名の両方をノードBに送信します。
  - 受け取ったノードBは、ノードAが本当にメッセージを書いたのか、メッセージが改ざんされていないのかを確認したい場合、署名を確認することができます。ノードBは、秘密鍵でスクランブルされた署名を受け取り、公開鍵を使って逆関数を適用します。展開されたダイジェストがノードB自身のバージョンのダイジェストと一致しない場合、メッセージが通信中に改ざんされたか、送信者がノードAの秘密鍵を持っていなかった（つまりノードAではなかった）ことになります。

## 14.6 Digital Certificates
- ここでは、インターネットの「IDカード」ともいえるデジタル証明書について説明します。デジタル証明書は、信頼された組織によって保証されたユーザーや企業に関する情報を含んでいます（ブレスミントのように「サート」と呼ばれます）。
- 私たちは皆、さまざまな形の身分証明書を持っています。パスポートや運転免許証などの一部の身分証明書は、多くの状況で自分の身元を証明するのに十分な信頼性を持っています。例えば、米国の運転免許証は、大晦日にニューヨーク行きの飛行機に乗るための十分な身分証明になりますし、ニューヨークに着いてから友人と一緒に酔っぱらった飲み物を飲むための十分な年齢証明にもなります。
- パスポートのような信頼性の高い身分証明書は、特別な紙に政府が署名・捺印したものです。偽造が困難であるため、本質的に高い信頼性を持っています。企業のバッジやスマートカードには、キャリアのアイデンティティを強化するための電子機器が搭載されています。政府の極秘機関の中には、指紋や網膜のパターンをIDと照合してからでないと信用できないところもあります。
- 名刺のような他の身分証明書は、比較的簡単に偽造できるため、人々はこれらの情報をあまり信用しません。仕事上のやりとりでは問題ないかもしれませんが、住宅ローンを申し込む際の雇用証明としては不十分でしょう。

### 14.6.1 The Guts of a Certificate
- デジタル証明書には、一連の情報が含まれており、そのすべてが公式の "認証局 "によってデジタル署名されています。基本的な電子証明書には、印刷されたIDと共通する以下のような基本的なものが含まれています。
  - 対象者の名前（個人、サーバー、組織など）
  - 有効期限
  - 証明書の発行者（証明書を保証する者）
  - 証明書発行者のデジタル署名
- また、電子証明書には、多くの場合、サブジェクトの公開鍵のほか、サブジェクトに関する記述的な情報や使用されている署名アルゴリズムに関する情報が含まれています。デジタル証明書は誰でも作成することができますが、誰もが著名な署名機関に証明書の情報を保証してもらい、その秘密鍵で証明書に署名してもらえるわけではありません。典型的な証明書の構造を図14-11に示す。

### 14.6.2 X.509 v3 Certificates
- 残念ながら、電子証明書には唯一無二の世界標準はありません。印刷されたIDカードのすべてが同じ場所に同じ情報を持っているわけではないように、デジタル証明書には微妙に異なる多くのスタイルがあります。幸いなことに、現在使用されているほとんどの証明書は、X.509 v3と呼ばれる標準的な形式で情報を保存しています。 X.509 v3証明書は、証明書の情報を解析可能なフィールドに構造化する標準的な方法を提供しています。証明書の種類によってフィールドの値は異なりますが、ほとんどの証明書はX.509 v3の構造に従っています。X.509証明書のフィールドは表14-2のとおりである。
- X.509ベースの証明書には、ウェブサーバ証明書、クライアント電子メール証明書、ソフトウェアコードサイニング証明書、認証局証明書など、いくつかの種類があります。

### 14.6.3 Using Certificates to Authenticate Servers
- HTTPS を通じて安全なウェブ取引を確立する際、最近のブラウザは、接続先のサーバーのデジタル証明書を自動的に取得します。サーバーが証明書を持っていない場合、安全な接続は失敗します。サーバー証明書には、以下のような多くのフィールドが含まれています。
  - Webサイトの名前とホスト名
  - Webサイトの公開鍵
  - 署名機関の名前
  - 署名機関の署名
- ブラウザは証明書を受け取ると、署名機関を確認します[10]。署名機関が公開されていて信頼性が高ければ、ブラウザはその公開鍵をすでに知っているので（ブラウザには多くの署名機関の証明書がプリインストールされています）、前節のセクション14.5で説明したように署名を検証することができます。図14-12は、デジタル・シグネチャを使って証明書の完全性を検証する方法を示している。
  - [10] ブラウザやその他のインターネット・アプリケーションは、閲覧を容易にするために、ほとんどの証明書管理の詳細を隠そうとします。しかし、安全な接続でブラウジングしている場合、すべての主要なブラウザでは、会話しているサイトの証明書を個人的に調べて、すべてが正常に行われていることを確認することができます。
- 署名機関が不明な場合、ブラウザはその署名機関を信頼すべきかどうか確信が持てず、通常はユーザーが署名者を信頼するかどうかを確認するためのダイアログボックスを表示します。署名者は、地域のIT部門やソフトウェア・ベンダーかもしれません。

## 14.7 HTTPS: The Details
- HTTPSは、HTTPの最も一般的なセキュアバージョンです。HTTPS は広く実装されており、すべての主要な商用ブラウザとサーバで利用可能です。HTTPS は、HTTP プロトコルと、対称型、非対称型、および証明書ベースの強力な暗号技術を組み合 わせたもので、HTTPS は非常に安全であると同時に、分散化されたグローバルなインターネットの無秩序 な状態においても、非常に柔軟で管理しやすいものとなっています。
- HTTPS はインターネットアプリケーションの成長を加速させ、Web ベースの電子商取引の急成長に大きな影響を与えています。また、HTTPS は、分散型ウェブアプリケーションの広域かつ安全な管理においても重要な役割を果たしています。

### 14.7.1 HTTPS Overview
- HTTPS は、HTTP を安全なトランスポート・レイヤーで送信しただけのものです。HTTPS は、HTTP メッセージを暗号化せずに TCP や世界中のインターネットに送信する代わりに（図 14-13a）、HTTP メッセージをまずセキュリティ層に送り、暗号化してから TCP に送信します（図 14-13b）。
- 現在、HTTP のセキュリティ層は、SSL とその最新の代替技術である TLS によって実装されています。ここでは、「SSL」という用語を、SSLまたはTLSのいずれかを意味するものとして使用するという一般的な慣習に従っています。

### 14.7.2 HTTPS Schemes
- 現在、セキュアなHTTPはオプションとなっています。したがって、ウェブサーバーにリクエストを行う際には、ウェブサーバーにセキュアプロトコル版のHTTPを実行するように伝える手段が必要です。これは、URL のスキームで行われます。セキュアではない通常のHTTPでは、URLのスキームのプレフィックスはhttpで、以下のようになります。

```
http://www.joes-hardware.com/index.html
```

- セキュアなHTTPSプロトコルでは、URLのスキームプレフィックスはhttpsで、以下のようになります。

```
https://cajun-shop.securesites.com/Merchant2/merchant.mv?Store_Code=AGCGS
```

- クライアント（ウェブブラウザなど）がウェブリソースに対するトランザクションを実行するように要求されると、クライアントはURLのスキームを調べます。
  - URLのスキームがhttpの場合、クライアントはポート80（デフォルト）でサーバへの接続を開き、平文のHTTPコマンドを送信します（図14-14a）。
  - URLのスキームがhttpsの場合、クライアントはポート443（デフォルト）でサーバへの接続を開き、サーバとの「ハンドシェイク」を行い、いくつかのSSLセキュリティパラメータをバイナリ形式でサーバと交換した後、暗号化されたHTTPコマンドを送信します（図14-14b）。
- SSLトラフィックはHTTPとは全く異なるバイナリプロトコルであるため、トラフィックは異なるポートで伝送されます（SSLは通常ポート443で伝送されます）。SSLとHTTPの両方のトラフィックがポート80に到達した場合、ほとんどのウェブサーバは、バイナリのSSLトラフィックを誤ったHTTPと解釈し、接続を閉じてしまいます。セキュリティサービスをHTTPに統合していれば、複数の送信先ポートを使用する必要はありませんが、実際には深刻な問題にはなりません。
- SSLがどのようにして安全なサーバーとの接続を設定するのか、もう少し詳しく見てみましょう。

### 14.7.3 Secure Transport Setup
- 暗号化されていないHTTPでは、クライアントはWebサーバのポート80へのTCPコネクションを開き、リクエストメッセージを送信し、レスポンスメッセージを受信してコネクションを閉じます。この一連の流れを図14-15aに示します。
- HTTPS では、SSL セキュリティレイヤーを使用するため、手順が若干複雑になります。HTTPS では、まずクライアントがウェブサーバのポート 443（セキュア HTTP のデフォルトポート）への接続を開きます。
- TCP 接続が確立されると、クライアントとサーバは SSL レイヤを初期化し、暗号化パラメータを取り決め、鍵を交換します。ハンドシェイクが完了すると、SSLの初期化が完了し、クライアントはセキュリティ層にリクエストメッセージを送信することができます。これらのメッセージはTCPに送信される前に暗号化されます。この手順は図14-15bに示されています。

### 14.7.4 SSL Handshake
- 暗号化されたHTTPメッセージを送信する前に、クライアントとサーバーは、SSLハンドシェイクを行う必要があります。
  - プロトコルのバージョン番号の交換 
  - 双方が知っている暗号を選択する
  - 両者のアイデンティティを認証する
  - チャンネルを暗号化するための一時的なセッションキーを生成する
- 暗号化されたHTTPデータがネットワークを通過する前に、SSLは通信を確立するためのハンドシェイクデータを送信します。SSL ハンドシェイクの概要を図14-16に示します。
- これは、SSLハンドシェイクの簡略化したバージョンです。SSLがどのように使われるかによって、ハンドシェイクはより複雑になることがありますが、これが一般的な考え方です。

### 14.7.5 Server Certificates
- SSLは、サーバーの証明書をクライアントに運び、クライアントの証明書をサーバーに戻すという相互認証をサポートしています。しかし、今日、クライアント証明書はブラウジングにはあまり使われていません。ほとんどのユーザーは個人のクライアント証明書を所有していません[11]。ウェブサーバーはクライアント証明書を要求することができますが、実際にはほとんど発生しません[12]。
  - [11] 一部の企業ではウェブ閲覧にクライアント証明書が使用されており、安全な電子メールにクライアント証明書が使用されています。将来的には、ウェブ閲覧にクライアント証明書が使われるようになるかもしれませんが、現在は非常にゆっくりと普及しています。
  - [12] 一部の組織のイントラネットでは、従業員の情報へのアクセスを制御するためにクライアント証明書を使用しています。一方、HTTPS による安全な取引には必ずサーバ証明書が必要である。クレジットカードの情報を掲載するなど、ウェブ・サーバ上で安全な取引を行う際には、自分が話している相手 が組織であることを確認したいものです。有名な機関によって署名されたサーバ証明書は、クレジットカードや個人情報を送信する前に、 そのサーバがどれだけ信頼できるかを評価するのに役立ちます。
- サーバ証明書は、組織の名前、住所、サーバのDNSドメイン名などが記載されたX.509 v3由来の証明書である（図14-17参照）。あなたとクライアント・ソフトウェアは、この証明書を調べて、すべてが順調に進んでいることを確認することができる。

### 14.7.6 Site Certificate Validation
- SSL自体は、ウェブサーバの証明書を検証する必要はありませんが、ほとんどの最新ブラウザは証明書の簡単なサニティチェックを行い、より詳細なチェックを行う手段を提供しています。Netscapeによって提案されたウェブサーバ証明書の検証アルゴリズムは、ほとんどのブラウザの検証技術の基礎となっています。その手順は以下の通りです。

- 日付チェック
  - まず、ブラウザは証明書の開始日と終了日をチェックし、証明書がまだ有効であることを確認します。証明書の有効期限が切れていたり、まだ有効になっていない場合、証明書の検証は失敗し、ブラウザはエラーを表示します。

- 署名者の信頼度チェック
  - すべての証明書は、サーバーを保証する認証局（CA）によって署名されています。証明書にはさまざまなレベルがあり、それぞれ異なるレベルの背景検証が必要です。例えば、電子商取引用のサーバー証明書を申請する場合、通常は企業として設立されたことを法的に証明する必要があります。
  - 証明書は誰でも作成することができますが、認証局の中には、よく理解された手順で証明書の申請者の身元やビジネス上の行動を確認する有名な組織もあります。このような理由から、ブラウザには信頼できる署名機関のリストが同梱されています。未知の（そしておそらく悪意のある）認証局によって署名された証明書をブラウザが受け取った場合、通常ブラウザは警告を表示します。また、ブラウザは、信頼できる認証局への有効な署名パスを持つ証明書を受け入れることもできます。つまり、信頼できるCAが「Sam's Signing Shop」の証明書に署名し、Sam's Signing Shopがサイト証明書に署名している場合、ブラウザはその証明書を有効なCAパスから派生したものとして受け入れることができます。

- 署名のチェック
  - 署名機関が信頼できると判断されると、ブラウザは署名機関の公開鍵を署名に適用し、チェックサムと比較することで、証明書の完全性をチェックします。

- サイトの同一性チェック
  - サーバーが他人の証明書をコピーしたり、トラフィックを傍受したりするのを防ぐために、ほとんどのブラウザは、証明書に記載されているドメイン名が、通信したサーバーのドメイン名と一致するかどうかを確認しようとします。サーバー証明書には通常1つのドメイン名が含まれていますが、CAによっては、サーバーのクラスタやファームのために、サーバー名のリストやワイルドカードのドメイン名を含む証明書を作成しているところもあります。ホスト名が証明書内のIDと一致しない場合、ユーザー指向のクライアントは、ユーザーに通知するか、不正な証明書のエラーで接続を終了しなければなりません。

### 14.7.7 Virtual Hosting and Certificates
- バーチャルホスト（1台のサーバに複数のホスト名を持つ）のサイトでは、安全なトラフィックを扱うのが難しい場合があります。一般的なウェブサーバープログラムの中には、単一の証明書しかサポートしていないものがあります。ユーザーが証明書名と厳密に一致しないバーチャルホスト名でアクセスしてきた場合、警告ボックスが表示されます。
- 例えば、ルイジアナ州をテーマにしたEコマースサイト「Cajun-Shop.com」を考えてみましょう。このサイトのホスティングプロバイダは、cajun-shop.securesites.com という正式名称を提供していました。ユーザが https://www.cajunshop.com にアクセスすると、サーバ証明書に記載されている公式ホスト名 (*.securesites.com)が、ユーザがブラウズした仮想ホスト名 (www.cajun-shop.com)と一致せず、図 14-18 の警告が表示される。
- この問題を防ぐため、Cajun-Shop.comのオーナーは、すべてのユーザーが安全な取引を開始する際に、cajunshop.securesites.comにリダイレクトします。バーチャルホストのサイトの証明書管理は、少し厄介です。

## 14.8 A Real HTTPS Client
- SSLは複雑なバイナリプロトコルです。暗号の専門家でない限り、生のSSLトラフィックを直接送信すべきではありません。ありがたいことに、いくつかの商用およびオープンソースのライブラリが存在し、SSLクライアントやサーバーのプログラムを簡単にすることができます。

### 14.8.1 OpenSSL
- OpenSSLは、SSLとTLSの最も一般的なオープンソースの実装です。OpenSSL プロジェクトは、SSL および TLS プロトコルを実装した堅牢で商用レベルの全機能を備えたツールキットと、強力な汎用暗号ライブラリを開発するための、ボランティアによる共同作業です。OpenSSLに関する情報やソフトウェアのダウンロードは、http://www.openssl.org から行えます。
- また、SSLeay（S-S-L-e-a-yと発音します）という名前を聞いたことがあるかもしれません。OpenSSLはSSLeayライブラリの後継で、よく似たインターフェースを持っています。SSLeayはもともとEric A. Young(SSLeayの "eay")によって開発されました。

### 14.8.2 A Simple HTTPS Client
- このセクションでは、OpenSSL パッケージを使って、極めて原始的な HTTPS クライアントを書いてみる。このクライアントは、サーバーとのSSL接続を確立し、サイト・サーバーからいくつかの識別情報をプリントアウトし、安全なチャネルを介してHTTP GETリクエストを送信し、HTTPレスポンスを受信し、レスポンスをプリントする。
- 以下に示すCプログラムは、一般的なHTTPSクライアントのOpenSSLによる実装です。プログラムをシンプルにするために、エラー処理や証明書処理のロジックは含まれていません。
- このサンプルプログラムでは、エラー処理が削除されているので、説明のためにのみ使用してください。このソフトウェアは、通常のエラー状態ではクラッシュしたり、誤動作したりします。
- サンプルプログラム（省略）
- この例はSun Solaris上でコンパイル、実行されていますが、SSLプログラムが多くのOSプラットフォームでどのように動作するかを示しています。OpenSSLの強力な機能のおかげで、暗号化、鍵や証明書の管理を含むこのプログラム全体が、3ページのCプログラムに収まっています。
- このプログラムをセクションごとに見ていきましょう。
  - プログラムの先頭には、TCPネットワークとSSLのサポートに必要なサポートファイルが含まれています。
  - セクション1では、SSL_CTX_newを呼び出して、ハンドシェークパラメータやSSL接続に関する他の状態を追跡するローカルコンテキストを生成します。
  - セクション2では、Unixのgethostbyname関数を用いて、（コマンドライン引数として与えられた）入力ホスト名をIPアドレスに変換します。他のプラットフォームでは、この機能を提供する別の方法があるかもしれません。
  - セクション3では、ローカル・ソケットを作成し、リモート・アドレス情報を設定し、リモート・サーバに接続することで、サーバのポート443へのTCP接続を開きます。
  - TCP接続が確立すると，SSL_newとSSL_set_fdを用いてTCP接続にSSLレイヤーをアタッチし，SSL_connectを呼び出してサーバとのSSLハンドシェイクを行います．セクション4が終了すると、暗号が選択され、証明書が交換された、機能するSSLチャンネルが確立されています。
  - セクション5では、選択された一括暗号化暗号の値を出力します。
  - セクション6では，サーバから返信されたX.509証明書に含まれる情報の一部を出力します．これには，証明書の所有者や証明書を発行した組織に関する情報が含まれます．OpenSSLライブラリは，サーバ証明書の情報に対して特別なことはしません．Webブラウザなどの実際のSSLアプリケーションでは、証明書が正しく署名されているか、正しいホストから発行されたものであるかを確認するために、証明書のサニティチェックを行います。ブラウザがサーバ証明書をどのように扱うかについては、セクション14.7.6で説明しました。
  - この時点で、SSL接続を使った安全なデータ転送の準備が整いました。セクション7では、SSL_writeを用いて、シンプルなHTTPリクエスト "GET / HTTP/1.0 "をSSLチャネル上で送信し、接続のアウトバウンド側をクローズします。
  - セクション8では、SSL_readを用いて、コネクションからのレスポンスを読み取り、スクリーンに表示しています。SSL層は全ての暗号化と復号化を行うため、通常のHTTPコマンドを書いたり読んだりすることができます。
  - 最後に、セクション9でクリーンアップを行います。
- OpenSSLライブラリの詳細については，http://www.openssl.org を参照してください．

### 14.8.3 Executing Our Simple OpenSSL Client
- 次に示すテキストは、安全なサーバーを指定したときの、先程のシンプルなHTTPクライアントの出力です。この例では、Morgan Stanley Online 証券会社のホームページにクライアントを向けている。オンライン・トレーディング・カンパニーは HTTPS を多用している。
- 出力（省略）
- 最初の4つのセクションが完了した時点で、クライアントはSSL接続を確立します。クライアントは、接続の状態や選択したパラメーターを問い合わせたり、サーバーの証明書を調べたりすることができます。
- この例では、クライアントとサーバーはDES-CBC3-MD5バルク暗号を使用しています。また、サーバのサイト証明書は、「米国ユタ州ソルトレイクシティ」にある「Morgan Stanley」という組織のものであることがわかります。この証明書はRSA Data Securityによって付与されたもので、ホスト名は "clients1.online.msdw.com "となっており、我々の要求と一致しています。
- SSLチャネルが確立され、クライアントがサイト証明書に安心感を覚えると、クライアントはHTTPリクエストを安全なチャネルで送信します。この例では、クライアントは「GET / HTTP/1.0」というシンプルなHTTPリクエストを送信し、302 Redirectレスポンスを受信して、ユーザーが別のURLを取得するように要求しています。

### 14.9 Tunneling Secure Traffic Through Proxies
- クライアントは、自分に代わってウェブサーバにアクセスするために、ウェブプロキシサーバを使用することがよくあります（プロキシについては第6章で説明します）。例えば、多くの企業では、企業ネットワークと公衆インターネットのセキュリティ境界にプロキシを配置しています（図14-19）。プロキシは、ファイアウォール・ルーターによってHTTPトラフィックのやり取りを許可された唯一のデバイスであり、ウイルスチェックやその他のコンテンツ制御を採用している場合もあります。
- しかし、クライアントがサーバの公開鍵を使ってサーバへのデータの暗号化を開始すると、プロキシはもはやHTTPヘッダを読むことができません。また、プロキシがHTTPヘッダを読めないと、どこにリクエストを転送すればいいのかわからなくなります（図14-20）。
- プロキシを使って HTTPS を動作させるには、プロキシに接続先を伝えるためにいくつかの変更が必要で す。一般的な手法として、HTTPS SSL トンネリング・プロトコルがある。HTTPS トンネリングプロトコルを使用すると、クライアントはまず、プロキシに対して、接続したいセキュアなホス トとポートを伝える。これは、暗号化が始まる前に平文で行われるため、プロキシはこの情報を読むことができます。
- 平文のエンドポイント情報の送信には、CONNECT と呼ばれる新しい拡張メソッドを用いて HTTP が使用されます。CONNECTメソッドは、プロキシに対して、目的のホストとポート番号への接続を開き、それが完了したら、クライアントとサーバーの間でデータを直接トンネリングするように指示します。CONNECTメソッドは1行のテキストコマンドで、セキュアオリジンサーバのホスト名とポート番号をコロンで区切って指定します。ホスト:ポートの後にはスペースが入り、HTTPバージョンの文字列の後にはCRLFが入ります。その後、0個以上のHTTPリクエストヘッダの行が続き、空の行が続きます。空の行の後、接続を確立するためのハンドシェイクが成功した場合、SSLデータの転送を開始することができます。以下はその例です。

```
CONNECT home.netscape.com:443 HTTP/1.0
User-agent: Mozilla/1.1N

<raw SSL-encrypted data would follow here...> 
```

- リクエストの空行の後、クライアントはプロキシからの応答を待ちます。プロキシはリクエストを評価し、リクエストが有効であること、ユーザーがこのような接続を要求することを許可されていることを確認します。問題がなければ、プロキシは接続先のサーバーに接続し、成功した場合は200 Connection Establishedレスポンスをクライアントに送信します。

```
HTTP/1.0 200 Connection established
Proxy-agent: Netscape-Proxy/1.1 
```

- セキュアトンネルとセキュリティプロキシの詳細については、セクション8.5に戻って参照してください。

# Part IV: Entities, Encodings, and Internationalization
- パート4では、HTTPメッセージのエンティティボディと、そのエンティティボディが貨物として運ぶコンテンツについて説明します。
  - 第15章では、HTTPコンテンツのフォーマットと構文について説明しています。
  - 第16章では、世界中の人々が異なる言語や異なる文字セットでコンテンツを交換することを可能にするウェブ標準について調査しています。
  - 第17章では、受け入れ可能なコンテンツを交渉するためのメカニズムについて説明します。

# Chapter 15. Entities and Encodings
- HTTPは毎日、あらゆる種類の何十億ものメディアオブジェクトを送信しています。画像、テキスト、動画、ソフトウェアプログラム......ありとあらゆるものを HTTP は送信しています。HTTP はまた、そのメッセージが適切に転送、識別、抽出、処理されることを確認しています。特に、HTTP はその貨物が以下のことを保証します。
  - ブラウザや他のクライアントがコンテンツを適切に処理できるよう、Content-Type メディアフォーマットや Content-Language ヘッダを使用して正しく識別できること
  - (Content-Length および Content-Encoding ヘッダを使用して) 適切にアンパックできること
  - 新鮮であること（エンティティバリデータやキャッシュ有効期限のコントロールを使用
  - ユーザーのニーズを満たしていること（content-negotiation Accept ヘッダを使用）。
  - ネットワーク上を迅速かつ効率的に移動する（レンジリクエスト、デルタエンコーディング、およびその他のデータ圧縮を使用）
  - 完全かつ改ざんされずに到着する（transfer encoding headers と Content-MD5 checksums を使用）。
- これらを実現するために、HTTPではコンテンツを運ぶためのラベル付けされたエンティティを使用しています。
- この章では、エンティティとそれに関連するエンティティヘッダー、そしてそれらがどのようにウェブカーゴを伝送するかについて説明します。ここでは、HTTP がコンテンツのサイズ、タイプ、エンコーディングなどの基本的な機能をどのように提供するかを説明します。また、レンジリクエスト、デルタエンコーディング、ダイジェスト、チャンクエンコーディングなど、HTTPエンティティのより複雑で強力な機能についても説明します。
- 本章の内容
  - HTTP データコンテナとしての HTTP メッセージエンティティの形式と動作
  - HTTP がエンティティボディのサイズをどのように記述するか、また、HTTP がサイズ決定の方法として何を要求するか
  - クライアントがコンテンツを適切に処理できるよう、コンテンツの形式、アルファベット、言語を記述するためのエンティティヘッダ
  - 可逆コンテンツエンコーディング： 送信者がコンテンツデータのフォーマットを送信前に変換し、容量を減らしたり、安全性を高めたりするために使用します。
  - ある種のコンテンツの通信を強化するために、HTTPのデータ転送方法を変更する転送エンコーディング、および長さが不明なコンテンツを安全に配信するためにデータを複数のピースに分割する転送エンコーディングであるチャンクドエンコーディング。
  - クライアントが要求されたコンテンツの最新バージョンを取得するためのタグ、ラベル、タイム、チェックサムの組み合わせ
  - コンテンツのバージョン番号のような役割を果たすバリデータは、ウェブアプリケーションが新鮮なコンテンツを確実に入手できるようにするためのもので、オブジェクトの鮮度を管理するためのHTTPヘッダーフィールドも含まれています。
  - 中断されたダウンロードを再開するのに便利なRanges 
  - HTTP デルタエンコーディングの拡張機能。これにより、クライアントは、以前に閲覧したリビジョンから実際に変更されたウェブページの部分のみを要求することができます。
  - エンティティボディのチェックサム。プロキシを通過する際のエンティティコンテンツの変更を検出するために使用されます。

## 15.1 Messages Are Crates, Entities Are Cargo
- HTTPメッセージをインターネットの配送システムの木箱と考えると、HTTPエンティティはメッセージの実際の貨物となります。図15-1は、HTTPレスポンスメッセージの中に運ばれる、単純なエンティティを示しています。
- HTTPエンティティヘッダ（第3章で解説）は、HTTPメッセージの内容を記述します。HTTP/1.1 では、10 個の主要なエンティティヘッダーフィールドが定義されています。

- Content-Type
  - エンティティが運ぶオブジェクトの種類。
- Content-Length
  - 送信されるメッセージの長さまたはサイズ。
- Content-Language
  - 送信されるオブジェクトに最もマッチする人間の言語。
- Content-Encoding
  - オブジェクトデータに対して行われる変換（圧縮など）。
- Content-Location
  - リクエストされた時点での、オブジェクトの別の位置。
- Content-Range
  - 部分的なエンティティの場合、このヘッダーは全体のどの部分が含まれるかを定義します。
- Content-MD5
  - エンティティボディのコンテンツのチェックサムです。
- Last-Modified
  - この特定のコンテンツがサーバーで作成または変更された日付。
- Expires
  - このエンティティデータが陳腐化する日付と時間。
- Allow
  - このリソースに対してどのようなリクエストメソッドが許可されているか（例：GETやHEAD）。
- ETag
  - この文書の特定のインスタンス[1]に固有のバリデータ。ETagヘッダは、正式にはエンティティヘッダとして定義されていませんが、エンティティを含む多くの操作にとって重要なヘッダです。
  - [1] インスタンスについては、本章の後半、セクション15.7で説明する。
- Cache-Control
  - このドキュメントをどのようにキャッシュするかについての指示です。Cache-Control ヘッダーは、ETag ヘッダーのように、エンティティヘッダーとして正式には定義されていません。

### 15.1.1 Entity Bodies
- エンティティボディには生の貨物が入っているだけです[2]。エンティティボディの貨物は単なる生のデータなので、そのデータの意味を記述するためにエンティティヘッダが必要となります。例えば、Content-Typeエンティティヘッダは、データをどのように解釈するか（画像、テキストなど）を示し、Content-Encodingエンティティヘッダは、データが圧縮されているかどうか、またはその他の方法で再コード化されているかどうかを示します。これらについては、次のセクションで説明します。
  - [2] Content-Encoding ヘッダがある場合、コンテンツはすでに content-encoding アルゴリズムによってエンコードされており、エンティティの最初のバイトはエンコードされた（例えば圧縮された）貨物の最初のバイトです。
- 生のコンテンツは、ヘッダーフィールドの終わりを示す空白のCRLF行の直後から始まります。コンテンツがテキストかバイナリか、文書か画像か、圧縮されているかいないか、英語かフランス語か日本語かなど、どのようなものであれ、CRLFの直後に配置されます。
- 図 15-2 は、実際の HTTP メッセージの例を 2 つ示しています。1 つはテキストエンティティを含み、もう 1 つはイメージエンティティを含みます。16進数の値はメッセージの正確な内容を示しています。
  - 図15-2aでは、エンティティボディはバイト番号65で始まり、end-of-headersのCRLFの直後にあります。エンティティボディには、"Hi! I'm a message. "のASCII文字が含まれています。
  - 図15-2bでは、エンティティボディはバイト番号67から始まります。エンティティボディにはGIF画像のバイナリコンテンツが含まれています。GIFファイルは6バイトのバージョン署名、16ビットの幅、16ビットの高さで始まります。この3つを直接エンティティボディで見ることができます。

## 15.2 Content-Length: The Entity's Size

## 15.3 Entity Digests

## 15.4 Media Type and Charset

## 15.5 Content Encoding

## 15.6 Transfer Encoding and Chunked Encoding

## 15.7 Time-Varying Instances

## 15.8 Validators and Freshness

## 15.9 Range Requests

## 15.10 Delta Encoding

# Chapter 16. Internationalization
- 毎日、何十億人もの人々が何百もの言語で文書を書いています。真にワールドワイドなウェブというビジョンを実現するためには、HTTPは、多くの言語やアルファベットで書かれた国際的な文書の転送と処理をサポートする必要があります。
- この章では、Webの国際化に関する2つの主要な問題、すなわち文字セットエンコーディングと言語タグについて説明します。HTTPアプリケーションは、文字セットエンコーディングを使用して、異なるアルファベットのテキストを要求、表示します。また、言語タグを使用して、ユーザーが理解できる言語にコンテンツを記述、制限します。最後に、多言語URIと日付について簡単に説明しています。
- 本章では下記を説明する。
  - HTTPが多言語アルファベットのスキームや規格とどのように関わっているかを説明しています。
  - HTTP プログラマーが正しく作業できるよう、用語、技術、規格の概要を迅速に説明しています (文字エンコーディングに精通している読者はこのセクションをスキップできます)。
  - 言語の標準的なネーミングシステムと、標準化された言語タグがどのようにコンテンツを記述し選択するかを説明しています。
  - 国際的な URI のルールと注意点について説明しています。
  - 日付の規則やその他の国際化に関する問題について簡単に説明しています。
